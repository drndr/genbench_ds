{"input": "Function for formatting numbers with two decimals [SEP] scaleFUN = function(x) sprintf(\"%.2f\", x)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Function for formatting numbers with two decimals [SEP] temp <- strsplit(dat$abstract, split=\" \")\ndat$abstract <- unlist(lapply(temp, function(x) paste(x[nchar(x)>=3], collapse=\" \")))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "remove stopwords [SEP] stopwords_regex = paste(stopwords('SMART'), collapse = '\\\\b|\\\\b')\nstopwords_regex = paste0('\\\\b', stopwords_regex, '\\\\b')\ndat$abstract <- unlist(lapply(dat$abstract, function(x) stringr::str_replace_all(x, stopwords_regex, '')))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "remove stopwords [SEP] no_outliers2 <- subset(df, DV2> (Q1 - 1.5*IQR2) & DV2< (Q3 + 1.5*IQR2))\nno_outliers3 <- subset(df, DV3> (Q1 - 1.5*IQR3) & DV3< (Q3 + 1.5*IQR3))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "remove double space [SEP] dat$abstract <- unlist(lapply(dat$abstract, function(x) gsub(' +',' ',x)))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "remove double space [SEP] set.seed(123) # Set random seed\nN = c(20,60) # Number of participants\nT = c(50,100) # Number of time points\nP = 4 # Number of variables in VAR(1) models\ncor.Sigma = 0.2 # Set the covariance of the within-individuals errors (i.e., all covariances are assumed to be equal)\nb.ar.min = 0.2 b.ar.max = 0.6\nb.cr.min = 0.05\nb.cr.max = 0.2\nK = c(2,4)\ndiff = c(1,2)\nsize = c(1,2,3)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "remove words of length 2 or less [SEP] temp <- strsplit(dat$abstract, split=\" \")\ndat$abstract <- unlist(lapply(temp, function(x) paste(x[nchar(x)>=3], collapse=\" \")))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "remove words of length 2 or less [SEP] save(df, file = paste(inputFolder, \"df_withIntercepts.rda\", sep = \"/\"))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "compute unigram frequency and probability [SEP] unigram_pre <- pre %>%\nunnest_tokens(word, abstract) %>%\ndplyr::count(word, sort = TRUE) %>%\nmutate(p = n / sum(n))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "compute unigram frequency and probability [SEP] results_corr <- list(res_corr_ext, res_corr_emo, res_corr_agr, res_corr_con, res_corr_ope, res_corr_mean)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "function to extract related words [SEP] search_related <- function(word_vectors, selected_vector) {\nsimilarities <- word_vectors %*% selected_vector %>%\ntidy() %>%\nas_tibble() %>%\ndplyr::rename(token = .rownames,\nsimilarity = unrowname.x.)\nsimilarities %>%\narrange(-similarity)}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "function to extract related words [SEP] source(file=\"Performance.Cluster.MVAR.FE.R\")", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Set the appropriate working directory [SEP] setwd(\"C:/Users/abell/Documents/exp_prep/analysis/S1_data\")", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Set the appropriate working directory [SEP] predictionmodel$results\nstopImplicitCluster()", "target": 0, "target_options": ["no_match", "match"]}
{"input": "only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3 [SEP] no_outliers2 <- subset(df, DV2> (Q1 - 1.5*IQR2) & DV2< (Q3 + 1.5*IQR2))\nno_outliers3 <- subset(df, DV3> (Q1 - 1.5*IQR3) & DV3< (Q3 + 1.5*IQR3))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3 [SEP] distances <- dist(cor_matrix, method = \"euclidean\", diag = FALSE, upper = FALSE)\nclusters <- hclust(distances) # note: tightest clusters appear on the left, defaults to the \"complete\" method", "target": 0, "target_options": ["no_match", "match"]}
{"input": "display the correlations as a histogram and heatmap [SEP] cor_matrix_half <- cor_matrix[upper.tri(cor_matrix)]\nmean(cor_matrix_half)\nsd(cor_matrix_half)\nhist(as.vector(cor_matrix_half), breaks=24, cex.axis=2) # Note: Novich et al. suppressed correlations of r<.4 in their visualisation\nheatmap(x = cor_matrix, symm = TRUE)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "display the correlations as a histogram and heatmap [SEP] calcNormProb <- function(x){\nnp <- x/cellStats(x,max)\nreturn(np)\n}", "target": 0, "target_options": ["no_match", "match"]}
{"input": "CLUSTERING similar correlations together [SEP] distances <- dist(cor_matrix, method = \"euclidean\", diag = FALSE, upper = FALSE)\nclusters <- hclust(distances) # note: tightest clusters appear on the left, defaults to the \"complete\" method", "target": 1, "target_options": ["no_match", "match"]}
{"input": "CLUSTERING similar correlations together [SEP] search_related <- function(word_vectors, selected_vector) {\nsimilarities <- word_vectors %*% selected_vector %>%\ntidy() %>%\nas_tibble() %>%\ndplyr::rename(token = .rownames,\nsimilarity = unrowname.x.)\nsimilarities %>%\narrange(-similarity)}", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Print docx [SEP] print(flextab, preview='docx')", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Print docx [SEP] tuning = expand.grid(alpha = 0, lambda = c(0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4))\ntuninga = expand.grid(alpha = 0, lambda = 0)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Split off the test set from the training and development data. [SEP] indices <- caret::createDataPartition(data$happiness, p = 0.75, list=FALSE)\ntrain_dev_data <- data[indices,]\ntestdata <- data[-indices,]", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Split off the test set from the training and development data. [SEP] stopwords_regex = paste(stopwords('SMART'), collapse = '\\\\b|\\\\b')\nstopwords_regex = paste0('\\\\b', stopwords_regex, '\\\\b')\ndat$abstract <- unlist(lapply(dat$abstract, function(x) stringr::str_replace_all(x, stopwords_regex, '')))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "activate kernels on computer for simultaneous processing. [SEP] cl <- makePSOCKcluster(10)\nregisterDoParallel(cl)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "activate kernels on computer for simultaneous processing. [SEP] mst1[which(mst1$age == 2), \"age\"] <- NA", "target": 0, "target_options": ["no_match", "match"]}
{"input": "We implement 10 fold cross-validation (same as above). [SEP] cross_validation = trainControl(method=\"cv\", number=10)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "We implement 10 fold cross-validation (same as above). [SEP] print(flextab, preview='docx')", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Set possible values for hyperparameter lambda. [SEP] tuning = expand.grid(alpha = 0, lambda = c(0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4))\ntuninga = expand.grid(alpha = 0, lambda = 0)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Set possible values for hyperparameter lambda. [SEP] calcNormProb <- function(x){\nnp <- x/cellStats(x,max)\nreturn(np)\n}", "target": 0, "target_options": ["no_match", "match"]}
{"input": "show the model parameters etc. [SEP] summary(myrsa)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "show the model parameters etc. [SEP] unigram_pre <- pre %>%\nunnest_tokens(word, abstract) %>%\ndplyr::count(word, sort = TRUE) %>%\nmutate(p = n / sum(n))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Get the results. [SEP] predictionmodel$results\nstopImplicitCluster()", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Get the results. [SEP] test_predictions = predict(predictionmodel, testdata)\nR2(test_predictions, testdata$happiness)\nlinreg = glm(happiness ~., data = train_dev_data) tp = predict(linreg, testdata)\nR2(tp, testdata$happiness)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Make predictions with best model and evaluate accuracy [SEP] test_predictions = predict(predictionmodel, testdata)\nR2(test_predictions, testdata$happiness)\nlinreg = glm(happiness ~., data = train_dev_data) tp = predict(linreg, testdata)\nR2(tp, testdata$happiness)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Make predictions with best model and evaluate accuracy [SEP] data$selfratephys[is.na(data$selfratephys)] <- median(data$selfratephys, na.rm = TRUE)\ndata$ucla3[is.na(data$ucla3)] <- median(data$ucla3, na.rm = TRUE)\ndata$estimateyrs_notlonely[is.na(data$estimateyrs_notlonely)] <- median(data$estimateyrs_notlonely, na.rm = TRUE)\ndata$estimateyrs_receivesocialsupp[is.na(data$estimateyrs_receivesocialsupp)] <- median(data$estimateyrs_receivesocialsupp, na.rm = TRUE)\ndata$estimateyrs_socialintegration[is.na(data$estimateyrs_socialintegration)] <- median(data$estimateyrs_socialintegration, na.rm = TRUE)\ndata$estimateyrs_nosmoke[is.na(data$estimateyrs_nosmoke)] <- median(data$estimateyrs_nosmoke, na.rm = TRUE)\ndata$estimateyrs_quitsmoke[is.na(data$estimateyrs_quitsmoke)] <- median(data$estimateyrs_quitsmoke, na.rm = TRUE)\ndata$estimateyrs_notexcessalcohol[is.na(data$estimateyrs_notexcessalcohol)] <- median(data$estimateyrs_notexcessalcohol, na.rm = TRUE)\ndata$estimateyrs_fluvax[is.na(data$estimateyrs_fluvax)] <- median(data$estimateyrs_fluvax, na.rm = TRUE)\ndata$estimateyrs_physact[is.na(data$estimateyrs_physact)] <- median(data$estimateyrs_physact, na.rm = TRUE)\ndata$estimateyrs_notobese[is.na(data$estimateyrs_notobese)] <- median(data$estimateyrs_notobese, na.rm = TRUE)\ndata$estimateyrs_meds[is.na(data$estimateyrs_meds)] <- median(data$estimateyrs_meds, na.rm = TRUE)\ndata$estimateyrs_lowpollution[is.na(data$estimateyrs_lowpollution)] <- median(data$estimateyrs_lowpollution, na.rm = TRUE)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "convert data to data.frame [SEP] prediction <- as.data.frame(prediction)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "convert data to data.frame [SEP] names(ProcessedData) <- c(\"Identifier\", \"Stimuli\", \"Distance\", \"Encoding_Rating\", \"Encoding_DecisionTime\", \"DecisionTime\", \"Deviation\", \"sdDecisionTime\", \"sdDeviation\")", "target": 0, "target_options": ["no_match", "match"]}
{"input": "number of observations [SEP] nrow(stdata)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "number of observations [SEP] ls()\nrm(list=ls())", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Check if there are any missing values in the selfratephys column [SEP] any(is.na(data$selfratephys)) # TRUE means there are missing data, (it came back TRUE)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Check if there are any missing values in the selfratephys column [SEP] set.seed(123) # Set random seed\nN = c(20,60) # Number of participants\nT = c(50,100) # Number of time points\nP = 4 # Number of variables in VAR(1) models\ncor.Sigma = 0.2 # Set the covariance of the within-individuals errors (i.e., all covariances are assumed to be equal)\nb.ar.min = 0.2 b.ar.max = 0.6\nb.cr.min = 0.05\nb.cr.max = 0.2\nK = c(2,4)\ndiff = c(1,2)\nsize = c(1,2,3)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Count the number of missing values in the selfratephys column [SEP] n_missing <- sum(is.na(data$selfratephys))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Count the number of missing values in the selfratephys column [SEP] col_order <- names(sort(col_scores, decreasing = FALSE))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Print the number of missing values [SEP] cat(\"Number of missing values in selfratephys column:\", n_missing) ## only 2 are missing - probably fine to impute a median value\ncat(\"Number of missing values in ucla3 column:\", n_missing) ## only 2 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_notlonely column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_receivesocialsupp column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_socialintegration column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_nosmoke column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_quitsmoke column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_notexcessalcohol column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_fluvax column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_physact column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_notobese column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_meds column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_lowpollution column:\", n_missing) ## only 3 are missing - probably fine to impute a median value", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Print the number of missing values [SEP] invisible(lapply(required\n, library\n, character.only = T))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Replace missing values with the median of the non-missing values [SEP] data$selfratephys[is.na(data$selfratephys)] <- median(data$selfratephys, na.rm = TRUE)\ndata$ucla3[is.na(data$ucla3)] <- median(data$ucla3, na.rm = TRUE)\ndata$estimateyrs_notlonely[is.na(data$estimateyrs_notlonely)] <- median(data$estimateyrs_notlonely, na.rm = TRUE)\ndata$estimateyrs_receivesocialsupp[is.na(data$estimateyrs_receivesocialsupp)] <- median(data$estimateyrs_receivesocialsupp, na.rm = TRUE)\ndata$estimateyrs_socialintegration[is.na(data$estimateyrs_socialintegration)] <- median(data$estimateyrs_socialintegration, na.rm = TRUE)\ndata$estimateyrs_nosmoke[is.na(data$estimateyrs_nosmoke)] <- median(data$estimateyrs_nosmoke, na.rm = TRUE)\ndata$estimateyrs_quitsmoke[is.na(data$estimateyrs_quitsmoke)] <- median(data$estimateyrs_quitsmoke, na.rm = TRUE)\ndata$estimateyrs_notexcessalcohol[is.na(data$estimateyrs_notexcessalcohol)] <- median(data$estimateyrs_notexcessalcohol, na.rm = TRUE)\ndata$estimateyrs_fluvax[is.na(data$estimateyrs_fluvax)] <- median(data$estimateyrs_fluvax, na.rm = TRUE)\ndata$estimateyrs_physact[is.na(data$estimateyrs_physact)] <- median(data$estimateyrs_physact, na.rm = TRUE)\ndata$estimateyrs_notobese[is.na(data$estimateyrs_notobese)] <- median(data$estimateyrs_notobese, na.rm = TRUE)\ndata$estimateyrs_meds[is.na(data$estimateyrs_meds)] <- median(data$estimateyrs_meds, na.rm = TRUE)\ndata$estimateyrs_lowpollution[is.na(data$estimateyrs_lowpollution)] <- median(data$estimateyrs_lowpollution, na.rm = TRUE)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Replace missing values with the median of the non-missing values [SEP] source(file=\"Performance.Cluster.MVAR.FE.R\")", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Create a histogram of age distribution [SEP] hist(data$\"Q20-age\", main = \"Age Distribution\", xlab = \"Age\")", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Create a histogram of age distribution [SEP] invisible(lapply(required\n, library\n, character.only = T))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Calculate the mean and median age without missing values [SEP] mean_age <- mean(data$\"Q20-age\", na.rm = TRUE)\nmedian_age <- median(data$\"Q20-age\", na.rm = TRUE)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Calculate the mean and median age without missing values [SEP] print(flextab, preview='docx')", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Sort the columns by their total score in descending order [SEP] col_order <- names(sort(col_scores, decreasing = FALSE))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Sort the columns by their total score in descending order [SEP] dir.create(\"Descriptives\", showWarnings = FALSE)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Create new dataframe that only includes US Nationally Representative sample by filtering , the data frame to include only rows from 2023 (dataframe named \"US_Rep_subset\") [SEP] US_Rep_subset <- data %>% filter(year(EndDate) == 2023)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Create new dataframe that only includes US Nationally Representative sample by filtering , the data frame to include only rows from 2023 (dataframe named \"US_Rep_subset\") [SEP] summary(myrsa)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Remove duplicates in this subset based on ProlificID column and keep all other variables [SEP] US_Rep_subset <- US_Rep_subset %>% distinct(ProlificID, .keep_all = TRUE)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Remove duplicates in this subset based on ProlificID column and keep all other variables [SEP] Exp1$scoringnum[Exp1$scoring==\"d\"] <- 0\nExp1$scoringnum[Exp1$scoring==\"p\"] <- 1", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Load packages [SEP] invisible(lapply(required\n, library\n, character.only = T))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Load packages [SEP] input <- list(\nmodel = path,\ncon.dat = \"1_Data/Conditioning_variables.RData\",\npart.dig = \"1_Data/PA12_Digital_Participation.RData\"\n)\nload(input$model)\nload(input$con.dat)\nload(input$part.dig)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "create folder for descriptive statistics [SEP] dir.create(\"Descriptives\", showWarnings = FALSE)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "create folder for descriptive statistics [SEP] dat <- dat[-which(dat$N < 10), ]\ndim(dat) # 18046 assessments\nlength(unique(dat$id)) # 286 participants (42 participants excluded)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "one participant has a typing error in the age variable (stated he was 2). Set to NA. [SEP] mst1[which(mst1$age == 2), \"age\"] <- NA", "target": 1, "target_options": ["no_match", "match"]}
{"input": "one participant has a typing error in the age variable (stated he was 2). Set to NA. [SEP] unigram_pre <- pre %>%\nunnest_tokens(word, abstract) %>%\ndplyr::count(word, sort = TRUE) %>%\nmutate(p = n / sum(n))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "remove the 12-year old participant [SEP] mst1 <- mst1[-which(mst1$age == 12),]", "target": 1, "target_options": ["no_match", "match"]}
{"input": "remove the 12-year old participant [SEP] no_outliers2 <- subset(df, DV2> (Q1 - 1.5*IQR2) & DV2< (Q3 + 1.5*IQR2))\nno_outliers3 <- subset(df, DV3> (Q1 - 1.5*IQR3) & DV3< (Q3 + 1.5*IQR3))", "target": 0, "target_options": ["no_match", "match"]}
{"input": " function to get normalized cell probabilites [SEP] calcNormProb <- function(x){\nnp <- x/cellStats(x,max)\nreturn(np)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": " function to get normalized cell probabilites [SEP] sum(pepen$SemanticRole==\"agent\")", "target": 0, "target_options": ["no_match", "match"]}
{"input": "clear workspace [SEP] ls()\nrm(list=ls())", "target": 1, "target_options": ["no_match", "match"]}
{"input": "clear workspace [SEP] US_Rep_subset <- US_Rep_subset %>% distinct(ProlificID, .keep_all = TRUE)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Upload the functions [SEP] source(file=\"Performance.Cluster.MVAR.FE.R\")", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Upload the functions [SEP] dat$abstract <- unlist(lapply(dat$abstract, function(x) gsub(' +',' ',x)))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Set the values to simulate data assuming a VAR(1) process [SEP] set.seed(123) # Set random seed\nN = c(20,60) # Number of participants\nT = c(50,100) # Number of time points\nP = 4 # Number of variables in VAR(1) models\ncor.Sigma = 0.2 # Set the covariance of the within-individuals errors (i.e., all covariances are assumed to be equal)\nb.ar.min = 0.2 b.ar.max = 0.6\nb.cr.min = 0.05\nb.cr.max = 0.2\nK = c(2,4)\ndiff = c(1,2)\nsize = c(1,2,3)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Set the values to simulate data assuming a VAR(1) process [SEP] prediction <- as.data.frame(prediction)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Set the number of replicates [SEP] R = 10 # Number of replicates", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Set the number of replicates [SEP] set.seed(123) # Set random seed\nN = c(20,60) # Number of participants\nT = c(50,100) # Number of time points\nP = 4 # Number of variables in VAR(1) models\ncor.Sigma = 0.2 # Set the covariance of the within-individuals errors (i.e., all covariances are assumed to be equal)\nb.ar.min = 0.2 b.ar.max = 0.6\nb.cr.min = 0.05\nb.cr.max = 0.2\nK = c(2,4)\ndiff = c(1,2)\nsize = c(1,2,3)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Set the number of blocks in block cross-validation [SEP] fold = 10", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Set the number of blocks in block cross-validation [SEP] indices <- caret::createDataPartition(data$happiness, p = 0.75, list=FALSE)\ntrain_dev_data <- data[indices,]\ntestdata <- data[-indices,]", "target": 0, "target_options": ["no_match", "match"]}
{"input": "## calculate similarity coefficient (mean of signif_line$similarity.scores) [SEP] mean(signif_line$similarity.scores[,2])", "target": 1, "target_options": ["no_match", "match"]}
{"input": "## calculate similarity coefficient (mean of signif_line$similarity.scores) [SEP] search_related <- function(word_vectors, selected_vector) {\nsimilarities <- word_vectors %*% selected_vector %>%\ntidy() %>%\nas_tibble() %>%\ndplyr::rename(token = .rownames,\nsimilarity = unrowname.x.)\nsimilarities %>%\narrange(-similarity)}", "target": 0, "target_options": ["no_match", "match"]}
{"input": "write extracted data to csv file in the parent folder [SEP] write.csv(extract_data_allbps, file = paste0(parentfolder, \"/\", name_video, \"_openpose_extracted_2p.csv\"),\nrow.names = F)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "write extracted data to csv file in the parent folder [SEP] hist(data$\"Q20-age\", main = \"Age Distribution\", xlab = \"Age\")", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Exclude participants who completed less than 10 surveys [SEP] dat <- dat[-which(dat$N < 10), ]\ndim(dat) # 18046 assessments\nlength(unique(dat$id)) # 286 participants (42 participants excluded)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Exclude participants who completed less than 10 surveys [SEP] setwd(\"C:/Users/abell/Documents/exp_prep/analysis/S1_data\")", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Run model and save output [SEP] runModels(pathfix)\nfitMplusPartial <- readModels(pathfix)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Run model and save output [SEP] sum(pepen$SemanticRole==\"agent\")", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Create folder [SEP] pathfix <- \"~/FinalModel\"\ndir.create(pathfix)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Create folder [SEP] prediction <- as.data.frame(prediction)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Store data in folder [SEP] prepareMplusData(df=DS14, filename=paste0(pathfix, \"/DS14dat.dat\"))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Store data in folder [SEP] cat(\"Number of missing values in selfratephys column:\", n_missing) ## only 2 are missing - probably fine to impute a median value\ncat(\"Number of missing values in ucla3 column:\", n_missing) ## only 2 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_notlonely column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_receivesocialsupp column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_socialintegration column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_nosmoke column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_quitsmoke column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_notexcessalcohol column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_fluvax column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_physact column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_notobese column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_meds column:\", n_missing) ## only 3 are missing - probably fine to impute a median value\ncat(\"Number of missing values in estimateyrs_lowpollution column:\", n_missing) ## only 3 are missing - probably fine to impute a median value", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Create column scoringnum that codes target completions numerically [SEP] Exp1$scoringnum[Exp1$scoring==\"d\"] <- 0\nExp1$scoringnum[Exp1$scoring==\"p\"] <- 1", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Create column scoringnum that codes target completions numerically [SEP] mst1[which(mst1$age == 2), \"age\"] <- NA", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Load data files\nChose file : \"PleioSimData.txt\" [SEP] dataSave <- read.table(file.choose(),h=T)\nhead(dataSave)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Load data files\nChose file : \"PleioSimData.txt\" [SEP] prediction <- as.data.frame(prediction)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "How many agents do we have in the data? [SEP] sum(pepen$SemanticRole==\"agent\")", "target": 1, "target_options": ["no_match", "match"]}
{"input": "How many agents do we have in the data? [SEP] mst1[which(mst1$age == 2), \"age\"] <- NA", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Preprocess data [SEP] df = preproc(df)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Preprocess data [SEP] unigram_pre <- pre %>%\nunnest_tokens(word, abstract) %>%\ndplyr::count(word, sort = TRUE) %>%\nmutate(p = n / sum(n))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "make sure it's a dataframe [SEP] df = as.data.frame(df)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "make sure it's a dataframe [SEP] stopwords_regex = paste(stopwords('SMART'), collapse = '\\\\b|\\\\b')\nstopwords_regex = paste0('\\\\b', stopwords_regex, '\\\\b')\ndat$abstract <- unlist(lapply(dat$abstract, function(x) stringr::str_replace_all(x, stopwords_regex, '')))", "target": 0, "target_options": ["no_match", "match"]}
{"input": "save data [SEP] save(df, file = paste(inputFolder, \"df_withIntercepts.rda\", sep = \"/\"))", "target": 1, "target_options": ["no_match", "match"]}
{"input": "save data [SEP] pathfix <- \"~/FinalModel\"\ndir.create(pathfix)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Verify the dimensions of the data frame. [SEP] dim(vrData)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Verify the dimensions of the data frame. [SEP] search_related <- function(word_vectors, selected_vector) {\nsimilarities <- word_vectors %*% selected_vector %>%\ntidy() %>%\nas_tibble() %>%\ndplyr::rename(token = .rownames,\nsimilarity = unrowname.x.)\nsimilarities %>%\narrange(-similarity)}", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Rename Data columns [SEP] names(ProcessedData) <- c(\"Identifier\", \"Stimuli\", \"Distance\", \"Encoding_Rating\", \"Encoding_DecisionTime\", \"DecisionTime\", \"Deviation\", \"sdDecisionTime\", \"sdDeviation\")", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Rename Data columns [SEP] summary(myrsa)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Load model and conditioning variables [SEP] input <- list(\nmodel = path,\ncon.dat = \"1_Data/Conditioning_variables.RData\",\npart.dig = \"1_Data/PA12_Digital_Participation.RData\"\n)\nload(input$model)\nload(input$con.dat)\nload(input$part.dig)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Load model and conditioning variables [SEP] search_related <- function(word_vectors, selected_vector) {\nsimilarities <- word_vectors %*% selected_vector %>%\ntidy() %>%\nas_tibble() %>%\ndplyr::rename(token = .rownames,\nsimilarity = unrowname.x.)\nsimilarities %>%\narrange(-similarity)}", "target": 0, "target_options": ["no_match", "match"]}
{"input": "DATA FILTERING [SEP] data = data %>% filter(filter_erspq == 2) %>% # 2 = keep the valid (not-empty) rows\nfilter(stem_soc == \"stem\" | stem_soc == \"soc\") %>% # include only STEM and SOC students\ndroplevels()# drop factor levels (for better display)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "DATA FILTERING [SEP] cor_matrix_half <- cor_matrix[upper.tri(cor_matrix)]\nmean(cor_matrix_half)\nsd(cor_matrix_half)\nhist(as.vector(cor_matrix_half), breaks=24, cex.axis=2) # Note: Novich et al. suppressed correlations of r<.4 in their visualisation\nheatmap(x = cor_matrix, symm = TRUE)", "target": 0, "target_options": ["no_match", "match"]}
{"input": "Put results into list to keep workspace tidy [SEP] results_corr <- list(res_corr_ext, res_corr_emo, res_corr_agr, res_corr_con, res_corr_ope, res_corr_mean)", "target": 1, "target_options": ["no_match", "match"]}
{"input": "Put results into list to keep workspace tidy [SEP] df = as.data.frame(df)", "target": 0, "target_options": ["no_match", "match"]}
