{"repo": "pbrod/numdifftools", "path": "src/numdifftools/fornberg.py", "func_name": "richardson", "original_string": "def richardson(vals, k, c=None):\n    \"\"\"Richardson extrapolation with parameter estimation\"\"\"\n    if c is None:\n        c = richardson_parameter(vals, k)\n    return vals[k] - (vals[k] - vals[k - 1]) / c", "language": "python", "code": "def richardson(vals, k, c=None):\n    \"\"\"Richardson extrapolation with parameter estimation\"\"\"\n    if c is None:\n        c = richardson_parameter(vals, k)\n    return vals[k] - (vals[k] - vals[k - 1]) / c", "code_tokens": ["def", "richardson", "(", "vals", ",", "k", ",", "c", "=", "None", ")", ":", "if", "c", "is", "None", ":", "c", "=", "richardson_parameter", "(", "vals", ",", "k", ")", "return", "vals", "[", "k", "]", "-", "(", "vals", "[", "k", "]", "-", "vals", "[", "k", "-", "1", "]", ")", "/", "c"], "docstring": "Richardson extrapolation with parameter estimation", "docstring_tokens": ["Richardson", "extrapolation", "with", "parameter", "estimation"], "sha": "2c88878df732c9c6629febea56e7a91fd898398d", "url": "https://github.com/pbrod/numdifftools/blob/2c88878df732c9c6629febea56e7a91fd898398d/src/numdifftools/fornberg.py#L253-L257", "partition": "train", "idx": 208177}
{"repo": "ncc-tools/python-pa-api", "path": "paapi/paapi.py", "func_name": "PaApi.get_all_jobtemplates", "original_string": "def get_all_jobtemplates(self):\n        \"\"\"\n        Retrieves the list of jobTemplates for the current realm.\n        \"\"\"\n        endpoint = self._build_url('jobTemplates', {\n            'paginationPageSize': self.PAGE_SIZE\n        })\n        data = self._query_api('GET', endpoint)\n        return data['results']", "language": "python", "code": "def get_all_jobtemplates(self):\n        \"\"\"\n        Retrieves the list of jobTemplates for the current realm.\n        \"\"\"\n        endpoint = self._build_url('jobTemplates', {\n            'paginationPageSize': self.PAGE_SIZE\n        })\n        data = self._query_api('GET', endpoint)\n        return data['results']", "code_tokens": ["def", "get_all_jobtemplates", "(", "self", ")", ":", "endpoint", "=", "self", ".", "_build_url", "(", "'jobTemplates'", ",", "{", "'paginationPageSize'", ":", "self", ".", "PAGE_SIZE", "}", ")", "data", "=", "self", ".", "_query_api", "(", "'GET'", ",", "endpoint", ")", "return", "data", "[", "'results'", "]"], "docstring": "Retrieves the list of jobTemplates for the current realm.", "docstring_tokens": ["Retrieves", "the", "list", "of", "jobTemplates", "for", "the", "current", "realm", "."], "sha": "a27481dd323d282d0f4457586198d9faec896f11", "url": "https://github.com/ncc-tools/python-pa-api/blob/a27481dd323d282d0f4457586198d9faec896f11/paapi/paapi.py#L151-L159", "partition": "train", "idx": 94127}
{"repo": "bcbio/bcbio-nextgen", "path": "bcbio/variation/population.py", "func_name": "prep_gemini_db", "original_string": "def prep_gemini_db(fnames, call_info, samples, extras):\n    \"\"\"Prepare a gemini database from VCF inputs prepared with snpEff.\n    \"\"\"\n    data = samples[0]\n    name, caller, is_batch = call_info\n    build_type = _get_build_type(fnames, samples, caller)\n    out_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"gemini\"))\n    gemini_vcf = get_multisample_vcf(fnames, name, caller, data)\n    # If we're building a gemini database, normalize the inputs\n    if build_type:\n        passonly = all(\"gemini_allvariants\" not in dd.get_tools_on(d) for d in samples)\n        gemini_vcf = normalize.normalize(gemini_vcf, data, passonly=passonly)\n        decomposed = True\n    else:\n        decomposed = False\n    ann_vcf = run_vcfanno(gemini_vcf, data, decomposed)\n    gemini_db = os.path.join(out_dir, \"%s-%s.db\" % (name, caller))\n    if ann_vcf and build_type and not utils.file_exists(gemini_db):\n        ped_file = create_ped_file(samples + extras, gemini_vcf)\n        # Original approach for hg19/GRCh37\n        if vcfanno.is_human(data, builds=[\"37\"]) and \"gemini_orig\" in build_type:\n            gemini_db = create_gemini_db_orig(gemini_vcf, data, gemini_db, ped_file)\n        else:\n            gemini_db = create_gemini_db(ann_vcf, data, gemini_db, ped_file)\n    # only pass along gemini_vcf_downstream if uniquely created here\n    if os.path.islink(gemini_vcf):\n        gemini_vcf = None\n    return [[(name, caller), {\"db\": gemini_db if utils.file_exists(gemini_db) else None,\n                              \"vcf\": ann_vcf or gemini_vcf,\n                              \"decomposed\": decomposed}]]", "language": "python", "code": "def prep_gemini_db(fnames, call_info, samples, extras):\n    \"\"\"Prepare a gemini database from VCF inputs prepared with snpEff.\n    \"\"\"\n    data = samples[0]\n    name, caller, is_batch = call_info\n    build_type = _get_build_type(fnames, samples, caller)\n    out_dir = utils.safe_makedir(os.path.join(data[\"dirs\"][\"work\"], \"gemini\"))\n    gemini_vcf = get_multisample_vcf(fnames, name, caller, data)\n    # If we're building a gemini database, normalize the inputs\n    if build_type:\n        passonly = all(\"gemini_allvariants\" not in dd.get_tools_on(d) for d in samples)\n        gemini_vcf = normalize.normalize(gemini_vcf, data, passonly=passonly)\n        decomposed = True\n    else:\n        decomposed = False\n    ann_vcf = run_vcfanno(gemini_vcf, data, decomposed)\n    gemini_db = os.path.join(out_dir, \"%s-%s.db\" % (name, caller))\n    if ann_vcf and build_type and not utils.file_exists(gemini_db):\n        ped_file = create_ped_file(samples + extras, gemini_vcf)\n        # Original approach for hg19/GRCh37\n        if vcfanno.is_human(data, builds=[\"37\"]) and \"gemini_orig\" in build_type:\n            gemini_db = create_gemini_db_orig(gemini_vcf, data, gemini_db, ped_file)\n        else:\n            gemini_db = create_gemini_db(ann_vcf, data, gemini_db, ped_file)\n    # only pass along gemini_vcf_downstream if uniquely created here\n    if os.path.islink(gemini_vcf):\n        gemini_vcf = None\n    return [[(name, caller), {\"db\": gemini_db if utils.file_exists(gemini_db) else None,\n                              \"vcf\": ann_vcf or gemini_vcf,\n                              \"decomposed\": decomposed}]]", "code_tokens": ["def", "prep_gemini_db", "(", "fnames", ",", "call_info", ",", "samples", ",", "extras", ")", ":", "data", "=", "samples", "[", "0", "]", "name", ",", "caller", ",", "is_batch", "=", "call_info", "build_type", "=", "_get_build_type", "(", "fnames", ",", "samples", ",", "caller", ")", "out_dir", "=", "utils", ".", "safe_makedir", "(", "os", ".", "path", ".", "join", "(", "data", "[", "\"dirs\"", "]", "[", "\"work\"", "]", ",", "\"gemini\"", ")", ")", "gemini_vcf", "=", "get_multisample_vcf", "(", "fnames", ",", "name", ",", "caller", ",", "data", ")", "# If we're building a gemini database, normalize the inputs", "if", "build_type", ":", "passonly", "=", "all", "(", "\"gemini_allvariants\"", "not", "in", "dd", ".", "get_tools_on", "(", "d", ")", "for", "d", "in", "samples", ")", "gemini_vcf", "=", "normalize", ".", "normalize", "(", "gemini_vcf", ",", "data", ",", "passonly", "=", "passonly", ")", "decomposed", "=", "True", "else", ":", "decomposed", "=", "False", "ann_vcf", "=", "run_vcfanno", "(", "gemini_vcf", ",", "data", ",", "decomposed", ")", "gemini_db", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"%s-%s.db\"", "%", "(", "name", ",", "caller", ")", ")", "if", "ann_vcf", "and", "build_type", "and", "not", "utils", ".", "file_exists", "(", "gemini_db", ")", ":", "ped_file", "=", "create_ped_file", "(", "samples", "+", "extras", ",", "gemini_vcf", ")", "# Original approach for hg19/GRCh37", "if", "vcfanno", ".", "is_human", "(", "data", ",", "builds", "=", "[", "\"37\"", "]", ")", "and", "\"gemini_orig\"", "in", "build_type", ":", "gemini_db", "=", "create_gemini_db_orig", "(", "gemini_vcf", ",", "data", ",", "gemini_db", ",", "ped_file", ")", "else", ":", "gemini_db", "=", "create_gemini_db", "(", "ann_vcf", ",", "data", ",", "gemini_db", ",", "ped_file", ")", "# only pass along gemini_vcf_downstream if uniquely created here", "if", "os", ".", "path", ".", "islink", "(", "gemini_vcf", ")", ":", "gemini_vcf", "=", "None", "return", "[", "[", "(", "name", ",", "caller", ")", ",", "{", "\"db\"", ":", "gemini_db", "if", "utils", ".", "file_exists", "(", "gemini_db", ")", "else", "None", ",", "\"vcf\"", ":", "ann_vcf", "or", "gemini_vcf", ",", "\"decomposed\"", ":", "decomposed", "}", "]", "]"], "docstring": "Prepare a gemini database from VCF inputs prepared with snpEff.", "docstring_tokens": ["Prepare", "a", "gemini", "database", "from", "VCF", "inputs", "prepared", "with", "snpEff", "."], "sha": "6a9348c0054ccd5baffd22f1bb7d0422f6978b20", "url": "https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/variation/population.py#L25-L54", "partition": "train", "idx": 218905}
{"repo": "inveniosoftware/invenio-admin", "path": "invenio_admin/ext.py", "func_name": "_AdminState.load_entry_point_group", "original_string": "def load_entry_point_group(self, entry_point_group):\n        \"\"\"Load administration interface from entry point group.\n\n        :param str entry_point_group: Name of the entry point group.\n        \"\"\"\n        for ep in pkg_resources.iter_entry_points(group=entry_point_group):\n            admin_ep = dict(ep.load())\n            keys = tuple(\n                k in admin_ep for k in ('model', 'modelview', 'view_class'))\n\n            if keys == (False, False, True):\n                self.register_view(\n                    admin_ep.pop('view_class'),\n                    *admin_ep.pop('args', []),\n                    **admin_ep.pop('kwargs', {})\n                )\n            elif keys == (True, True, False):\n                warnings.warn(\n                    'Usage of model and modelview kwargs are deprecated in '\n                    'favor of view_class, args and kwargs.',\n                    PendingDeprecationWarning\n                )\n                self.register_view(\n                    admin_ep.pop('modelview'),\n                    admin_ep.pop('model'),\n                    admin_ep.pop('session', db.session),\n                    **admin_ep\n                )\n            else:\n                raise Exception(\n                    'Admin entry point dictionary must contain '\n                    'either \"view_class\" OR \"model\" and \"modelview\" keys.')", "language": "python", "code": "def load_entry_point_group(self, entry_point_group):\n        \"\"\"Load administration interface from entry point group.\n\n        :param str entry_point_group: Name of the entry point group.\n        \"\"\"\n        for ep in pkg_resources.iter_entry_points(group=entry_point_group):\n            admin_ep = dict(ep.load())\n            keys = tuple(\n                k in admin_ep for k in ('model', 'modelview', 'view_class'))\n\n            if keys == (False, False, True):\n                self.register_view(\n                    admin_ep.pop('view_class'),\n                    *admin_ep.pop('args', []),\n                    **admin_ep.pop('kwargs', {})\n                )\n            elif keys == (True, True, False):\n                warnings.warn(\n                    'Usage of model and modelview kwargs are deprecated in '\n                    'favor of view_class, args and kwargs.',\n                    PendingDeprecationWarning\n                )\n                self.register_view(\n                    admin_ep.pop('modelview'),\n                    admin_ep.pop('model'),\n                    admin_ep.pop('session', db.session),\n                    **admin_ep\n                )\n            else:\n                raise Exception(\n                    'Admin entry point dictionary must contain '\n                    'either \"view_class\" OR \"model\" and \"modelview\" keys.')", "code_tokens": ["def", "load_entry_point_group", "(", "self", ",", "entry_point_group", ")", ":", "for", "ep", "in", "pkg_resources", ".", "iter_entry_points", "(", "group", "=", "entry_point_group", ")", ":", "admin_ep", "=", "dict", "(", "ep", ".", "load", "(", ")", ")", "keys", "=", "tuple", "(", "k", "in", "admin_ep", "for", "k", "in", "(", "'model'", ",", "'modelview'", ",", "'view_class'", ")", ")", "if", "keys", "==", "(", "False", ",", "False", ",", "True", ")", ":", "self", ".", "register_view", "(", "admin_ep", ".", "pop", "(", "'view_class'", ")", ",", "*", "admin_ep", ".", "pop", "(", "'args'", ",", "[", "]", ")", ",", "*", "*", "admin_ep", ".", "pop", "(", "'kwargs'", ",", "{", "}", ")", ")", "elif", "keys", "==", "(", "True", ",", "True", ",", "False", ")", ":", "warnings", ".", "warn", "(", "'Usage of model and modelview kwargs are deprecated in '", "'favor of view_class, args and kwargs.'", ",", "PendingDeprecationWarning", ")", "self", ".", "register_view", "(", "admin_ep", ".", "pop", "(", "'modelview'", ")", ",", "admin_ep", ".", "pop", "(", "'model'", ")", ",", "admin_ep", ".", "pop", "(", "'session'", ",", "db", ".", "session", ")", ",", "*", "*", "admin_ep", ")", "else", ":", "raise", "Exception", "(", "'Admin entry point dictionary must contain '", "'either \"view_class\" OR \"model\" and \"modelview\" keys.'", ")"], "docstring": "Load administration interface from entry point group.\n\n        :param str entry_point_group: Name of the entry point group.", "docstring_tokens": ["Load", "administration", "interface", "from", "entry", "point", "group", "."], "sha": "b5ff8f7de66d1d6b67efc9f81ff094eb2428f969", "url": "https://github.com/inveniosoftware/invenio-admin/blob/b5ff8f7de66d1d6b67efc9f81ff094eb2428f969/invenio_admin/ext.py#L53-L84", "partition": "train", "idx": 68439}
{"repo": "manns/pyspread", "path": "pyspread/src/lib/xrect.py", "func_name": "RotoRect.get_vec_tb", "original_string": "def get_vec_tb(self):\n        \"\"\"Returns vector from top to bottom\"\"\"\n\n        return self.height * self.sin_a(), self.height * self.cos_a()", "language": "python", "code": "def get_vec_tb(self):\n        \"\"\"Returns vector from top to bottom\"\"\"\n\n        return self.height * self.sin_a(), self.height * self.cos_a()", "code_tokens": ["def", "get_vec_tb", "(", "self", ")", ":", "return", "self", ".", "height", "*", "self", ".", "sin_a", "(", ")", ",", "self", ".", "height", "*", "self", ".", "cos_a", "(", ")"], "docstring": "Returns vector from top to bottom", "docstring_tokens": ["Returns", "vector", "from", "top", "to", "bottom"], "sha": "0e2fd44c2e0f06605efc3058c20a43a8c1f9e7e0", "url": "https://github.com/manns/pyspread/blob/0e2fd44c2e0f06605efc3058c20a43a8c1f9e7e0/pyspread/src/lib/xrect.py#L273-L276", "partition": "train", "idx": 231310}
{"repo": "biocore/burrito-fillings", "path": "bfillings/muscle_v38.py", "func_name": "align_and_build_tree", "original_string": "def align_and_build_tree(seqs, moltype, best_tree=False, params=None):\n    \"\"\"Returns an alignment and a tree from Sequences object seqs.\n\n    seqs: a cogent.core.alignment.SequenceCollection object, or data that can\n    be used to build one.\n\n    moltype: cogent.core.moltype.MolType object\n\n    best_tree: if True (default:False), uses a slower but more accurate\n    algorithm to build the tree.\n\n    params: dict of parameters to pass in to the Muscle app controller.\n\n    The result will be a tuple containing a cogent.core.alignment.Alignment\n    and a cogent.core.tree.PhyloNode object (or None for the alignment\n    and/or tree if either fails).\n    \"\"\"\n    aln = align_unaligned_seqs(seqs, moltype=moltype, params=params)\n    tree = build_tree_from_alignment(aln, moltype, best_tree, params)\n    return {'Align':aln, 'Tree':tree}", "language": "python", "code": "def align_and_build_tree(seqs, moltype, best_tree=False, params=None):\n    \"\"\"Returns an alignment and a tree from Sequences object seqs.\n\n    seqs: a cogent.core.alignment.SequenceCollection object, or data that can\n    be used to build one.\n\n    moltype: cogent.core.moltype.MolType object\n\n    best_tree: if True (default:False), uses a slower but more accurate\n    algorithm to build the tree.\n\n    params: dict of parameters to pass in to the Muscle app controller.\n\n    The result will be a tuple containing a cogent.core.alignment.Alignment\n    and a cogent.core.tree.PhyloNode object (or None for the alignment\n    and/or tree if either fails).\n    \"\"\"\n    aln = align_unaligned_seqs(seqs, moltype=moltype, params=params)\n    tree = build_tree_from_alignment(aln, moltype, best_tree, params)\n    return {'Align':aln, 'Tree':tree}", "code_tokens": ["def", "align_and_build_tree", "(", "seqs", ",", "moltype", ",", "best_tree", "=", "False", ",", "params", "=", "None", ")", ":", "aln", "=", "align_unaligned_seqs", "(", "seqs", ",", "moltype", "=", "moltype", ",", "params", "=", "params", ")", "tree", "=", "build_tree_from_alignment", "(", "aln", ",", "moltype", ",", "best_tree", ",", "params", ")", "return", "{", "'Align'", ":", "aln", ",", "'Tree'", ":", "tree", "}"], "docstring": "Returns an alignment and a tree from Sequences object seqs.\n\n    seqs: a cogent.core.alignment.SequenceCollection object, or data that can\n    be used to build one.\n\n    moltype: cogent.core.moltype.MolType object\n\n    best_tree: if True (default:False), uses a slower but more accurate\n    algorithm to build the tree.\n\n    params: dict of parameters to pass in to the Muscle app controller.\n\n    The result will be a tuple containing a cogent.core.alignment.Alignment\n    and a cogent.core.tree.PhyloNode object (or None for the alignment\n    and/or tree if either fails).", "docstring_tokens": ["Returns", "an", "alignment", "and", "a", "tree", "from", "Sequences", "object", "seqs", "."], "sha": "02ab71a46119b40793bd56a4ae00ca15f6dc3329", "url": "https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/muscle_v38.py#L572-L591", "partition": "train", "idx": 73979}
{"repo": "pre-commit/pre-commit", "path": "pre_commit/commands/autoupdate.py", "func_name": "autoupdate", "original_string": "def autoupdate(config_file, store, tags_only, repos=()):\n    \"\"\"Auto-update the pre-commit config to the latest versions of repos.\"\"\"\n    migrate_config(config_file, quiet=True)\n    retv = 0\n    output_repos = []\n    changed = False\n\n    input_config = load_config(config_file)\n\n    for repo_config in input_config['repos']:\n        if (\n            repo_config['repo'] in {LOCAL, META} or\n            # Skip updating any repo_configs that aren't for the specified repo\n            repos and repo_config['repo'] not in repos\n        ):\n            output_repos.append(repo_config)\n            continue\n        output.write('Updating {}...'.format(repo_config['repo']))\n        try:\n            new_repo_config = _update_repo(repo_config, store, tags_only)\n        except RepositoryCannotBeUpdatedError as error:\n            output.write_line(error.args[0])\n            output_repos.append(repo_config)\n            retv = 1\n            continue\n\n        if new_repo_config['rev'] != repo_config['rev']:\n            changed = True\n            output.write_line(\n                'updating {} -> {}.'.format(\n                    repo_config['rev'], new_repo_config['rev'],\n                ),\n            )\n            output_repos.append(new_repo_config)\n        else:\n            output.write_line('already up to date.')\n            output_repos.append(repo_config)\n\n    if changed:\n        output_config = input_config.copy()\n        output_config['repos'] = output_repos\n        _write_new_config_file(config_file, output_config)\n\n    return retv", "language": "python", "code": "def autoupdate(config_file, store, tags_only, repos=()):\n    \"\"\"Auto-update the pre-commit config to the latest versions of repos.\"\"\"\n    migrate_config(config_file, quiet=True)\n    retv = 0\n    output_repos = []\n    changed = False\n\n    input_config = load_config(config_file)\n\n    for repo_config in input_config['repos']:\n        if (\n            repo_config['repo'] in {LOCAL, META} or\n            # Skip updating any repo_configs that aren't for the specified repo\n            repos and repo_config['repo'] not in repos\n        ):\n            output_repos.append(repo_config)\n            continue\n        output.write('Updating {}...'.format(repo_config['repo']))\n        try:\n            new_repo_config = _update_repo(repo_config, store, tags_only)\n        except RepositoryCannotBeUpdatedError as error:\n            output.write_line(error.args[0])\n            output_repos.append(repo_config)\n            retv = 1\n            continue\n\n        if new_repo_config['rev'] != repo_config['rev']:\n            changed = True\n            output.write_line(\n                'updating {} -> {}.'.format(\n                    repo_config['rev'], new_repo_config['rev'],\n                ),\n            )\n            output_repos.append(new_repo_config)\n        else:\n            output.write_line('already up to date.')\n            output_repos.append(repo_config)\n\n    if changed:\n        output_config = input_config.copy()\n        output_config['repos'] = output_repos\n        _write_new_config_file(config_file, output_config)\n\n    return retv", "code_tokens": ["def", "autoupdate", "(", "config_file", ",", "store", ",", "tags_only", ",", "repos", "=", "(", ")", ")", ":", "migrate_config", "(", "config_file", ",", "quiet", "=", "True", ")", "retv", "=", "0", "output_repos", "=", "[", "]", "changed", "=", "False", "input_config", "=", "load_config", "(", "config_file", ")", "for", "repo_config", "in", "input_config", "[", "'repos'", "]", ":", "if", "(", "repo_config", "[", "'repo'", "]", "in", "{", "LOCAL", ",", "META", "}", "or", "# Skip updating any repo_configs that aren't for the specified repo", "repos", "and", "repo_config", "[", "'repo'", "]", "not", "in", "repos", ")", ":", "output_repos", ".", "append", "(", "repo_config", ")", "continue", "output", ".", "write", "(", "'Updating {}...'", ".", "format", "(", "repo_config", "[", "'repo'", "]", ")", ")", "try", ":", "new_repo_config", "=", "_update_repo", "(", "repo_config", ",", "store", ",", "tags_only", ")", "except", "RepositoryCannotBeUpdatedError", "as", "error", ":", "output", ".", "write_line", "(", "error", ".", "args", "[", "0", "]", ")", "output_repos", ".", "append", "(", "repo_config", ")", "retv", "=", "1", "continue", "if", "new_repo_config", "[", "'rev'", "]", "!=", "repo_config", "[", "'rev'", "]", ":", "changed", "=", "True", "output", ".", "write_line", "(", "'updating {} -> {}.'", ".", "format", "(", "repo_config", "[", "'rev'", "]", ",", "new_repo_config", "[", "'rev'", "]", ",", ")", ",", ")", "output_repos", ".", "append", "(", "new_repo_config", ")", "else", ":", "output", ".", "write_line", "(", "'already up to date.'", ")", "output_repos", ".", "append", "(", "repo_config", ")", "if", "changed", ":", "output_config", "=", "input_config", ".", "copy", "(", ")", "output_config", "[", "'repos'", "]", "=", "output_repos", "_write_new_config_file", "(", "config_file", ",", "output_config", ")", "return", "retv"], "docstring": "Auto-update the pre-commit config to the latest versions of repos.", "docstring_tokens": ["Auto", "-", "update", "the", "pre", "-", "commit", "config", "to", "the", "latest", "versions", "of", "repos", "."], "sha": "72f98d26e690da11dc2e41861d14c58eb21930cb", "url": "https://github.com/pre-commit/pre-commit/blob/72f98d26e690da11dc2e41861d14c58eb21930cb/pre_commit/commands/autoupdate.py#L117-L160", "partition": "train", "idx": 138349}
{"repo": "Opentrons/opentrons", "path": "api/src/opentrons/drivers/smoothie_drivers/driver_3_0.py", "func_name": "SmoothieDriver_3_0_0.set_active_current", "original_string": "def set_active_current(self, settings):\n        '''\n        Sets the amperage of each motor for when it is activated by driver.\n        Values are initialized from the `robot_config.high_current` values,\n        and can then be changed through this method by other parts of the API.\n\n        For example, `Pipette` setting the active-current of it's pipette,\n        depending on what model pipette it is, and what action it is performing\n\n        settings\n            Dict with axes as valies (e.g.: 'X', 'Y', 'Z', 'A', 'B', or 'C')\n            and floating point number for current (generally between 0.1 and 2)\n        '''\n        self._active_current_settings['now'].update(settings)\n\n        # if an axis specified in the `settings` is currently active,\n        # reset it's current to the new active-current value\n        active_axes_to_update = {\n            axis: amperage\n            for axis, amperage in self._active_current_settings['now'].items()\n            if self._active_axes.get(axis) is True\n            if self.current[axis] != amperage\n        }\n        if active_axes_to_update:\n            self._save_current(active_axes_to_update, axes_active=True)", "language": "python", "code": "def set_active_current(self, settings):\n        '''\n        Sets the amperage of each motor for when it is activated by driver.\n        Values are initialized from the `robot_config.high_current` values,\n        and can then be changed through this method by other parts of the API.\n\n        For example, `Pipette` setting the active-current of it's pipette,\n        depending on what model pipette it is, and what action it is performing\n\n        settings\n            Dict with axes as valies (e.g.: 'X', 'Y', 'Z', 'A', 'B', or 'C')\n            and floating point number for current (generally between 0.1 and 2)\n        '''\n        self._active_current_settings['now'].update(settings)\n\n        # if an axis specified in the `settings` is currently active,\n        # reset it's current to the new active-current value\n        active_axes_to_update = {\n            axis: amperage\n            for axis, amperage in self._active_current_settings['now'].items()\n            if self._active_axes.get(axis) is True\n            if self.current[axis] != amperage\n        }\n        if active_axes_to_update:\n            self._save_current(active_axes_to_update, axes_active=True)", "code_tokens": ["def", "set_active_current", "(", "self", ",", "settings", ")", ":", "self", ".", "_active_current_settings", "[", "'now'", "]", ".", "update", "(", "settings", ")", "# if an axis specified in the `settings` is currently active,", "# reset it's current to the new active-current value", "active_axes_to_update", "=", "{", "axis", ":", "amperage", "for", "axis", ",", "amperage", "in", "self", ".", "_active_current_settings", "[", "'now'", "]", ".", "items", "(", ")", "if", "self", ".", "_active_axes", ".", "get", "(", "axis", ")", "is", "True", "if", "self", ".", "current", "[", "axis", "]", "!=", "amperage", "}", "if", "active_axes_to_update", ":", "self", ".", "_save_current", "(", "active_axes_to_update", ",", "axes_active", "=", "True", ")"], "docstring": "Sets the amperage of each motor for when it is activated by driver.\n        Values are initialized from the `robot_config.high_current` values,\n        and can then be changed through this method by other parts of the API.\n\n        For example, `Pipette` setting the active-current of it's pipette,\n        depending on what model pipette it is, and what action it is performing\n\n        settings\n            Dict with axes as valies (e.g.: 'X', 'Y', 'Z', 'A', 'B', or 'C')\n            and floating point number for current (generally between 0.1 and 2)", "docstring_tokens": ["Sets", "the", "amperage", "of", "each", "motor", "for", "when", "it", "is", "activated", "by", "driver", ".", "Values", "are", "initialized", "from", "the", "robot_config", ".", "high_current", "values", "and", "can", "then", "be", "changed", "through", "this", "method", "by", "other", "parts", "of", "the", "API", "."], "sha": "a7c15cc2636ecb64ab56c7edc1d8a57163aaeadf", "url": "https://github.com/Opentrons/opentrons/blob/a7c15cc2636ecb64ab56c7edc1d8a57163aaeadf/api/src/opentrons/drivers/smoothie_drivers/driver_3_0.py#L630-L654", "partition": "train", "idx": 198206}
{"repo": "DistrictDataLabs/yellowbrick", "path": "yellowbrick/classifier/rocauc.py", "func_name": "ROCAUC._score_macro_average", "original_string": "def _score_macro_average(self, n_classes):\n        \"\"\"\n        Compute the macro average scores for the ROCAUC curves.\n        \"\"\"\n        # Gather all FPRs\n        all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n        avg_tpr = np.zeros_like(all_fpr)\n\n        # Compute the averages per class\n        for i in range(n_classes):\n            avg_tpr += interp(all_fpr, self.fpr[i], self.tpr[i])\n\n        # Finalize the average\n        avg_tpr /= n_classes\n\n        # Store the macro averages\n        self.fpr[MACRO] = all_fpr\n        self.tpr[MACRO] = avg_tpr\n        self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])", "language": "python", "code": "def _score_macro_average(self, n_classes):\n        \"\"\"\n        Compute the macro average scores for the ROCAUC curves.\n        \"\"\"\n        # Gather all FPRs\n        all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n        avg_tpr = np.zeros_like(all_fpr)\n\n        # Compute the averages per class\n        for i in range(n_classes):\n            avg_tpr += interp(all_fpr, self.fpr[i], self.tpr[i])\n\n        # Finalize the average\n        avg_tpr /= n_classes\n\n        # Store the macro averages\n        self.fpr[MACRO] = all_fpr\n        self.tpr[MACRO] = avg_tpr\n        self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])", "code_tokens": ["def", "_score_macro_average", "(", "self", ",", "n_classes", ")", ":", "# Gather all FPRs", "all_fpr", "=", "np", ".", "unique", "(", "np", ".", "concatenate", "(", "[", "self", ".", "fpr", "[", "i", "]", "for", "i", "in", "range", "(", "n_classes", ")", "]", ")", ")", "avg_tpr", "=", "np", ".", "zeros_like", "(", "all_fpr", ")", "# Compute the averages per class", "for", "i", "in", "range", "(", "n_classes", ")", ":", "avg_tpr", "+=", "interp", "(", "all_fpr", ",", "self", ".", "fpr", "[", "i", "]", ",", "self", ".", "tpr", "[", "i", "]", ")", "# Finalize the average", "avg_tpr", "/=", "n_classes", "# Store the macro averages", "self", ".", "fpr", "[", "MACRO", "]", "=", "all_fpr", "self", ".", "tpr", "[", "MACRO", "]", "=", "avg_tpr", "self", ".", "roc_auc", "[", "MACRO", "]", "=", "auc", "(", "self", ".", "fpr", "[", "MACRO", "]", ",", "self", ".", "tpr", "[", "MACRO", "]", ")"], "docstring": "Compute the macro average scores for the ROCAUC curves.", "docstring_tokens": ["Compute", "the", "macro", "average", "scores", "for", "the", "ROCAUC", "curves", "."], "sha": "59b67236a3862c73363e8edad7cd86da5b69e3b2", "url": "https://github.com/DistrictDataLabs/yellowbrick/blob/59b67236a3862c73363e8edad7cd86da5b69e3b2/yellowbrick/classifier/rocauc.py#L374-L392", "partition": "train", "idx": 130442}
{"repo": "jimzhan/pyx", "path": "rex/debug/function.py", "func_name": "timeit", "original_string": "def timeit(func):\n    \"\"\"\n    Decorator that logs the cost time of a function.\n    \"\"\"\n    @wraps(func)\n    def wrapped_func(*args, **kwargs):\n        start  = timer()\n        result = func(*args, **kwargs)\n        cost   = timer() - start\n        logger.debug('<method: %s> finished in %2.2f sec' % (func.__name__, cost))\n        return result\n    return wrapped_func", "language": "python", "code": "def timeit(func):\n    \"\"\"\n    Decorator that logs the cost time of a function.\n    \"\"\"\n    @wraps(func)\n    def wrapped_func(*args, **kwargs):\n        start  = timer()\n        result = func(*args, **kwargs)\n        cost   = timer() - start\n        logger.debug('<method: %s> finished in %2.2f sec' % (func.__name__, cost))\n        return result\n    return wrapped_func", "code_tokens": ["def", "timeit", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapped_func", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "start", "=", "timer", "(", ")", "result", "=", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "cost", "=", "timer", "(", ")", "-", "start", "logger", ".", "debug", "(", "'<method: %s> finished in %2.2f sec'", "%", "(", "func", ".", "__name__", ",", "cost", ")", ")", "return", "result", "return", "wrapped_func"], "docstring": "Decorator that logs the cost time of a function.", "docstring_tokens": ["Decorator", "that", "logs", "the", "cost", "time", "of", "a", "function", "."], "sha": "819e8251323a7923e196c0c438aa8524f5aaee6e", "url": "https://github.com/jimzhan/pyx/blob/819e8251323a7923e196c0c438aa8524f5aaee6e/rex/debug/function.py#L36-L47", "partition": "train", "idx": 89284}
