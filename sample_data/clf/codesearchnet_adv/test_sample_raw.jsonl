{"url": "https://github.com/inspirehep/harvesting-kit/blob/33a7f8aa9dade1d863110c6d8b27dfd955cb471f/harvestingkit/etree_utils.py#L41-L57", "sha": "33a7f8aa9dade1d863110c6d8b27dfd955cb471f", "docstring_summary": "Strip out namespace data from an ElementTree.", "language": "python", "parameters": "(root)", "return_statement": "", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ")", ":", "try", ":", "arg_0", ".", "tag", "=", "arg_0", ".", "tag", ".", "split", "(", "'}'", ")", "[", "1", "]", "except", "IndexError", ":", "pass", "for", "arg_2", "in", "arg_0", ".", "getchildren", "(", ")", ":", "Func", "(", "arg_2", ")"], "function": "def Func(arg_0):\n    \"\"\"Strip out namespace data from an ElementTree.\n\n    This function is recursive and will traverse all\n    subnodes to the root element\n\n    @param root: the root element\n\n    @return: the same root element, minus namespace\n    \"\"\"\n    try:\n        arg_0.tag = arg_0.tag.split('}')[1]\n    except IndexError:\n        pass\n\n    for arg_2 in arg_0.getchildren():\n        Func(arg_2)", "path": "harvestingkit/etree_utils.py", "identifier": "strip_xml_namespace", "docstring": "Strip out namespace data from an ElementTree.\n\n    This function is recursive and will traverse all\n    subnodes to the root element\n\n    @param root: the root element\n\n    @return: the same root element, minus namespace", "docstring_tokens": ["Strip", "out", "namespace", "data", "from", "an", "ElementTree", "."], "nwo": "inspirehep/harvesting-kit", "score": 0.23137166388621372, "idx": 273976}
{"url": "https://github.com/opencast/pyCA/blob/c89b168d4780d157e1b3f7676628c1b131956a88/pyca/utils.py#L217-L231", "sha": "c89b168d4780d157e1b3f7676628c1b131956a88", "docstring_summary": "Update the current agent state in opencast.", "language": "python", "parameters": "()", "return_statement": "", "argument_list": "", "function_tokens": ["def", "Func", "(", ")", ":", "configure_service", "(", "'capture.admin'", ")", "arg_0", "=", "'idle'", "if", "get_service_status", "(", "db", ".", "Service", ".", "SCHEDULE", ")", "==", "db", ".", "ServiceStatus", ".", "STOPPED", ":", "arg_0", "=", "'offline'", "elif", "get_service_status", "(", "db", ".", "Service", ".", "CAPTURE", ")", "==", "db", ".", "ServiceStatus", ".", "BUSY", ":", "arg_0", "=", "'capturing'", "elif", "get_service_status", "(", "db", ".", "Service", ".", "INGEST", ")", "==", "db", ".", "ServiceStatus", ".", "BUSY", ":", "arg_0", "=", "'uploading'", "register_ca", "(", "arg_0", "=", "arg_0", ")"], "function": "def Func():\n    '''Update the current agent state in opencast.\n    '''\n    configure_service('capture.admin')\n    arg_0 = 'idle'\n\n    # Determine reported agent state with priority list\n    if get_service_status(db.Service.SCHEDULE) == db.ServiceStatus.STOPPED:\n        arg_0 = 'offline'\n    elif get_service_status(db.Service.CAPTURE) == db.ServiceStatus.BUSY:\n        arg_0 = 'capturing'\n    elif get_service_status(db.Service.INGEST) == db.ServiceStatus.BUSY:\n        arg_0 = 'uploading'\n\n    register_ca(arg_0=arg_0)", "path": "pyca/utils.py", "identifier": "update_agent_state", "docstring": "Update the current agent state in opencast.", "docstring_tokens": ["Update", "the", "current", "agent", "state", "in", "opencast", "."], "nwo": "opencast/pyCA", "score": 0.38750565733100095, "idx": 268664}
{"url": "https://github.com/ferhatelmas/sexmachine/blob/85d33bb47ccc017676e69788750f116e391f52db/sexmachine/detector.py#L74-L86", "sha": "85d33bb47ccc017676e69788750f116e391f52db", "docstring_summary": "Finds the most popular gender for the given name counting by given counter", "language": "python", "parameters": "(self, name, counter)", "return_statement": "return best if max_count > 0 else self.unknown_value", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ",", "arg_1", ",", "arg_2", ")", ":", "if", "arg_1", "not", "in", "arg_0", ".", "names", ":", "return", "arg_0", ".", "unknown_value", "arg_3", ",", "arg_4", "=", "(", "0", ",", "0", ")", "arg_5", "=", "arg_0", ".", "names", "[", "arg_1", "]", ".", "keys", "(", ")", "[", "0", "]", "for", "arg_6", ",", "arg_7", "in", "arg_0", ".", "names", "[", "arg_1", "]", ".", "items", "(", ")", ":", "arg_8", ",", "arg_9", "=", "arg_2", "(", "arg_7", ")", "if", "arg_8", ">", "arg_3", "or", "(", "arg_8", "==", "arg_3", "and", "arg_9", ">", "arg_4", ")", ":", "arg_3", ",", "arg_4", ",", "arg_5", "=", "arg_8", ",", "arg_9", ",", "arg_6", "return", "arg_5", "if", "arg_3", ">", "0", "else", "arg_0", ".", "unknown_value"], "function": "def Func(arg_0, arg_1, arg_2):\n        \"\"\"Finds the most popular gender for the given name counting by given counter\"\"\"\n        if arg_1 not in arg_0.names:\n            return arg_0.unknown_value\n\n        arg_3, arg_4 = (0, 0)\n        arg_5 = arg_0.names[arg_1].keys()[0]\n        for arg_6, arg_7 in arg_0.names[arg_1].items():\n            arg_8, arg_9 = arg_2(arg_7)\n            if arg_8 > arg_3 or (arg_8 == arg_3 and arg_9 > arg_4):\n                arg_3, arg_4, arg_5 = arg_8, arg_9, arg_6\n\n        return arg_5 if arg_3 > 0 else arg_0.unknown_value", "path": "sexmachine/detector.py", "identifier": "Detector._most_popular_gender", "docstring": "Finds the most popular gender for the given name counting by given counter", "docstring_tokens": ["Finds", "the", "most", "popular", "gender", "for", "the", "given", "name", "counting", "by", "given", "counter"], "nwo": "ferhatelmas/sexmachine", "score": 0.5262758950020962, "idx": 265551}
{"url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L342-L352", "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "docstring_summary": "Delete pool.", "language": "python", "parameters": "(name)", "return_statement": "", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ")", ":", "try", ":", "arg_1", "=", "pool_api", ".", "Func", "(", "arg_0", "=", "arg_0", ")", "except", "AirflowException", "as", "err", ":", "_log", ".", "error", "(", "err", ")", "arg_2", "=", "jsonify", "(", "error", "=", "\"{}\"", ".", "format", "(", "err", ")", ")", "arg_2", ".", "status_code", "=", "err", ".", "status_code", "return", "arg_2", "else", ":", "return", "jsonify", "(", "arg_1", ".", "to_json", "(", ")", ")"], "function": "def Func(arg_0):\n    \"\"\"Delete pool.\"\"\"\n    try:\n        arg_1 = pool_api.Func(arg_0=arg_0)\n    except AirflowException as err:\n        _log.error(err)\n        arg_2 = jsonify(error=\"{}\".format(err))\n        arg_2.status_code = err.status_code\n        return arg_2\n    else:\n        return jsonify(arg_1.to_json())", "path": "airflow/www/api/experimental/endpoints.py", "identifier": "delete_pool", "docstring": "Delete pool.", "docstring_tokens": ["Delete", "pool", "."], "nwo": "apache/airflow", "score": 0.9994400765917697, "idx": 277549}
{"url": "https://github.com/SectorLabs/django-postgres-extra/blob/eef2ed5504d225858d4e4f5d77a838082ca6053e/psqlextra/backend/hstore_unique.py#L117-L131", "sha": "eef2ed5504d225858d4e4f5d77a838082ca6053e", "docstring_summary": "Creates a UNIQUE constraint for the specified hstore keys.", "language": "python", "parameters": "(self, model, field, keys)", "return_statement": "", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ",", "arg_1", ",", "arg_2", ",", "arg_3", ")", ":", "arg_4", "=", "arg_0", ".", "_unique_constraint_name", "(", "arg_1", ".", "_meta", ".", "db_table", ",", "arg_2", ",", "arg_3", ")", "arg_5", "=", "[", "'(%s->\\'%s\\')'", "%", "(", "arg_2", ".", "column", ",", "key", ")", "for", "key", "in", "arg_3", "]", "arg_6", "=", "arg_0", ".", "sql_hstore_unique_create", ".", "format", "(", "arg_4", "=", "arg_0", ".", "quote_name", "(", "arg_4", ")", ",", "table", "=", "arg_0", ".", "quote_name", "(", "arg_1", ".", "_meta", ".", "db_table", ")", ",", "arg_5", "=", "','", ".", "join", "(", "arg_5", ")", ")", "arg_0", ".", "execute", "(", "arg_6", ")"], "function": "def Func(arg_0, arg_1, arg_2, arg_3):\n        \"\"\"Creates a UNIQUE constraint for the specified hstore keys.\"\"\"\n\n        arg_4 = arg_0._unique_constraint_name(\n            arg_1._meta.db_table, arg_2, arg_3)\n        arg_5 = [\n            '(%s->\\'%s\\')' % (arg_2.column, key)\n            for key in arg_3\n        ]\n        arg_6 = arg_0.sql_hstore_unique_create.format(\n            arg_4=arg_0.quote_name(arg_4),\n            table=arg_0.quote_name(arg_1._meta.db_table),\n            arg_5=','.join(arg_5)\n        )\n        arg_0.execute(arg_6)", "path": "psqlextra/backend/hstore_unique.py", "identifier": "HStoreUniqueSchemaEditorMixin._create_hstore_unique", "docstring": "Creates a UNIQUE constraint for the specified hstore keys.", "docstring_tokens": ["Creates", "a", "UNIQUE", "constraint", "for", "the", "specified", "hstore", "keys", "."], "nwo": "SectorLabs/django-postgres-extra", "score": 0.938393672171891, "idx": 261674}
{"url": "https://github.com/waqasbhatti/astrobase/blob/2922a14619d183fb28005fa7d02027ac436f2265/astrobase/periodbase/_oldpf.py#L250-L325", "sha": "2922a14619d183fb28005fa7d02027ac436f2265", "docstring_summary": "This is the parallel worker for the function below.", "language": "python", "parameters": "(task)", "return_statement": "", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ")", ":", "arg_1", "=", "arg_0", "[", "0", "]", "arg_2", ",", "arg_3", "=", "arg_0", "[", "1", "]", ",", "arg_0", "[", "2", "]", "arg_4", "=", "arg_0", "[", "3", "]", "arg_5", "=", "range", "(", "arg_0", "[", "4", "]", ")", "arg_6", "=", "arg_0", "[", "5", "]", "arg_7", "=", "arg_0", "[", "6", "]", "arg_8", "=", "arg_0", "[", "7", "]", "try", ":", "arg_9", "=", "1.0", "/", "arg_1", "arg_10", "=", "phase_magseries", "(", "arg_2", ",", "arg_3", ",", "arg_9", ",", "arg_4", ",", "wrap", "=", "False", ",", "sort", "=", "True", ")", "if", "arg_8", "is", "not", "None", "and", "arg_8", ">", "0", ":", "arg_11", "=", "pwd_phasebin", "(", "arg_10", "[", "'phase'", "]", ",", "arg_10", "[", "'mags'", "]", ",", "binsize", "=", "arg_8", ")", "arg_12", "=", "arg_11", "[", "0", "]", "arg_13", "=", "arg_11", "[", "1", "]", "arg_5", "=", "range", "(", "len", "(", "arg_13", ")", "-", "1", ")", "else", ":", "arg_12", "=", "arg_10", "[", "'phase'", "]", "arg_13", "=", "arg_10", "[", "'mags'", "]", "arg_14", "=", "nproll", "(", "arg_13", ",", "1", ")", "arg_15", "=", "nproll", "(", "arg_12", ",", "1", ")", "arg_16", "=", "(", "(", "arg_14", "-", "arg_13", ")", "*", "(", "arg_14", "-", "arg_13", ")", "+", "(", "arg_15", "-", "arg_12", ")", "*", "(", "arg_15", "-", "arg_12", ")", ")", "arg_16", "[", "0", "]", "=", "(", "(", "(", "arg_13", "[", "0", "]", "-", "arg_13", "[", "-", "1", "]", ")", "*", "(", "arg_13", "[", "0", "]", "-", "arg_13", "[", "-", "1", "]", ")", ")", "+", "(", "(", "arg_12", "[", "0", "]", "-", "arg_12", "[", "-", "1", "]", "+", "1", ")", "*", "(", "arg_12", "[", "0", "]", "-", "arg_12", "[", "-", "1", "]", "+", "1", ")", ")", ")", "arg_17", "=", "npsum", "(", "npsqrt", "(", "arg_16", ")", ")", "if", "(", "arg_6", "<", "arg_17", "<", "arg_7", ")", ":", "arg_18", "=", "True", "else", ":", "arg_18", "=", "False", "return", "(", "arg_9", ",", "arg_17", ",", "arg_18", ")", "except", "Exception", "as", "e", ":", "LOGEXCEPTION", "(", "'error in DWP'", ")", "return", "(", "arg_9", ",", "npnan", ",", "False", ")"], "function": "def Func(arg_0):\n    '''\n    This is the parallel worker for the function below.\n\n    task[0] = frequency for this worker\n    task[1] = times array\n    task[2] = mags array\n    task[3] = fold_time\n    task[4] = j_range\n    task[5] = keep_threshold_1\n    task[6] = keep_threshold_2\n    task[7] = phasebinsize\n\n    we don't need errs for the worker.\n\n    '''\n\n    arg_1 = arg_0[0]\n    arg_2, arg_3 = arg_0[1], arg_0[2]\n    arg_4 = arg_0[3]\n    arg_5 = range(arg_0[4])\n    arg_6 = arg_0[5]\n    arg_7 = arg_0[6]\n    arg_8 = arg_0[7]\n\n\n    try:\n\n        arg_9 = 1.0/arg_1\n\n        # use the common phaser to phase and sort the mag\n        arg_10 = phase_magseries(arg_2,\n                                 arg_3,\n                                 arg_9,\n                                 arg_4,\n                                 wrap=False,\n                                 sort=True)\n\n        # bin in phase if requested, this turns this into a sort of PDM method\n        if arg_8 is not None and arg_8 > 0:\n            arg_11 = pwd_phasebin(arg_10['phase'],\n                                   arg_10['mags'],\n                                   binsize=arg_8)\n            arg_12 = arg_11[0]\n            arg_13 = arg_11[1]\n            arg_5 = range(len(arg_13) - 1)\n        else:\n            arg_12 = arg_10['phase']\n            arg_13 = arg_10['mags']\n\n        # now calculate the string length\n        arg_14 = nproll(arg_13,1)\n        arg_15 = nproll(arg_12,1)\n        arg_16 = (\n            (arg_14 - arg_13)*(arg_14 - arg_13) +\n            (arg_15 - arg_12)*(arg_15 - arg_12)\n        )\n        arg_16[0] = (\n            ((arg_13[0] - arg_13[-1]) *\n             (arg_13[0] - arg_13[-1])) +\n            ((arg_12[0] - arg_12[-1] + 1) *\n             (arg_12[0] - arg_12[-1] + 1))\n        )\n        arg_17 = npsum(npsqrt(arg_16))\n\n        if (arg_6 < arg_17 < arg_7):\n            arg_18  = True\n        else:\n            arg_18 = False\n\n        return (arg_9, arg_17, arg_18)\n\n    except Exception as e:\n\n        LOGEXCEPTION('error in DWP')\n        return(arg_9, npnan, False)", "path": "astrobase/periodbase/_oldpf.py", "identifier": "pdw_worker", "docstring": "This is the parallel worker for the function below.\n\n    task[0] = frequency for this worker\n    task[1] = times array\n    task[2] = mags array\n    task[3] = fold_time\n    task[4] = j_range\n    task[5] = keep_threshold_1\n    task[6] = keep_threshold_2\n    task[7] = phasebinsize\n\n    we don't need errs for the worker.", "docstring_tokens": ["This", "is", "the", "parallel", "worker", "for", "the", "function", "below", "."], "nwo": "waqasbhatti/astrobase", "score": 0.5072804572339592, "idx": 261468}
{"url": "https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/Parser.py#L76-L90", "sha": "8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027", "docstring_summary": "Parse a knitting pattern set.", "language": "python", "parameters": "(self, values)", "return_statement": "return self._pattern_set", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ",", "arg_1", ")", ":", "arg_0", ".", "_start", "(", ")", "arg_2", "=", "arg_0", ".", "_new_pattern_collection", "(", ")", "arg_0", ".", "_fill_pattern_collection", "(", "arg_2", ",", "arg_1", ")", "arg_0", ".", "_create_pattern_set", "(", "arg_2", ",", "arg_1", ")", "return", "arg_0", ".", "_pattern_set"], "function": "def Func(arg_0, arg_1):\n        \"\"\"Parse a knitting pattern set.\n\n        :param dict value: the specification of the knitting pattern set\n        :rtype: knittingpattern.KnittingPatternSet.KnittingPatternSet\n        :raises knittingpattern.KnittingPatternSet.ParsingError: if\n          :paramref:`value` does not fulfill the :ref:`specification\n          <FileFormatSpecification>`.\n\n        \"\"\"\n        arg_0._start()\n        arg_2 = arg_0._new_pattern_collection()\n        arg_0._fill_pattern_collection(arg_2, arg_1)\n        arg_0._create_pattern_set(arg_2, arg_1)\n        return arg_0._pattern_set", "path": "knittingpattern/Parser.py", "identifier": "Parser.knitting_pattern_set", "docstring": "Parse a knitting pattern set.\n\n        :param dict value: the specification of the knitting pattern set\n        :rtype: knittingpattern.KnittingPatternSet.KnittingPatternSet\n        :raises knittingpattern.KnittingPatternSet.ParsingError: if\n          :paramref:`value` does not fulfill the :ref:`specification\n          <FileFormatSpecification>`.", "docstring_tokens": ["Parse", "a", "knitting", "pattern", "set", "."], "nwo": "fossasia/knittingpattern", "score": 0.3735534547473614, "idx": 273832}
{"url": "https://github.com/getgauge/gauge-python/blob/90f3547dcfd2d16d51f116cdd4e53527eeab1a57/getgauge/parser.py#L21-L38", "sha": "90f3547dcfd2d16d51f116cdd4e53527eeab1a57", "docstring_summary": "Select default parser for loading and refactoring steps. Passing `redbaron` as argument\n        will select the old paring engine from v0.3.3", "language": "python", "parameters": "(parser=None)", "return_statement": "", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", "=", "None", ")", ":", "if", "arg_0", "==", "'redbaron'", "or", "os", ".", "environ", ".", "get", "(", "'GETGAUGE_USE_0_3_3_PARSER'", ")", ":", "arg_1", ".", "Class", "=", "RedbaronPythonFile", "else", ":", "arg_1", ".", "Class", "=", "ParsoPythonFile"], "function": "def Func(arg_0=None):\n        \"\"\"\n        Select default parser for loading and refactoring steps. Passing `redbaron` as argument\n        will select the old paring engine from v0.3.3\n\n        Replacing the redbaron parser was necessary to support Python 3 syntax. We have tried our\n        best to make sure there is no user impact on users. However, there may be regressions with\n        new parser backend.\n\n        To revert to the old parser implementation, add `GETGAUGE_USE_0_3_3_PARSER=true` property\n        to the `python.properties` file in the `<PROJECT_DIR>/env/default directory.\n\n        This property along with the redbaron parser will be removed in future releases.\n        \"\"\"\n        if arg_0 == 'redbaron' or os.environ.get('GETGAUGE_USE_0_3_3_PARSER'):\n            arg_1.Class = RedbaronPythonFile\n        else:\n            arg_1.Class = ParsoPythonFile", "path": "getgauge/parser.py", "identifier": "PythonFile.select_python_parser", "docstring": "Select default parser for loading and refactoring steps. Passing `redbaron` as argument\n        will select the old paring engine from v0.3.3\n\n        Replacing the redbaron parser was necessary to support Python 3 syntax. We have tried our\n        best to make sure there is no user impact on users. However, there may be regressions with\n        new parser backend.\n\n        To revert to the old parser implementation, add `GETGAUGE_USE_0_3_3_PARSER=true` property\n        to the `python.properties` file in the `<PROJECT_DIR>/env/default directory.\n\n        This property along with the redbaron parser will be removed in future releases.", "docstring_tokens": ["Select", "default", "parser", "for", "loading", "and", "refactoring", "steps", ".", "Passing", "redbaron", "as", "argument", "will", "select", "the", "old", "paring", "engine", "from", "v0", ".", "3", ".", "3"], "nwo": "getgauge/gauge-python", "score": 0.6538673018201949, "idx": 267944}
{"url": "https://github.com/marl/jams/blob/b16778399b9528efbd71434842a079f7691a7a66/jams/core.py#L2047-L2078", "sha": "b16778399b9528efbd71434842a079f7691a7a66", "docstring_summary": "Test if a string matches a query.", "language": "python", "parameters": "(string, query)", "return_statement": "", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ",", "arg_1", ")", ":", "if", "six", ".", "callable", "(", "arg_1", ")", ":", "return", "arg_1", "(", "arg_0", ")", "elif", "(", "isinstance", "(", "arg_1", ",", "six", ".", "string_types", ")", "and", "isinstance", "(", "arg_0", ",", "six", ".", "string_types", ")", ")", ":", "return", "re", ".", "match", "(", "arg_1", ",", "arg_0", ")", "is", "not", "None", "else", ":", "return", "arg_1", "==", "arg_0"], "function": "def Func(arg_0, arg_1):\n    '''Test if a string matches a query.\n\n    Parameters\n    ----------\n    string : str\n        The string to test\n\n    query : string, callable, or object\n        Either a regular expression, callable function, or object.\n\n    Returns\n    -------\n    match : bool\n        `True` if:\n        - `query` is a callable and `query(string) == True`\n        - `query` is a regular expression and `re.match(query, string)`\n        - or `string == query` for any other query\n\n        `False` otherwise\n\n    '''\n\n    if six.callable(arg_1):\n        return arg_1(arg_0)\n\n    elif (isinstance(arg_1, six.string_types) and\n          isinstance(arg_0, six.string_types)):\n        return re.match(arg_1, arg_0) is not None\n\n    else:\n        return arg_1 == arg_0", "path": "jams/core.py", "identifier": "match_query", "docstring": "Test if a string matches a query.\n\n    Parameters\n    ----------\n    string : str\n        The string to test\n\n    query : string, callable, or object\n        Either a regular expression, callable function, or object.\n\n    Returns\n    -------\n    match : bool\n        `True` if:\n        - `query` is a callable and `query(string) == True`\n        - `query` is a regular expression and `re.match(query, string)`\n        - or `string == query` for any other query\n\n        `False` otherwise", "docstring_tokens": ["Test", "if", "a", "string", "matches", "a", "query", "."], "nwo": "marl/jams", "score": 0.41097029610471103, "idx": 263983}
{"url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/segment.py#L748-L877", "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "docstring_summary": "Multi-angle path enhancement for self- and cross-similarity matrices.", "language": "python", "parameters": "(R, n, window='hann', max_ratio=2.0, min_ratio=None, n_filters=7,\n                 zero_mean=False, clip=True, **kwargs)", "return_statement": "return R_smooth", "argument_list": "", "function_tokens": ["def", "Func", "(", "arg_0", ",", "arg_1", ",", "arg_2", "=", "'hann'", ",", "arg_3", "=", "2.0", ",", "arg_4", "=", "None", ",", "arg_5", "=", "7", ",", "arg_6", "=", "False", ",", "arg_7", "=", "True", ",", "**", "arg_8", ")", ":", "if", "arg_4", "is", "None", ":", "arg_4", "=", "1.", "/", "arg_3", "elif", "arg_4", ">", "arg_3", ":", "raise", "ParameterError", "(", "'min_ratio={} cannot exceed max_ratio={}'", ".", "format", "(", "arg_4", ",", "arg_3", ")", ")", "arg_9", "=", "None", "for", "arg_10", "in", "np", ".", "logspace", "(", "np", ".", "log2", "(", "arg_4", ")", ",", "np", ".", "log2", "(", "arg_3", ")", ",", "num", "=", "arg_5", ",", "base", "=", "2", ")", ":", "arg_11", "=", "diagonal_filter", "(", "arg_2", ",", "arg_1", ",", "slope", "=", "arg_10", ",", "arg_6", "=", "arg_6", ")", "if", "arg_9", "is", "None", ":", "arg_9", "=", "scipy", ".", "ndimage", ".", "convolve", "(", "arg_0", ",", "arg_11", ",", "**", "arg_8", ")", "else", ":", "np", ".", "maximum", "(", "arg_9", ",", "scipy", ".", "ndimage", ".", "convolve", "(", "arg_0", ",", "arg_11", ",", "**", "arg_8", ")", ",", "out", "=", "arg_9", ")", "if", "arg_7", ":", "np", ".", "clip", "(", "arg_9", ",", "0", ",", "None", ",", "out", "=", "arg_9", ")", "return", "arg_9"], "function": "def Func(arg_0, arg_1, arg_2='hann', arg_3=2.0, arg_4=None, arg_5=7,\n                 arg_6=False, arg_7=True, **arg_8):\n    '''Multi-angle path enhancement for self- and cross-similarity matrices.\n\n    This function convolves multiple diagonal smoothing filters with a self-similarity (or\n    recurrence) matrix R, and aggregates the result by an element-wise maximum.\n\n    Technically, the output is a matrix R_smooth such that\n\n        `R_smooth[i, j] = max_theta (R * filter_theta)[i, j]`\n\n    where `*` denotes 2-dimensional convolution, and `filter_theta` is a smoothing filter at\n    orientation theta.\n\n    This is intended to provide coherent temporal smoothing of self-similarity matrices\n    when there are changes in tempo.\n\n    Smoothing filters are generated at evenly spaced orientations between min_ratio and\n    max_ratio.\n\n    This function is inspired by the multi-angle path enhancement of [1]_, but differs by\n    modeling tempo differences in the space of similarity matrices rather than re-sampling\n    the underlying features prior to generating the self-similarity matrix.\n\n    .. [1] M\u00fcller, Meinard and Frank Kurth.\n            \"Enhancing similarity matrices for music audio analysis.\"\n            2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings.\n            Vol. 5. IEEE, 2006.\n\n    .. note:: if using recurrence_matrix to construct the input similarity matrix, be sure to include the main\n              diagonal by setting `self=True`.  Otherwise, the diagonal will be suppressed, and this is likely to\n              produce discontinuities which will pollute the smoothing filter response.\n\n    Parameters\n    ----------\n    R : np.ndarray\n        The self- or cross-similarity matrix to be smoothed.\n        Note: sparse inputs are not supported.\n\n    n : int > 0\n        The length of the smoothing filter\n\n    window : window specification\n        The type of smoothing filter to use.  See `filters.get_window` for more information\n        on window specification formats.\n\n    max_ratio : float > 0\n        The maximum tempo ratio to support\n\n    min_ratio : float > 0\n        The minimum tempo ratio to support.\n        If not provided, it will default to `1/max_ratio`\n\n    n_filters : int >= 1\n        The number of different smoothing filters to use, evenly spaced\n        between `min_ratio` and `max_ratio`.\n\n        If `min_ratio = 1/max_ratio` (the default), using an odd number\n        of filters will ensure that the main diagonal (ratio=1) is included.\n\n    zero_mean : bool\n        By default, the smoothing filters are non-negative and sum to one (i.e. are averaging\n        filters).\n\n        If `zero_mean=True`, then the smoothing filters are made to sum to zero by subtracting\n        a constant value from the non-diagonal coordinates of the filter.  This is primarily\n        useful for suppressing blocks while enhancing diagonals.\n\n    clip : bool\n        If True, the smoothed similarity matrix will be thresholded at 0, and will not contain\n        negative entries.\n\n    kwargs : additional keyword arguments\n        Additional arguments to pass to `scipy.ndimage.convolve`\n\n\n    Returns\n    -------\n    R_smooth : np.ndarray, shape=R.shape\n        The smoothed self- or cross-similarity matrix\n\n    See Also\n    --------\n    filters.diagonal_filter\n    recurrence_matrix\n\n\n    Examples\n    --------\n    Use a 51-frame diagonal smoothing filter to enhance paths in a recurrence matrix\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=30)\n    >>> chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n    >>> rec = librosa.segment.recurrence_matrix(chroma, mode='affinity', self=True)\n    >>> rec_smooth = librosa.segment.Func(rec, 51, window='hann', n_filters=7)\n\n    Plot the recurrence matrix before and after smoothing\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1,2,1)\n    >>> librosa.display.specshow(rec, x_axis='time', y_axis='time')\n    >>> plt.title('Unfiltered recurrence')\n    >>> plt.subplot(1,2,2)\n    >>> librosa.display.specshow(rec_smooth, x_axis='time', y_axis='time')\n    >>> plt.title('Multi-angle enhanced recurrence')\n    >>> plt.tight_layout()\n    '''\n\n    if arg_4 is None:\n        arg_4 = 1./arg_3\n    elif arg_4 > arg_3:\n        raise ParameterError('min_ratio={} cannot exceed max_ratio={}'.format(arg_4, arg_3))\n\n    arg_9 = None\n    for arg_10 in np.logspace(np.log2(arg_4), np.log2(arg_3), num=arg_5, base=2):\n        arg_11 = diagonal_filter(arg_2, arg_1, slope=arg_10, arg_6=arg_6)\n\n        if arg_9 is None:\n            arg_9 = scipy.ndimage.convolve(arg_0, arg_11, **arg_8)\n        else:\n            # Compute the point-wise maximum in-place\n            np.maximum(arg_9, scipy.ndimage.convolve(arg_0, arg_11, **arg_8),\n                       out=arg_9)\n\n    if arg_7:\n        # Clip the output in-place\n        np.clip(arg_9, 0, None, out=arg_9)\n\n    return arg_9", "path": "librosa/segment.py", "identifier": "path_enhance", "docstring": "Multi-angle path enhancement for self- and cross-similarity matrices.\n\n    This function convolves multiple diagonal smoothing filters with a self-similarity (or\n    recurrence) matrix R, and aggregates the result by an element-wise maximum.\n\n    Technically, the output is a matrix R_smooth such that\n\n        `R_smooth[i, j] = max_theta (R * filter_theta)[i, j]`\n\n    where `*` denotes 2-dimensional convolution, and `filter_theta` is a smoothing filter at\n    orientation theta.\n\n    This is intended to provide coherent temporal smoothing of self-similarity matrices\n    when there are changes in tempo.\n\n    Smoothing filters are generated at evenly spaced orientations between min_ratio and\n    max_ratio.\n\n    This function is inspired by the multi-angle path enhancement of [1]_, but differs by\n    modeling tempo differences in the space of similarity matrices rather than re-sampling\n    the underlying features prior to generating the self-similarity matrix.\n\n    .. [1] M\u00fcller, Meinard and Frank Kurth.\n            \"Enhancing similarity matrices for music audio analysis.\"\n            2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings.\n            Vol. 5. IEEE, 2006.\n\n    .. note:: if using recurrence_matrix to construct the input similarity matrix, be sure to include the main\n              diagonal by setting `self=True`.  Otherwise, the diagonal will be suppressed, and this is likely to\n              produce discontinuities which will pollute the smoothing filter response.\n\n    Parameters\n    ----------\n    R : np.ndarray\n        The self- or cross-similarity matrix to be smoothed.\n        Note: sparse inputs are not supported.\n\n    n : int > 0\n        The length of the smoothing filter\n\n    window : window specification\n        The type of smoothing filter to use.  See `filters.get_window` for more information\n        on window specification formats.\n\n    max_ratio : float > 0\n        The maximum tempo ratio to support\n\n    min_ratio : float > 0\n        The minimum tempo ratio to support.\n        If not provided, it will default to `1/max_ratio`\n\n    n_filters : int >= 1\n        The number of different smoothing filters to use, evenly spaced\n        between `min_ratio` and `max_ratio`.\n\n        If `min_ratio = 1/max_ratio` (the default), using an odd number\n        of filters will ensure that the main diagonal (ratio=1) is included.\n\n    zero_mean : bool\n        By default, the smoothing filters are non-negative and sum to one (i.e. are averaging\n        filters).\n\n        If `zero_mean=True`, then the smoothing filters are made to sum to zero by subtracting\n        a constant value from the non-diagonal coordinates of the filter.  This is primarily\n        useful for suppressing blocks while enhancing diagonals.\n\n    clip : bool\n        If True, the smoothed similarity matrix will be thresholded at 0, and will not contain\n        negative entries.\n\n    kwargs : additional keyword arguments\n        Additional arguments to pass to `scipy.ndimage.convolve`\n\n\n    Returns\n    -------\n    R_smooth : np.ndarray, shape=R.shape\n        The smoothed self- or cross-similarity matrix\n\n    See Also\n    --------\n    filters.diagonal_filter\n    recurrence_matrix\n\n\n    Examples\n    --------\n    Use a 51-frame diagonal smoothing filter to enhance paths in a recurrence matrix\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=30)\n    >>> chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n    >>> rec = librosa.segment.recurrence_matrix(chroma, mode='affinity', self=True)\n    >>> rec_smooth = librosa.segment.path_enhance(rec, 51, window='hann', n_filters=7)\n\n    Plot the recurrence matrix before and after smoothing\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1,2,1)\n    >>> librosa.display.specshow(rec, x_axis='time', y_axis='time')\n    >>> plt.title('Unfiltered recurrence')\n    >>> plt.subplot(1,2,2)\n    >>> librosa.display.specshow(rec_smooth, x_axis='time', y_axis='time')\n    >>> plt.title('Multi-angle enhanced recurrence')\n    >>> plt.tight_layout()", "docstring_tokens": ["Multi", "-", "angle", "path", "enhancement", "for", "self", "-", "and", "cross", "-", "similarity", "matrices", "."], "nwo": "librosa/librosa", "score": 0.98482969517341, "idx": 262574}
