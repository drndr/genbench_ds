{"repo": "nikcub/floyd", "path": "floyd/core/multiopt.py", "func_name": "MultioptHelpFormatter.command_help_long", "original_string": "def command_help_long(self):\n    \"\"\"\n      Return command help for use in global parser usage string\n      \n      @TODO update to support self.current_indent from formatter\n    \"\"\"\n    indent = \" \" * 2 # replace with current_indent\n    help = \"Command must be one of:\\n\"\n    for action_name in self.parser.valid_commands:\n      help += \"%s%-10s %-70s\\n\" % (indent, action_name, self.parser.commands[action_name].desc_short.capitalize())\n    help += '\\nSee \\'%s help COMMAND\\' for help and information on a command' % self.parser.prog\n    return help", "language": "python", "code": "def command_help_long(self):\n    \"\"\"\n      Return command help for use in global parser usage string\n      \n      @TODO update to support self.current_indent from formatter\n    \"\"\"\n    indent = \" \" * 2 # replace with current_indent\n    help = \"Command must be one of:\\n\"\n    for action_name in self.parser.valid_commands:\n      help += \"%s%-10s %-70s\\n\" % (indent, action_name, self.parser.commands[action_name].desc_short.capitalize())\n    help += '\\nSee \\'%s help COMMAND\\' for help and information on a command' % self.parser.prog\n    return help", "code_tokens": ["def", "command_help_long", "(", "self", ")", ":", "indent", "=", "\" \"", "*", "2", "# replace with current_indent", "help", "=", "\"Command must be one of:\\n\"", "for", "action_name", "in", "self", ".", "parser", ".", "valid_commands", ":", "help", "+=", "\"%s%-10s %-70s\\n\"", "%", "(", "indent", ",", "action_name", ",", "self", ".", "parser", ".", "commands", "[", "action_name", "]", ".", "desc_short", ".", "capitalize", "(", ")", ")", "help", "+=", "'\\nSee \\'%s help COMMAND\\' for help and information on a command'", "%", "self", ".", "parser", ".", "prog", "return", "help"], "docstring": "Return command help for use in global parser usage string\n      \n      @TODO update to support self.current_indent from formatter", "docstring_tokens": ["Return", "command", "help", "for", "use", "in", "global", "parser", "usage", "string"], "sha": "5772d0047efb11c9ce5f7d234a9da4576ce24edc", "url": "https://github.com/nikcub/floyd/blob/5772d0047efb11c9ce5f7d234a9da4576ce24edc/floyd/core/multiopt.py#L66-L77", "partition": "train", "idx": 55241}
{"repo": "brechtm/rinohtype", "path": "src/rinoh/backend/pdf/xobject/purepng.py", "func_name": "MergedPlanes.rigthgen", "original_string": "def rigthgen(self, value=0):\n        \"\"\"Generate rows to fill right pixels in int mode\"\"\"\n        while True:\n            yield self.newarray(self.nplanes_right * self.width, value)", "language": "python", "code": "def rigthgen(self, value=0):\n        \"\"\"Generate rows to fill right pixels in int mode\"\"\"\n        while True:\n            yield self.newarray(self.nplanes_right * self.width, value)", "code_tokens": ["def", "rigthgen", "(", "self", ",", "value", "=", "0", ")", ":", "while", "True", ":", "yield", "self", ".", "newarray", "(", "self", ".", "nplanes_right", "*", "self", ".", "width", ",", "value", ")"], "docstring": "Generate rows to fill right pixels in int mode", "docstring_tokens": ["Generate", "rows", "to", "fill", "right", "pixels", "in", "int", "mode"], "sha": "40a63c4e5ad7550f62b6860f1812cb67cafb9dc7", "url": "https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/xobject/purepng.py#L1611-L1614", "partition": "train", "idx": 31335}
{"repo": "apache/incubator-mxnet", "path": "docs/mxdoc.py", "func_name": "_get_python_block_output", "original_string": "def _get_python_block_output(src, global_dict, local_dict):\n    \"\"\"Evaluate python source codes\n\n    Returns\n    (bool, str):\n      - True if success\n      - output\n    \"\"\"\n    src = '\\n'.join([l for l in src.split('\\n')\n                     if not l.startswith('%') and not 'plt.show()' in l])\n    ret_status = True\n    err = ''\n    with _string_io() as s:\n        try:\n            exec(src, global_dict, global_dict)\n        except Exception as e:\n            err = str(e)\n            ret_status = False\n    return (ret_status, s.getvalue()+err)", "language": "python", "code": "def _get_python_block_output(src, global_dict, local_dict):\n    \"\"\"Evaluate python source codes\n\n    Returns\n    (bool, str):\n      - True if success\n      - output\n    \"\"\"\n    src = '\\n'.join([l for l in src.split('\\n')\n                     if not l.startswith('%') and not 'plt.show()' in l])\n    ret_status = True\n    err = ''\n    with _string_io() as s:\n        try:\n            exec(src, global_dict, global_dict)\n        except Exception as e:\n            err = str(e)\n            ret_status = False\n    return (ret_status, s.getvalue()+err)", "code_tokens": ["def", "_get_python_block_output", "(", "src", ",", "global_dict", ",", "local_dict", ")", ":", "src", "=", "'\\n'", ".", "join", "(", "[", "l", "for", "l", "in", "src", ".", "split", "(", "'\\n'", ")", "if", "not", "l", ".", "startswith", "(", "'%'", ")", "and", "not", "'plt.show()'", "in", "l", "]", ")", "ret_status", "=", "True", "err", "=", "''", "with", "_string_io", "(", ")", "as", "s", ":", "try", ":", "exec", "(", "src", ",", "global_dict", ",", "global_dict", ")", "except", "Exception", "as", "e", ":", "err", "=", "str", "(", "e", ")", "ret_status", "=", "False", "return", "(", "ret_status", ",", "s", ".", "getvalue", "(", ")", "+", "err", ")"], "docstring": "Evaluate python source codes\n\n    Returns\n    (bool, str):\n      - True if success\n      - output", "docstring_tokens": ["Evaluate", "python", "source", "codes"], "sha": "1af29e9c060a4c7d60eeaacba32afdb9a7775ba7", "url": "https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/docs/mxdoc.py#L321-L339", "partition": "train", "idx": 163457}
{"repo": "tomnor/channelpack", "path": "channelpack/datautils.py", "func_name": "slicelist", "original_string": "def slicelist(b):\n    \"\"\"Produce a list of slices given the boolean array b.\n\n    Start and stop in each slice describe the True sections in b.\"\"\"\n\n    slicelst = []\n    started = False\n    for i, e in enumerate(b):\n        if e and not started:\n            start = i\n            started = True\n        elif not e and started:\n            slicelst.append(slice(start, i))\n            started = False\n\n    if e:\n        slicelst.append(slice(start, i + 1))  # True in the end.\n\n    return slicelst", "language": "python", "code": "def slicelist(b):\n    \"\"\"Produce a list of slices given the boolean array b.\n\n    Start and stop in each slice describe the True sections in b.\"\"\"\n\n    slicelst = []\n    started = False\n    for i, e in enumerate(b):\n        if e and not started:\n            start = i\n            started = True\n        elif not e and started:\n            slicelst.append(slice(start, i))\n            started = False\n\n    if e:\n        slicelst.append(slice(start, i + 1))  # True in the end.\n\n    return slicelst", "code_tokens": ["def", "slicelist", "(", "b", ")", ":", "slicelst", "=", "[", "]", "started", "=", "False", "for", "i", ",", "e", "in", "enumerate", "(", "b", ")", ":", "if", "e", "and", "not", "started", ":", "start", "=", "i", "started", "=", "True", "elif", "not", "e", "and", "started", ":", "slicelst", ".", "append", "(", "slice", "(", "start", ",", "i", ")", ")", "started", "=", "False", "if", "e", ":", "slicelst", ".", "append", "(", "slice", "(", "start", ",", "i", "+", "1", ")", ")", "# True in the end.", "return", "slicelst"], "docstring": "Produce a list of slices given the boolean array b.\n\n    Start and stop in each slice describe the True sections in b.", "docstring_tokens": ["Produce", "a", "list", "of", "slices", "given", "the", "boolean", "array", "b", "."], "sha": "9ad3cd11c698aed4c0fc178385b2ba38a7d0efae", "url": "https://github.com/tomnor/channelpack/blob/9ad3cd11c698aed4c0fc178385b2ba38a7d0efae/channelpack/datautils.py#L174-L192", "partition": "train", "idx": 100496}
{"repo": "openstax/cnx-archive", "path": "cnxarchive/views/in_book_search.py", "func_name": "in_book_search_highlighted_results", "original_string": "def in_book_search_highlighted_results(request):\n    \"\"\"In-book search - returns a highlighted version of the HTML.\"\"\"\n    results = {}\n\n    args = request.matchdict\n    ident_hash = args['ident_hash']\n\n    page_ident_hash = args['page_ident_hash']\n    try:\n        page_uuid, _ = split_ident_hash(page_ident_hash)\n    except IdentHashShortId as e:\n        page_uuid = get_uuid(e.id)\n    except IdentHashMissingVersion as e:\n        page_uuid = e.id\n    args['page_uuid'] = page_uuid\n\n    args['search_term'] = request.params.get('q', '')\n\n    query_type = request.params.get('query_type', '')\n    combiner = ''\n    if query_type:\n        if query_type.lower() == 'or':\n            combiner = '_or'\n\n    # Get version from URL params\n    id, version = split_ident_hash(ident_hash)\n    args['uuid'] = id\n    args['version'] = version\n\n    with db_connect() as db_connection:\n        with db_connection.cursor() as cursor:\n            cursor.execute(SQL['get-collated-state'], args)\n            res = cursor.fetchall()\n            if res and res[0][0]:\n                statement = SQL['get-in-collated-book-search-full-page']\n            else:\n                statement = SQL['get-in-book-search-full-page']\n            cursor.execute(statement.format(combiner=combiner), args)\n            res = cursor.fetchall()\n\n            results['results'] = {'query': [],\n                                  'total': len(res),\n                                  'items': []}\n            results['results']['query'] = {\n                'search_term': args['search_term'],\n                'collection_id': ident_hash,\n            }\n            for uuid, version, title, headline, rank in res:\n                results['results']['items'].append({\n                    'rank': '{}'.format(rank),\n                    'id': '{}'.format(page_ident_hash),\n                    'title': '{}'.format(title),\n                    'html': '{}'.format(headline),\n                })\n\n    resp = request.response\n    resp.status = '200 OK'\n    resp.content_type = 'application/json'\n    resp.body = json.dumps(results)\n\n    return resp", "language": "python", "code": "def in_book_search_highlighted_results(request):\n    \"\"\"In-book search - returns a highlighted version of the HTML.\"\"\"\n    results = {}\n\n    args = request.matchdict\n    ident_hash = args['ident_hash']\n\n    page_ident_hash = args['page_ident_hash']\n    try:\n        page_uuid, _ = split_ident_hash(page_ident_hash)\n    except IdentHashShortId as e:\n        page_uuid = get_uuid(e.id)\n    except IdentHashMissingVersion as e:\n        page_uuid = e.id\n    args['page_uuid'] = page_uuid\n\n    args['search_term'] = request.params.get('q', '')\n\n    query_type = request.params.get('query_type', '')\n    combiner = ''\n    if query_type:\n        if query_type.lower() == 'or':\n            combiner = '_or'\n\n    # Get version from URL params\n    id, version = split_ident_hash(ident_hash)\n    args['uuid'] = id\n    args['version'] = version\n\n    with db_connect() as db_connection:\n        with db_connection.cursor() as cursor:\n            cursor.execute(SQL['get-collated-state'], args)\n            res = cursor.fetchall()\n            if res and res[0][0]:\n                statement = SQL['get-in-collated-book-search-full-page']\n            else:\n                statement = SQL['get-in-book-search-full-page']\n            cursor.execute(statement.format(combiner=combiner), args)\n            res = cursor.fetchall()\n\n            results['results'] = {'query': [],\n                                  'total': len(res),\n                                  'items': []}\n            results['results']['query'] = {\n                'search_term': args['search_term'],\n                'collection_id': ident_hash,\n            }\n            for uuid, version, title, headline, rank in res:\n                results['results']['items'].append({\n                    'rank': '{}'.format(rank),\n                    'id': '{}'.format(page_ident_hash),\n                    'title': '{}'.format(title),\n                    'html': '{}'.format(headline),\n                })\n\n    resp = request.response\n    resp.status = '200 OK'\n    resp.content_type = 'application/json'\n    resp.body = json.dumps(results)\n\n    return resp", "code_tokens": ["def", "in_book_search_highlighted_results", "(", "request", ")", ":", "results", "=", "{", "}", "args", "=", "request", ".", "matchdict", "ident_hash", "=", "args", "[", "'ident_hash'", "]", "page_ident_hash", "=", "args", "[", "'page_ident_hash'", "]", "try", ":", "page_uuid", ",", "_", "=", "split_ident_hash", "(", "page_ident_hash", ")", "except", "IdentHashShortId", "as", "e", ":", "page_uuid", "=", "get_uuid", "(", "e", ".", "id", ")", "except", "IdentHashMissingVersion", "as", "e", ":", "page_uuid", "=", "e", ".", "id", "args", "[", "'page_uuid'", "]", "=", "page_uuid", "args", "[", "'search_term'", "]", "=", "request", ".", "params", ".", "get", "(", "'q'", ",", "''", ")", "query_type", "=", "request", ".", "params", ".", "get", "(", "'query_type'", ",", "''", ")", "combiner", "=", "''", "if", "query_type", ":", "if", "query_type", ".", "lower", "(", ")", "==", "'or'", ":", "combiner", "=", "'_or'", "# Get version from URL params", "id", ",", "version", "=", "split_ident_hash", "(", "ident_hash", ")", "args", "[", "'uuid'", "]", "=", "id", "args", "[", "'version'", "]", "=", "version", "with", "db_connect", "(", ")", "as", "db_connection", ":", "with", "db_connection", ".", "cursor", "(", ")", "as", "cursor", ":", "cursor", ".", "execute", "(", "SQL", "[", "'get-collated-state'", "]", ",", "args", ")", "res", "=", "cursor", ".", "fetchall", "(", ")", "if", "res", "and", "res", "[", "0", "]", "[", "0", "]", ":", "statement", "=", "SQL", "[", "'get-in-collated-book-search-full-page'", "]", "else", ":", "statement", "=", "SQL", "[", "'get-in-book-search-full-page'", "]", "cursor", ".", "execute", "(", "statement", ".", "format", "(", "combiner", "=", "combiner", ")", ",", "args", ")", "res", "=", "cursor", ".", "fetchall", "(", ")", "results", "[", "'results'", "]", "=", "{", "'query'", ":", "[", "]", ",", "'total'", ":", "len", "(", "res", ")", ",", "'items'", ":", "[", "]", "}", "results", "[", "'results'", "]", "[", "'query'", "]", "=", "{", "'search_term'", ":", "args", "[", "'search_term'", "]", ",", "'collection_id'", ":", "ident_hash", ",", "}", "for", "uuid", ",", "version", ",", "title", ",", "headline", ",", "rank", "in", "res", ":", "results", "[", "'results'", "]", "[", "'items'", "]", ".", "append", "(", "{", "'rank'", ":", "'{}'", ".", "format", "(", "rank", ")", ",", "'id'", ":", "'{}'", ".", "format", "(", "page_ident_hash", ")", ",", "'title'", ":", "'{}'", ".", "format", "(", "title", ")", ",", "'html'", ":", "'{}'", ".", "format", "(", "headline", ")", ",", "}", ")", "resp", "=", "request", ".", "response", "resp", ".", "status", "=", "'200 OK'", "resp", ".", "content_type", "=", "'application/json'", "resp", ".", "body", "=", "json", ".", "dumps", "(", "results", ")", "return", "resp"], "docstring": "In-book search - returns a highlighted version of the HTML.", "docstring_tokens": ["In", "-", "book", "search", "-", "returns", "a", "highlighted", "version", "of", "the", "HTML", "."], "sha": "d31d34aa8bbc8a9fde6cd4227a0df92726e8daf4", "url": "https://github.com/openstax/cnx-archive/blob/d31d34aa8bbc8a9fde6cd4227a0df92726e8daf4/cnxarchive/views/in_book_search.py#L89-L149", "partition": "train", "idx": 48711}
{"repo": "aleontiev/dj", "path": "dj/utils/imports.py", "func_name": "load_module", "original_string": "def load_module(filename):\n    \"\"\"Loads a module from anywhere in the system.\n\n    Does not depend on or modify sys.path.\n    \"\"\"\n    path, name = os.path.split(filename)\n    name, ext = os.path.splitext(name)\n    (file, filename, desc) = imp.find_module(name, [path])\n\n    try:\n        return imp.load_module(name, file, filename, desc)\n    finally:\n        if file:\n            file.close()", "language": "python", "code": "def load_module(filename):\n    \"\"\"Loads a module from anywhere in the system.\n\n    Does not depend on or modify sys.path.\n    \"\"\"\n    path, name = os.path.split(filename)\n    name, ext = os.path.splitext(name)\n    (file, filename, desc) = imp.find_module(name, [path])\n\n    try:\n        return imp.load_module(name, file, filename, desc)\n    finally:\n        if file:\n            file.close()", "code_tokens": ["def", "load_module", "(", "filename", ")", ":", "path", ",", "name", "=", "os", ".", "path", ".", "split", "(", "filename", ")", "name", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "name", ")", "(", "file", ",", "filename", ",", "desc", ")", "=", "imp", ".", "find_module", "(", "name", ",", "[", "path", "]", ")", "try", ":", "return", "imp", ".", "load_module", "(", "name", ",", "file", ",", "filename", ",", "desc", ")", "finally", ":", "if", "file", ":", "file", ".", "close", "(", ")"], "docstring": "Loads a module from anywhere in the system.\n\n    Does not depend on or modify sys.path.", "docstring_tokens": ["Loads", "a", "module", "from", "anywhere", "in", "the", "system", "."], "sha": "0612d442fdd8d472aea56466568b9857556ecb51", "url": "https://github.com/aleontiev/dj/blob/0612d442fdd8d472aea56466568b9857556ecb51/dj/utils/imports.py#L5-L18", "partition": "train", "idx": 88499}
{"repo": "Robpol86/sphinxcontrib-disqus", "path": "sphinxcontrib/disqus.py", "func_name": "event_html_page_context", "original_string": "def event_html_page_context(app, pagename, templatename, context, doctree):\n    \"\"\"Called when the HTML builder has created a context dictionary to render a template with.\n\n    Conditionally adding disqus.js to <head /> if the directive is used in a page.\n\n    :param sphinx.application.Sphinx app: Sphinx application object.\n    :param str pagename: Name of the page being rendered (without .html or any file extension).\n    :param str templatename: Page name with .html.\n    :param dict context: Jinja2 HTML context.\n    :param docutils.nodes.document doctree: Tree of docutils nodes.\n    \"\"\"\n    assert app or pagename or templatename  # Unused, for linting.\n    if 'script_files' in context and doctree and any(hasattr(n, 'disqus_shortname') for n in doctree.traverse()):\n        # Clone list to prevent leaking into other pages and add disqus.js to this page.\n        context['script_files'] = context['script_files'][:] + ['_static/disqus.js']", "language": "python", "code": "def event_html_page_context(app, pagename, templatename, context, doctree):\n    \"\"\"Called when the HTML builder has created a context dictionary to render a template with.\n\n    Conditionally adding disqus.js to <head /> if the directive is used in a page.\n\n    :param sphinx.application.Sphinx app: Sphinx application object.\n    :param str pagename: Name of the page being rendered (without .html or any file extension).\n    :param str templatename: Page name with .html.\n    :param dict context: Jinja2 HTML context.\n    :param docutils.nodes.document doctree: Tree of docutils nodes.\n    \"\"\"\n    assert app or pagename or templatename  # Unused, for linting.\n    if 'script_files' in context and doctree and any(hasattr(n, 'disqus_shortname') for n in doctree.traverse()):\n        # Clone list to prevent leaking into other pages and add disqus.js to this page.\n        context['script_files'] = context['script_files'][:] + ['_static/disqus.js']", "code_tokens": ["def", "event_html_page_context", "(", "app", ",", "pagename", ",", "templatename", ",", "context", ",", "doctree", ")", ":", "assert", "app", "or", "pagename", "or", "templatename", "# Unused, for linting.", "if", "'script_files'", "in", "context", "and", "doctree", "and", "any", "(", "hasattr", "(", "n", ",", "'disqus_shortname'", ")", "for", "n", "in", "doctree", ".", "traverse", "(", ")", ")", ":", "# Clone list to prevent leaking into other pages and add disqus.js to this page.", "context", "[", "'script_files'", "]", "=", "context", "[", "'script_files'", "]", "[", ":", "]", "+", "[", "'_static/disqus.js'", "]"], "docstring": "Called when the HTML builder has created a context dictionary to render a template with.\n\n    Conditionally adding disqus.js to <head /> if the directive is used in a page.\n\n    :param sphinx.application.Sphinx app: Sphinx application object.\n    :param str pagename: Name of the page being rendered (without .html or any file extension).\n    :param str templatename: Page name with .html.\n    :param dict context: Jinja2 HTML context.\n    :param docutils.nodes.document doctree: Tree of docutils nodes.", "docstring_tokens": ["Called", "when", "the", "HTML", "builder", "has", "created", "a", "context", "dictionary", "to", "render", "a", "template", "with", "."], "sha": "1da36bcb83b82b6493a33481c03a0956a557bd5c", "url": "https://github.com/Robpol86/sphinxcontrib-disqus/blob/1da36bcb83b82b6493a33481c03a0956a557bd5c/sphinxcontrib/disqus.py#L102-L116", "partition": "train", "idx": 113021}
{"repo": "jonathf/chaospy", "path": "chaospy/poly/collection/numpy_.py", "func_name": "cumsum", "original_string": "def cumsum(vari, axis=None):\n    \"\"\"\n    Cumulative sum the components of a shapeable quantity along a given axis.\n\n    Args:\n        vari (chaospy.poly.base.Poly, numpy.ndarray):\n            Input data.\n        axis (int):\n            Axis over which the sum is taken. By default ``axis`` is None, and\n            all elements are summed.\n\n    Returns:\n        (chaospy.poly.base.Poly, numpy.ndarray):\n            Polynomial array with same shape as ``vari``.\n\n    Examples:\n        >>> poly = cp.prange(3)\n        >>> print(poly)\n        [1, q0, q0^2]\n        >>> print(cp.cumsum(poly))\n        [1, q0+1, q0^2+q0+1]\n    \"\"\"\n    if isinstance(vari, Poly):\n        core = vari.A.copy()\n        for key, val in core.items():\n            core[key] = cumsum(val, axis)\n        return Poly(core, vari.dim, None, vari.dtype)\n\n    return np.cumsum(vari, axis)", "language": "python", "code": "def cumsum(vari, axis=None):\n    \"\"\"\n    Cumulative sum the components of a shapeable quantity along a given axis.\n\n    Args:\n        vari (chaospy.poly.base.Poly, numpy.ndarray):\n            Input data.\n        axis (int):\n            Axis over which the sum is taken. By default ``axis`` is None, and\n            all elements are summed.\n\n    Returns:\n        (chaospy.poly.base.Poly, numpy.ndarray):\n            Polynomial array with same shape as ``vari``.\n\n    Examples:\n        >>> poly = cp.prange(3)\n        >>> print(poly)\n        [1, q0, q0^2]\n        >>> print(cp.cumsum(poly))\n        [1, q0+1, q0^2+q0+1]\n    \"\"\"\n    if isinstance(vari, Poly):\n        core = vari.A.copy()\n        for key, val in core.items():\n            core[key] = cumsum(val, axis)\n        return Poly(core, vari.dim, None, vari.dtype)\n\n    return np.cumsum(vari, axis)", "code_tokens": ["def", "cumsum", "(", "vari", ",", "axis", "=", "None", ")", ":", "if", "isinstance", "(", "vari", ",", "Poly", ")", ":", "core", "=", "vari", ".", "A", ".", "copy", "(", ")", "for", "key", ",", "val", "in", "core", ".", "items", "(", ")", ":", "core", "[", "key", "]", "=", "cumsum", "(", "val", ",", "axis", ")", "return", "Poly", "(", "core", ",", "vari", ".", "dim", ",", "None", ",", "vari", ".", "dtype", ")", "return", "np", ".", "cumsum", "(", "vari", ",", "axis", ")"], "docstring": "Cumulative sum the components of a shapeable quantity along a given axis.\n\n    Args:\n        vari (chaospy.poly.base.Poly, numpy.ndarray):\n            Input data.\n        axis (int):\n            Axis over which the sum is taken. By default ``axis`` is None, and\n            all elements are summed.\n\n    Returns:\n        (chaospy.poly.base.Poly, numpy.ndarray):\n            Polynomial array with same shape as ``vari``.\n\n    Examples:\n        >>> poly = cp.prange(3)\n        >>> print(poly)\n        [1, q0, q0^2]\n        >>> print(cp.cumsum(poly))\n        [1, q0+1, q0^2+q0+1]", "docstring_tokens": ["Cumulative", "sum", "the", "components", "of", "a", "shapeable", "quantity", "along", "a", "given", "axis", "."], "sha": "25ecfa7bf5608dc10c0b31d142ded0e3755f5d74", "url": "https://github.com/jonathf/chaospy/blob/25ecfa7bf5608dc10c0b31d142ded0e3755f5d74/chaospy/poly/collection/numpy_.py#L46-L74", "partition": "train", "idx": 206963}
{"repo": "klen/muffin-metrics", "path": "muffin_metrics.py", "func_name": "StatsDMixin.timing", "original_string": "def timing(self, stat, delta, rate=1):\n        \"\"\"Send new timing information. `delta` is in milliseconds.\"\"\"\n        return self.send(stat, \"%d|ms\" % delta, rate)", "language": "python", "code": "def timing(self, stat, delta, rate=1):\n        \"\"\"Send new timing information. `delta` is in milliseconds.\"\"\"\n        return self.send(stat, \"%d|ms\" % delta, rate)", "code_tokens": ["def", "timing", "(", "self", ",", "stat", ",", "delta", ",", "rate", "=", "1", ")", ":", "return", "self", ".", "send", "(", "stat", ",", "\"%d|ms\"", "%", "delta", ",", "rate", ")"], "docstring": "Send new timing information. `delta` is in milliseconds.", "docstring_tokens": ["Send", "new", "timing", "information", ".", "delta", "is", "in", "milliseconds", "."], "sha": "b62fc25172e3e1e9fc6dc6c8da3170935ee69f01", "url": "https://github.com/klen/muffin-metrics/blob/b62fc25172e3e1e9fc6dc6c8da3170935ee69f01/muffin_metrics.py#L247-L249", "partition": "train", "idx": 88459}
{"repo": "fastai/fastai", "path": "fastai/text/models/awd_lstm.py", "func_name": "AWD_LSTM._one_hidden", "original_string": "def _one_hidden(self, l:int)->Tensor:\n        \"Return one hidden state.\"\n        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n        return one_param(self).new(1, self.bs, nh).zero_()", "language": "python", "code": "def _one_hidden(self, l:int)->Tensor:\n        \"Return one hidden state.\"\n        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n        return one_param(self).new(1, self.bs, nh).zero_()", "code_tokens": ["def", "_one_hidden", "(", "self", ",", "l", ":", "int", ")", "->", "Tensor", ":", "nh", "=", "(", "self", ".", "n_hid", "if", "l", "!=", "self", ".", "n_layers", "-", "1", "else", "self", ".", "emb_sz", ")", "//", "self", ".", "n_dir", "return", "one_param", "(", "self", ")", ".", "new", "(", "1", ",", "self", ".", "bs", ",", "nh", ")", ".", "zero_", "(", ")"], "docstring": "Return one hidden state.", "docstring_tokens": ["Return", "one", "hidden", "state", "."], "sha": "9fb84a5cdefe5a766cdb792b8f5d8971737b7e67", "url": "https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/text/models/awd_lstm.py#L125-L128", "partition": "train", "idx": 160628}
{"repo": "bukun/TorCMS", "path": "torcms/model/post_model.py", "func_name": "MPost.create_post", "original_string": "def create_post(post_uid, post_data):\n        '''\n        create the post.\n        '''\n        title = post_data['title'].strip()\n        if len(title) < 2:\n            return False\n\n\n        cur_rec = MPost.get_by_uid(post_uid)\n        if cur_rec:\n            return False\n\n        entry = TabPost.create(\n            title=title,\n            date=datetime.now(),\n            cnt_md=tornado.escape.xhtml_escape(post_data['cnt_md'].strip()),\n            cnt_html=tools.markdown2html(post_data['cnt_md']),\n            uid=post_uid,\n            time_create=post_data.get('time_create', tools.timestamp()),\n            time_update=post_data.get('time_update', tools.timestamp()),\n            user_name=post_data['user_name'],\n            view_count=post_data['view_count'] if 'view_count' in post_data else 1,\n            logo=post_data['logo'],\n            memo=post_data['memo'] if 'memo' in post_data else '',\n            order=post_data['order'] if 'order' in post_data else '',\n            keywords=post_data['keywords'] if 'keywords' in post_data else '',\n            extinfo=post_data['extinfo'] if 'extinfo' in post_data else {},\n            kind=post_data['kind'] if 'kind' in post_data else '1',\n            valid=post_data.get('valid', 1)\n        )\n        return entry.uid", "language": "python", "code": "def create_post(post_uid, post_data):\n        '''\n        create the post.\n        '''\n        title = post_data['title'].strip()\n        if len(title) < 2:\n            return False\n\n\n        cur_rec = MPost.get_by_uid(post_uid)\n        if cur_rec:\n            return False\n\n        entry = TabPost.create(\n            title=title,\n            date=datetime.now(),\n            cnt_md=tornado.escape.xhtml_escape(post_data['cnt_md'].strip()),\n            cnt_html=tools.markdown2html(post_data['cnt_md']),\n            uid=post_uid,\n            time_create=post_data.get('time_create', tools.timestamp()),\n            time_update=post_data.get('time_update', tools.timestamp()),\n            user_name=post_data['user_name'],\n            view_count=post_data['view_count'] if 'view_count' in post_data else 1,\n            logo=post_data['logo'],\n            memo=post_data['memo'] if 'memo' in post_data else '',\n            order=post_data['order'] if 'order' in post_data else '',\n            keywords=post_data['keywords'] if 'keywords' in post_data else '',\n            extinfo=post_data['extinfo'] if 'extinfo' in post_data else {},\n            kind=post_data['kind'] if 'kind' in post_data else '1',\n            valid=post_data.get('valid', 1)\n        )\n        return entry.uid", "code_tokens": ["def", "create_post", "(", "post_uid", ",", "post_data", ")", ":", "title", "=", "post_data", "[", "'title'", "]", ".", "strip", "(", ")", "if", "len", "(", "title", ")", "<", "2", ":", "return", "False", "cur_rec", "=", "MPost", ".", "get_by_uid", "(", "post_uid", ")", "if", "cur_rec", ":", "return", "False", "entry", "=", "TabPost", ".", "create", "(", "title", "=", "title", ",", "date", "=", "datetime", ".", "now", "(", ")", ",", "cnt_md", "=", "tornado", ".", "escape", ".", "xhtml_escape", "(", "post_data", "[", "'cnt_md'", "]", ".", "strip", "(", ")", ")", ",", "cnt_html", "=", "tools", ".", "markdown2html", "(", "post_data", "[", "'cnt_md'", "]", ")", ",", "uid", "=", "post_uid", ",", "time_create", "=", "post_data", ".", "get", "(", "'time_create'", ",", "tools", ".", "timestamp", "(", ")", ")", ",", "time_update", "=", "post_data", ".", "get", "(", "'time_update'", ",", "tools", ".", "timestamp", "(", ")", ")", ",", "user_name", "=", "post_data", "[", "'user_name'", "]", ",", "view_count", "=", "post_data", "[", "'view_count'", "]", "if", "'view_count'", "in", "post_data", "else", "1", ",", "logo", "=", "post_data", "[", "'logo'", "]", ",", "memo", "=", "post_data", "[", "'memo'", "]", "if", "'memo'", "in", "post_data", "else", "''", ",", "order", "=", "post_data", "[", "'order'", "]", "if", "'order'", "in", "post_data", "else", "''", ",", "keywords", "=", "post_data", "[", "'keywords'", "]", "if", "'keywords'", "in", "post_data", "else", "''", ",", "extinfo", "=", "post_data", "[", "'extinfo'", "]", "if", "'extinfo'", "in", "post_data", "else", "{", "}", ",", "kind", "=", "post_data", "[", "'kind'", "]", "if", "'kind'", "in", "post_data", "else", "'1'", ",", "valid", "=", "post_data", ".", "get", "(", "'valid'", ",", "1", ")", ")", "return", "entry", ".", "uid"], "docstring": "create the post.", "docstring_tokens": ["create", "the", "post", "."], "sha": "6567c7fe2604a1d646d4570c017840958630ed2b", "url": "https://github.com/bukun/TorCMS/blob/6567c7fe2604a1d646d4570c017840958630ed2b/torcms/model/post_model.py#L189-L220", "partition": "train", "idx": 108732}
{"repo": "bitshares/uptick", "path": "uptick/decorators.py", "func_name": "offline", "original_string": "def offline(f):\n    \"\"\" This decorator allows you to access ``ctx.bitshares`` which is\n        an instance of BitShares with ``offline=True``.\n    \"\"\"\n\n    @click.pass_context\n    @verbose\n    def new_func(ctx, *args, **kwargs):\n        ctx.obj[\"offline\"] = True\n        ctx.bitshares = BitShares(**ctx.obj)\n        ctx.blockchain = ctx.bitshares\n        ctx.bitshares.set_shared_instance()\n        return ctx.invoke(f, *args, **kwargs)\n\n    return update_wrapper(new_func, f)", "language": "python", "code": "def offline(f):\n    \"\"\" This decorator allows you to access ``ctx.bitshares`` which is\n        an instance of BitShares with ``offline=True``.\n    \"\"\"\n\n    @click.pass_context\n    @verbose\n    def new_func(ctx, *args, **kwargs):\n        ctx.obj[\"offline\"] = True\n        ctx.bitshares = BitShares(**ctx.obj)\n        ctx.blockchain = ctx.bitshares\n        ctx.bitshares.set_shared_instance()\n        return ctx.invoke(f, *args, **kwargs)\n\n    return update_wrapper(new_func, f)", "code_tokens": ["def", "offline", "(", "f", ")", ":", "@", "click", ".", "pass_context", "@", "verbose", "def", "new_func", "(", "ctx", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "ctx", ".", "obj", "[", "\"offline\"", "]", "=", "True", "ctx", ".", "bitshares", "=", "BitShares", "(", "*", "*", "ctx", ".", "obj", ")", "ctx", ".", "blockchain", "=", "ctx", ".", "bitshares", "ctx", ".", "bitshares", ".", "set_shared_instance", "(", ")", "return", "ctx", ".", "invoke", "(", "f", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "update_wrapper", "(", "new_func", ",", "f", ")"], "docstring": "This decorator allows you to access ``ctx.bitshares`` which is\n        an instance of BitShares with ``offline=True``.", "docstring_tokens": ["This", "decorator", "allows", "you", "to", "access", "ctx", ".", "bitshares", "which", "is", "an", "instance", "of", "BitShares", "with", "offline", "=", "True", "."], "sha": "66c102200fdbf96cef4fd55cc69d00e690f62001", "url": "https://github.com/bitshares/uptick/blob/66c102200fdbf96cef4fd55cc69d00e690f62001/uptick/decorators.py#L55-L69", "partition": "train", "idx": 110782}
{"repo": "django-getpaid/django-getpaid", "path": "getpaid/backends/__init__.py", "func_name": "PaymentProcessorBase.get_order_description", "original_string": "def get_order_description(self, payment, order):\n        \"\"\"\n        Renders order description using django template provided in ``settings.GETPAID_ORDER_DESCRIPTION``\n        or if not provided return unicode representation of ``Order object``.\n        \"\"\"\n        template = getattr(settings, 'GETPAID_ORDER_DESCRIPTION', None)\n        if template:\n            return Template(template).render(Context({\"payment\": payment, \"order\": order}))\n        else:\n            return six.text_type(order)", "language": "python", "code": "def get_order_description(self, payment, order):\n        \"\"\"\n        Renders order description using django template provided in ``settings.GETPAID_ORDER_DESCRIPTION``\n        or if not provided return unicode representation of ``Order object``.\n        \"\"\"\n        template = getattr(settings, 'GETPAID_ORDER_DESCRIPTION', None)\n        if template:\n            return Template(template).render(Context({\"payment\": payment, \"order\": order}))\n        else:\n            return six.text_type(order)", "code_tokens": ["def", "get_order_description", "(", "self", ",", "payment", ",", "order", ")", ":", "template", "=", "getattr", "(", "settings", ",", "'GETPAID_ORDER_DESCRIPTION'", ",", "None", ")", "if", "template", ":", "return", "Template", "(", "template", ")", ".", "render", "(", "Context", "(", "{", "\"payment\"", ":", "payment", ",", "\"order\"", ":", "order", "}", ")", ")", "else", ":", "return", "six", ".", "text_type", "(", "order", ")"], "docstring": "Renders order description using django template provided in ``settings.GETPAID_ORDER_DESCRIPTION``\n        or if not provided return unicode representation of ``Order object``.", "docstring_tokens": ["Renders", "order", "description", "using", "django", "template", "provided", "in", "settings", ".", "GETPAID_ORDER_DESCRIPTION", "or", "if", "not", "provided", "return", "unicode", "representation", "of", "Order", "object", "."], "sha": "f32badcd0ebc28d24adceb4f649c0c2b84c03987", "url": "https://github.com/django-getpaid/django-getpaid/blob/f32badcd0ebc28d24adceb4f649c0c2b84c03987/getpaid/backends/__init__.py#L54-L63", "partition": "train", "idx": 246377}
{"repo": "OpenVolunteeringPlatform/django-ovp-search", "path": "ovp_search/signals.py", "func_name": "TiedModelRealtimeSignalProcessor.handle_address_save", "original_string": "def handle_address_save(self, sender, instance, **kwargs):\n    \"\"\" Custom handler for address save \"\"\"\n    objects = self.find_associated_with_address(instance)\n    for obj in objects:\n      self.handle_save(obj.__class__, obj)", "language": "python", "code": "def handle_address_save(self, sender, instance, **kwargs):\n    \"\"\" Custom handler for address save \"\"\"\n    objects = self.find_associated_with_address(instance)\n    for obj in objects:\n      self.handle_save(obj.__class__, obj)", "code_tokens": ["def", "handle_address_save", "(", "self", ",", "sender", ",", "instance", ",", "*", "*", "kwargs", ")", ":", "objects", "=", "self", ".", "find_associated_with_address", "(", "instance", ")", "for", "obj", "in", "objects", ":", "self", ".", "handle_save", "(", "obj", ".", "__class__", ",", "obj", ")"], "docstring": "Custom handler for address save", "docstring_tokens": ["Custom", "handler", "for", "address", "save"], "sha": "003ceecc0a87be31fe8195f65367c52631f72b57", "url": "https://github.com/OpenVolunteeringPlatform/django-ovp-search/blob/003ceecc0a87be31fe8195f65367c52631f72b57/ovp_search/signals.py#L63-L67", "partition": "train", "idx": 103477}
{"repo": "allenai/allennlp", "path": "allennlp/training/tensorboard_writer.py", "func_name": "TensorboardWriter.log_histograms", "original_string": "def log_histograms(self, model: Model, histogram_parameters: Set[str]) -> None:\n        \"\"\"\n        Send histograms of parameters to tensorboard.\n        \"\"\"\n        for name, param in model.named_parameters():\n            if name in histogram_parameters:\n                self.add_train_histogram(\"parameter_histogram/\" + name, param)", "language": "python", "code": "def log_histograms(self, model: Model, histogram_parameters: Set[str]) -> None:\n        \"\"\"\n        Send histograms of parameters to tensorboard.\n        \"\"\"\n        for name, param in model.named_parameters():\n            if name in histogram_parameters:\n                self.add_train_histogram(\"parameter_histogram/\" + name, param)", "code_tokens": ["def", "log_histograms", "(", "self", ",", "model", ":", "Model", ",", "histogram_parameters", ":", "Set", "[", "str", "]", ")", "->", "None", ":", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "if", "name", "in", "histogram_parameters", ":", "self", ".", "add_train_histogram", "(", "\"parameter_histogram/\"", "+", "name", ",", "param", ")"], "docstring": "Send histograms of parameters to tensorboard.", "docstring_tokens": ["Send", "histograms", "of", "parameters", "to", "tensorboard", "."], "sha": "648a36f77db7e45784c047176074f98534c76636", "url": "https://github.com/allenai/allennlp/blob/648a36f77db7e45784c047176074f98534c76636/allennlp/training/tensorboard_writer.py#L133-L139", "partition": "train", "idx": 163051}
{"repo": "JukeboxPipeline/jukebox-core", "path": "src/jukeboxcore/reftrack.py", "func_name": "Reftrack.fetch_alien", "original_string": "def fetch_alien(self, ):\n        \"\"\"Set and return, if the reftrack element is linked to the current scene.\n\n        Askes the refobj interface for the current scene.\n        If there is no current scene then True is returned.\n\n        :returns: whether the element is linked to the current scene\n        :rtype: bool\n        :raises: None\n        \"\"\"\n        parent = self.get_parent()\n        if parent:\n            parentelement = parent.get_element()\n        else:\n            parentelement = self.get_refobjinter().get_current_element()\n            if not parentelement:\n                self._alien = True\n                return self._alien\n        element = self.get_element()\n        if element == parentelement:\n            self._alien = False\n        # test if it is the element is a global shot\n        # first test if we have a shot\n        # then test if it is in a global sequence. then the shot is global too.\n        # test if the parent element is a shot, if they share the sequence, and element is global\n        elif isinstance(element, djadapter.models.Shot)\\\n            and (element.sequence.name == djadapter.GLOBAL_NAME\\\n            or (isinstance(parentelement, djadapter.models.Shot)\\\n                and parentelement.sequence == element.sequence and element.name == djadapter.GLOBAL_NAME)):\n            self._alien = False\n        else:\n            assets = parentelement.assets.all()\n            self._alien = element not in assets\n        return self._alien", "language": "python", "code": "def fetch_alien(self, ):\n        \"\"\"Set and return, if the reftrack element is linked to the current scene.\n\n        Askes the refobj interface for the current scene.\n        If there is no current scene then True is returned.\n\n        :returns: whether the element is linked to the current scene\n        :rtype: bool\n        :raises: None\n        \"\"\"\n        parent = self.get_parent()\n        if parent:\n            parentelement = parent.get_element()\n        else:\n            parentelement = self.get_refobjinter().get_current_element()\n            if not parentelement:\n                self._alien = True\n                return self._alien\n        element = self.get_element()\n        if element == parentelement:\n            self._alien = False\n        # test if it is the element is a global shot\n        # first test if we have a shot\n        # then test if it is in a global sequence. then the shot is global too.\n        # test if the parent element is a shot, if they share the sequence, and element is global\n        elif isinstance(element, djadapter.models.Shot)\\\n            and (element.sequence.name == djadapter.GLOBAL_NAME\\\n            or (isinstance(parentelement, djadapter.models.Shot)\\\n                and parentelement.sequence == element.sequence and element.name == djadapter.GLOBAL_NAME)):\n            self._alien = False\n        else:\n            assets = parentelement.assets.all()\n            self._alien = element not in assets\n        return self._alien", "code_tokens": ["def", "fetch_alien", "(", "self", ",", ")", ":", "parent", "=", "self", ".", "get_parent", "(", ")", "if", "parent", ":", "parentelement", "=", "parent", ".", "get_element", "(", ")", "else", ":", "parentelement", "=", "self", ".", "get_refobjinter", "(", ")", ".", "get_current_element", "(", ")", "if", "not", "parentelement", ":", "self", ".", "_alien", "=", "True", "return", "self", ".", "_alien", "element", "=", "self", ".", "get_element", "(", ")", "if", "element", "==", "parentelement", ":", "self", ".", "_alien", "=", "False", "# test if it is the element is a global shot", "# first test if we have a shot", "# then test if it is in a global sequence. then the shot is global too.", "# test if the parent element is a shot, if they share the sequence, and element is global", "elif", "isinstance", "(", "element", ",", "djadapter", ".", "models", ".", "Shot", ")", "and", "(", "element", ".", "sequence", ".", "name", "==", "djadapter", ".", "GLOBAL_NAME", "or", "(", "isinstance", "(", "parentelement", ",", "djadapter", ".", "models", ".", "Shot", ")", "and", "parentelement", ".", "sequence", "==", "element", ".", "sequence", "and", "element", ".", "name", "==", "djadapter", ".", "GLOBAL_NAME", ")", ")", ":", "self", ".", "_alien", "=", "False", "else", ":", "assets", "=", "parentelement", ".", "assets", ".", "all", "(", ")", "self", ".", "_alien", "=", "element", "not", "in", "assets", "return", "self", ".", "_alien"], "docstring": "Set and return, if the reftrack element is linked to the current scene.\n\n        Askes the refobj interface for the current scene.\n        If there is no current scene then True is returned.\n\n        :returns: whether the element is linked to the current scene\n        :rtype: bool\n        :raises: None", "docstring_tokens": ["Set", "and", "return", "if", "the", "reftrack", "element", "is", "linked", "to", "the", "current", "scene", "."], "sha": "bac2280ca49940355270e4b69400ce9976ab2e6f", "url": "https://github.com/JukeboxPipeline/jukebox-core/blob/bac2280ca49940355270e4b69400ce9976ab2e6f/src/jukeboxcore/reftrack.py#L1033-L1066", "partition": "train", "idx": 89951}
{"repo": "Alignak-monitoring/alignak", "path": "alignak/http/generic_interface.py", "func_name": "GenericInterface.stop_request", "original_string": "def stop_request(self, stop_now='0'):\n        \"\"\"Request the daemon to stop\n\n        If `stop_now` is set to '1' the daemon will stop now. Else, the daemon\n        will enter the stop wait mode. In this mode the daemon stops its activity and\n        waits until it receives a new `stop_now` request to stop really.\n\n        :param stop_now: stop now or go to stop wait mode\n        :type stop_now: bool\n        :return: None\n        \"\"\"\n        self.app.interrupted = (stop_now == '1')\n        self.app.will_stop = True\n\n        return True", "language": "python", "code": "def stop_request(self, stop_now='0'):\n        \"\"\"Request the daemon to stop\n\n        If `stop_now` is set to '1' the daemon will stop now. Else, the daemon\n        will enter the stop wait mode. In this mode the daemon stops its activity and\n        waits until it receives a new `stop_now` request to stop really.\n\n        :param stop_now: stop now or go to stop wait mode\n        :type stop_now: bool\n        :return: None\n        \"\"\"\n        self.app.interrupted = (stop_now == '1')\n        self.app.will_stop = True\n\n        return True", "code_tokens": ["def", "stop_request", "(", "self", ",", "stop_now", "=", "'0'", ")", ":", "self", ".", "app", ".", "interrupted", "=", "(", "stop_now", "==", "'1'", ")", "self", ".", "app", ".", "will_stop", "=", "True", "return", "True"], "docstring": "Request the daemon to stop\n\n        If `stop_now` is set to '1' the daemon will stop now. Else, the daemon\n        will enter the stop wait mode. In this mode the daemon stops its activity and\n        waits until it receives a new `stop_now` request to stop really.\n\n        :param stop_now: stop now or go to stop wait mode\n        :type stop_now: bool\n        :return: None", "docstring_tokens": ["Request", "the", "daemon", "to", "stop"], "sha": "f3c145207e83159b799d3714e4241399c7740a64", "url": "https://github.com/Alignak-monitoring/alignak/blob/f3c145207e83159b799d3714e4241399c7740a64/alignak/http/generic_interface.py#L143-L157", "partition": "train", "idx": 19498}
{"repo": "underworldcode/stripy", "path": "stripy-src/stripy/cartesian.py", "func_name": "Triangulation.areas", "original_string": "def areas(self):\n        \"\"\"\n        Compute the area of each triangle within the triangulation of points.\n\n        Returns\n        -------\n         area : array of floats, shape (nt,)\n            area of each triangle in self.simplices where nt\n            is the number of triangles.\n\n        \"\"\"\n        v1 = self.points[self.simplices[:,1]] - self.points[self.simplices[:,0]]\n        v2 = self.points[self.simplices[:,2]] - self.points[self.simplices[:,1]]\n\n        area = 0.5*(v1[:,0]*v2[:,1] - v1[:,1]*v2[:,0])\n        return area", "language": "python", "code": "def areas(self):\n        \"\"\"\n        Compute the area of each triangle within the triangulation of points.\n\n        Returns\n        -------\n         area : array of floats, shape (nt,)\n            area of each triangle in self.simplices where nt\n            is the number of triangles.\n\n        \"\"\"\n        v1 = self.points[self.simplices[:,1]] - self.points[self.simplices[:,0]]\n        v2 = self.points[self.simplices[:,2]] - self.points[self.simplices[:,1]]\n\n        area = 0.5*(v1[:,0]*v2[:,1] - v1[:,1]*v2[:,0])\n        return area", "code_tokens": ["def", "areas", "(", "self", ")", ":", "v1", "=", "self", ".", "points", "[", "self", ".", "simplices", "[", ":", ",", "1", "]", "]", "-", "self", ".", "points", "[", "self", ".", "simplices", "[", ":", ",", "0", "]", "]", "v2", "=", "self", ".", "points", "[", "self", ".", "simplices", "[", ":", ",", "2", "]", "]", "-", "self", ".", "points", "[", "self", ".", "simplices", "[", ":", ",", "1", "]", "]", "area", "=", "0.5", "*", "(", "v1", "[", ":", ",", "0", "]", "*", "v2", "[", ":", ",", "1", "]", "-", "v1", "[", ":", ",", "1", "]", "*", "v2", "[", ":", ",", "0", "]", ")", "return", "area"], "docstring": "Compute the area of each triangle within the triangulation of points.\n\n        Returns\n        -------\n         area : array of floats, shape (nt,)\n            area of each triangle in self.simplices where nt\n            is the number of triangles.", "docstring_tokens": ["Compute", "the", "area", "of", "each", "triangle", "within", "the", "triangulation", "of", "points", "."], "sha": "d4c3480c3e58c88489ded695eadbe7cd5bf94b48", "url": "https://github.com/underworldcode/stripy/blob/d4c3480c3e58c88489ded695eadbe7cd5bf94b48/stripy-src/stripy/cartesian.py#L871-L886", "partition": "train", "idx": 141628}
{"repo": "idlesign/dbf_light", "path": "dbf_light/light.py", "func_name": "Dbf.iter_rows", "original_string": "def iter_rows(self):\n        \"\"\"Generator reading .dbf row one by one.\n\n        Yields named tuple Row object.\n\n        :rtype: Row\n        \"\"\"\n        fileobj = self._fileobj\n        cls_row = self.cls_row\n        fields = self.fields\n\n        for idx in range(self.prolog.records_count):\n            data = fileobj.read(1)\n\n            marker = struct.unpack('<1s', data)[0]\n            is_deleted = marker == b'*'\n\n            if is_deleted:\n                continue\n\n            row_values = []\n            for field in fields:\n                val = field.cast(fileobj.read(field.len))\n                row_values.append(val)\n\n            yield cls_row(*row_values)", "language": "python", "code": "def iter_rows(self):\n        \"\"\"Generator reading .dbf row one by one.\n\n        Yields named tuple Row object.\n\n        :rtype: Row\n        \"\"\"\n        fileobj = self._fileobj\n        cls_row = self.cls_row\n        fields = self.fields\n\n        for idx in range(self.prolog.records_count):\n            data = fileobj.read(1)\n\n            marker = struct.unpack('<1s', data)[0]\n            is_deleted = marker == b'*'\n\n            if is_deleted:\n                continue\n\n            row_values = []\n            for field in fields:\n                val = field.cast(fileobj.read(field.len))\n                row_values.append(val)\n\n            yield cls_row(*row_values)", "code_tokens": ["def", "iter_rows", "(", "self", ")", ":", "fileobj", "=", "self", ".", "_fileobj", "cls_row", "=", "self", ".", "cls_row", "fields", "=", "self", ".", "fields", "for", "idx", "in", "range", "(", "self", ".", "prolog", ".", "records_count", ")", ":", "data", "=", "fileobj", ".", "read", "(", "1", ")", "marker", "=", "struct", ".", "unpack", "(", "'<1s'", ",", "data", ")", "[", "0", "]", "is_deleted", "=", "marker", "==", "b'*'", "if", "is_deleted", ":", "continue", "row_values", "=", "[", "]", "for", "field", "in", "fields", ":", "val", "=", "field", ".", "cast", "(", "fileobj", ".", "read", "(", "field", ".", "len", ")", ")", "row_values", ".", "append", "(", "val", ")", "yield", "cls_row", "(", "*", "row_values", ")"], "docstring": "Generator reading .dbf row one by one.\n\n        Yields named tuple Row object.\n\n        :rtype: Row", "docstring_tokens": ["Generator", "reading", ".", "dbf", "row", "one", "by", "one", "."], "sha": "59487bcdf92893a8a4b24239371dc0e4722c4c37", "url": "https://github.com/idlesign/dbf_light/blob/59487bcdf92893a8a4b24239371dc0e4722c4c37/dbf_light/light.py#L109-L134", "partition": "train", "idx": 75187}
{"repo": "serhatbolsu/robotframework-appiumlibrary", "path": "AppiumLibrary/keywords/_touch.py", "func_name": "_TouchKeywords.scroll_up", "original_string": "def scroll_up(self, locator):\r\n        \"\"\"Scrolls up to element\"\"\"\r\n        driver = self._current_application()\r\n        element = self._element_find(locator, True, True)\r\n        driver.execute_script(\"mobile: scroll\", {\"direction\": 'up', 'element': element.id})", "language": "python", "code": "def scroll_up(self, locator):\r\n        \"\"\"Scrolls up to element\"\"\"\r\n        driver = self._current_application()\r\n        element = self._element_find(locator, True, True)\r\n        driver.execute_script(\"mobile: scroll\", {\"direction\": 'up', 'element': element.id})", "code_tokens": ["def", "scroll_up", "(", "self", ",", "locator", ")", ":", "driver", "=", "self", ".", "_current_application", "(", ")", "element", "=", "self", ".", "_element_find", "(", "locator", ",", "True", ",", "True", ")", "driver", ".", "execute_script", "(", "\"mobile: scroll\"", ",", "{", "\"direction\"", ":", "'up'", ",", "'element'", ":", "element", ".", "id", "}", ")"], "docstring": "Scrolls up to element", "docstring_tokens": ["Scrolls", "up", "to", "element"], "sha": "91c808cf0602af6be8135ac529fa488fded04a85", "url": "https://github.com/serhatbolsu/robotframework-appiumlibrary/blob/91c808cf0602af6be8135ac529fa488fded04a85/AppiumLibrary/keywords/_touch.py#L102-L106", "partition": "train", "idx": 226388}
{"repo": "hellock/icrawler", "path": "icrawler/utils/proxy_pool.py", "func_name": "ProxyPool.get_next", "original_string": "def get_next(self, protocol='http', format=False, policy='loop'):\n        \"\"\"Get the next proxy\n\n        Args:\n            protocol (str): 'http' or 'https'. (default 'http')\n            format (bool): Whether to format the proxy. (default False)\n            policy (str): Either 'loop' or 'random', indicating the policy of\n                getting the next proxy. If set to 'loop', will return proxies\n                in turn, otherwise will return a proxy randomly.\n\n        Returns:\n            Proxy or dict: If format is true, then return the formatted proxy\n                which is compatible with requests.Session parameters,\n                otherwise a Proxy object.\n        \"\"\"\n        if not self.proxies[protocol]:\n            return None\n        if policy == 'loop':\n            idx = self.idx[protocol]\n            self.idx[protocol] = (idx + 1) % len(self.proxies[protocol])\n        elif policy == 'random':\n            idx = random.randint(0, self.proxy_num(protocol) - 1)\n        else:\n            self.logger.error('Unsupported get_next policy: {}'.format(policy))\n            exit()\n        proxy = self.proxies[protocol][self.addr_list[protocol][idx]]\n        if proxy.weight < random.random():\n            return self.get_next(protocol, format, policy)\n        if format:\n            return proxy.format()\n        else:\n            return proxy", "language": "python", "code": "def get_next(self, protocol='http', format=False, policy='loop'):\n        \"\"\"Get the next proxy\n\n        Args:\n            protocol (str): 'http' or 'https'. (default 'http')\n            format (bool): Whether to format the proxy. (default False)\n            policy (str): Either 'loop' or 'random', indicating the policy of\n                getting the next proxy. If set to 'loop', will return proxies\n                in turn, otherwise will return a proxy randomly.\n\n        Returns:\n            Proxy or dict: If format is true, then return the formatted proxy\n                which is compatible with requests.Session parameters,\n                otherwise a Proxy object.\n        \"\"\"\n        if not self.proxies[protocol]:\n            return None\n        if policy == 'loop':\n            idx = self.idx[protocol]\n            self.idx[protocol] = (idx + 1) % len(self.proxies[protocol])\n        elif policy == 'random':\n            idx = random.randint(0, self.proxy_num(protocol) - 1)\n        else:\n            self.logger.error('Unsupported get_next policy: {}'.format(policy))\n            exit()\n        proxy = self.proxies[protocol][self.addr_list[protocol][idx]]\n        if proxy.weight < random.random():\n            return self.get_next(protocol, format, policy)\n        if format:\n            return proxy.format()\n        else:\n            return proxy", "code_tokens": ["def", "get_next", "(", "self", ",", "protocol", "=", "'http'", ",", "format", "=", "False", ",", "policy", "=", "'loop'", ")", ":", "if", "not", "self", ".", "proxies", "[", "protocol", "]", ":", "return", "None", "if", "policy", "==", "'loop'", ":", "idx", "=", "self", ".", "idx", "[", "protocol", "]", "self", ".", "idx", "[", "protocol", "]", "=", "(", "idx", "+", "1", ")", "%", "len", "(", "self", ".", "proxies", "[", "protocol", "]", ")", "elif", "policy", "==", "'random'", ":", "idx", "=", "random", ".", "randint", "(", "0", ",", "self", ".", "proxy_num", "(", "protocol", ")", "-", "1", ")", "else", ":", "self", ".", "logger", ".", "error", "(", "'Unsupported get_next policy: {}'", ".", "format", "(", "policy", ")", ")", "exit", "(", ")", "proxy", "=", "self", ".", "proxies", "[", "protocol", "]", "[", "self", ".", "addr_list", "[", "protocol", "]", "[", "idx", "]", "]", "if", "proxy", ".", "weight", "<", "random", ".", "random", "(", ")", ":", "return", "self", ".", "get_next", "(", "protocol", ",", "format", ",", "policy", ")", "if", "format", ":", "return", "proxy", ".", "format", "(", ")", "else", ":", "return", "proxy"], "docstring": "Get the next proxy\n\n        Args:\n            protocol (str): 'http' or 'https'. (default 'http')\n            format (bool): Whether to format the proxy. (default False)\n            policy (str): Either 'loop' or 'random', indicating the policy of\n                getting the next proxy. If set to 'loop', will return proxies\n                in turn, otherwise will return a proxy randomly.\n\n        Returns:\n            Proxy or dict: If format is true, then return the formatted proxy\n                which is compatible with requests.Session parameters,\n                otherwise a Proxy object.", "docstring_tokens": ["Get", "the", "next", "proxy"], "sha": "38c925758fd3d3e568d3ecc993f77bc0acfa4788", "url": "https://github.com/hellock/icrawler/blob/38c925758fd3d3e568d3ecc993f77bc0acfa4788/icrawler/utils/proxy_pool.py#L123-L154", "partition": "train", "idx": 215421}
{"repo": "tensorflow/tensor2tensor", "path": "tensor2tensor/models/video/savp.py", "func_name": "NextFrameSavpBase.get_fc_dimensions", "original_string": "def get_fc_dimensions(self, strides, kernel_sizes):\n    \"\"\"Get expected fully connected shape after a series of convolutions.\"\"\"\n    output_height, output_width, _ = self.hparams.problem.frame_shape\n    output_steps = self.hparams.video_num_target_frames\n    output_shape = np.array([output_steps, output_height, output_width])\n    for curr_stride, kernel_size in zip(strides, kernel_sizes):\n      output_shape = self.expected_output_shape(\n          output_shape, np.array(curr_stride), 1, kernel_size)\n    return np.prod(output_shape) * self.hparams.num_discriminator_filters * 8", "language": "python", "code": "def get_fc_dimensions(self, strides, kernel_sizes):\n    \"\"\"Get expected fully connected shape after a series of convolutions.\"\"\"\n    output_height, output_width, _ = self.hparams.problem.frame_shape\n    output_steps = self.hparams.video_num_target_frames\n    output_shape = np.array([output_steps, output_height, output_width])\n    for curr_stride, kernel_size in zip(strides, kernel_sizes):\n      output_shape = self.expected_output_shape(\n          output_shape, np.array(curr_stride), 1, kernel_size)\n    return np.prod(output_shape) * self.hparams.num_discriminator_filters * 8", "code_tokens": ["def", "get_fc_dimensions", "(", "self", ",", "strides", ",", "kernel_sizes", ")", ":", "output_height", ",", "output_width", ",", "_", "=", "self", ".", "hparams", ".", "problem", ".", "frame_shape", "output_steps", "=", "self", ".", "hparams", ".", "video_num_target_frames", "output_shape", "=", "np", ".", "array", "(", "[", "output_steps", ",", "output_height", ",", "output_width", "]", ")", "for", "curr_stride", ",", "kernel_size", "in", "zip", "(", "strides", ",", "kernel_sizes", ")", ":", "output_shape", "=", "self", ".", "expected_output_shape", "(", "output_shape", ",", "np", ".", "array", "(", "curr_stride", ")", ",", "1", ",", "kernel_size", ")", "return", "np", ".", "prod", "(", "output_shape", ")", "*", "self", ".", "hparams", ".", "num_discriminator_filters", "*", "8"], "docstring": "Get expected fully connected shape after a series of convolutions.", "docstring_tokens": ["Get", "expected", "fully", "connected", "shape", "after", "a", "series", "of", "convolutions", "."], "sha": "272500b6efe353aeb638d2745ed56e519462ca31", "url": "https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/models/video/savp.py#L110-L118", "partition": "train", "idx": 161765}
{"repo": "saltstack/salt", "path": "salt/returners/local_cache.py", "func_name": "prep_jid", "original_string": "def prep_jid(nocache=False, passed_jid=None, recurse_count=0):\n    '''\n    Return a job id and prepare the job id directory.\n\n    This is the function responsible for making sure jids don't collide (unless\n    it is passed a jid).\n    So do what you have to do to make sure that stays the case\n    '''\n    if recurse_count >= 5:\n        err = 'prep_jid could not store a jid after {0} tries.'.format(recurse_count)\n        log.error(err)\n        raise salt.exceptions.SaltCacheError(err)\n    if passed_jid is None:  # this can be a None or an empty string.\n        jid = salt.utils.jid.gen_jid(__opts__)\n    else:\n        jid = passed_jid\n\n    jid_dir = salt.utils.jid.jid_dir(jid, _job_dir(), __opts__['hash_type'])\n\n    # Make sure we create the jid dir, otherwise someone else is using it,\n    # meaning we need a new jid.\n    if not os.path.isdir(jid_dir):\n        try:\n            os.makedirs(jid_dir)\n        except OSError:\n            time.sleep(0.1)\n            if passed_jid is None:\n                return prep_jid(nocache=nocache, recurse_count=recurse_count+1)\n\n    try:\n        with salt.utils.files.fopen(os.path.join(jid_dir, 'jid'), 'wb+') as fn_:\n            fn_.write(salt.utils.stringutils.to_bytes(jid))\n        if nocache:\n            with salt.utils.files.fopen(os.path.join(jid_dir, 'nocache'), 'wb+'):\n                pass\n    except IOError:\n        log.warning(\n            'Could not write out jid file for job %s. Retrying.', jid)\n        time.sleep(0.1)\n        return prep_jid(passed_jid=jid, nocache=nocache,\n                        recurse_count=recurse_count+1)\n\n    return jid", "language": "python", "code": "def prep_jid(nocache=False, passed_jid=None, recurse_count=0):\n    '''\n    Return a job id and prepare the job id directory.\n\n    This is the function responsible for making sure jids don't collide (unless\n    it is passed a jid).\n    So do what you have to do to make sure that stays the case\n    '''\n    if recurse_count >= 5:\n        err = 'prep_jid could not store a jid after {0} tries.'.format(recurse_count)\n        log.error(err)\n        raise salt.exceptions.SaltCacheError(err)\n    if passed_jid is None:  # this can be a None or an empty string.\n        jid = salt.utils.jid.gen_jid(__opts__)\n    else:\n        jid = passed_jid\n\n    jid_dir = salt.utils.jid.jid_dir(jid, _job_dir(), __opts__['hash_type'])\n\n    # Make sure we create the jid dir, otherwise someone else is using it,\n    # meaning we need a new jid.\n    if not os.path.isdir(jid_dir):\n        try:\n            os.makedirs(jid_dir)\n        except OSError:\n            time.sleep(0.1)\n            if passed_jid is None:\n                return prep_jid(nocache=nocache, recurse_count=recurse_count+1)\n\n    try:\n        with salt.utils.files.fopen(os.path.join(jid_dir, 'jid'), 'wb+') as fn_:\n            fn_.write(salt.utils.stringutils.to_bytes(jid))\n        if nocache:\n            with salt.utils.files.fopen(os.path.join(jid_dir, 'nocache'), 'wb+'):\n                pass\n    except IOError:\n        log.warning(\n            'Could not write out jid file for job %s. Retrying.', jid)\n        time.sleep(0.1)\n        return prep_jid(passed_jid=jid, nocache=nocache,\n                        recurse_count=recurse_count+1)\n\n    return jid", "code_tokens": ["def", "prep_jid", "(", "nocache", "=", "False", ",", "passed_jid", "=", "None", ",", "recurse_count", "=", "0", ")", ":", "if", "recurse_count", ">=", "5", ":", "err", "=", "'prep_jid could not store a jid after {0} tries.'", ".", "format", "(", "recurse_count", ")", "log", ".", "error", "(", "err", ")", "raise", "salt", ".", "exceptions", ".", "SaltCacheError", "(", "err", ")", "if", "passed_jid", "is", "None", ":", "# this can be a None or an empty string.", "jid", "=", "salt", ".", "utils", ".", "jid", ".", "gen_jid", "(", "__opts__", ")", "else", ":", "jid", "=", "passed_jid", "jid_dir", "=", "salt", ".", "utils", ".", "jid", ".", "jid_dir", "(", "jid", ",", "_job_dir", "(", ")", ",", "__opts__", "[", "'hash_type'", "]", ")", "# Make sure we create the jid dir, otherwise someone else is using it,", "# meaning we need a new jid.", "if", "not", "os", ".", "path", ".", "isdir", "(", "jid_dir", ")", ":", "try", ":", "os", ".", "makedirs", "(", "jid_dir", ")", "except", "OSError", ":", "time", ".", "sleep", "(", "0.1", ")", "if", "passed_jid", "is", "None", ":", "return", "prep_jid", "(", "nocache", "=", "nocache", ",", "recurse_count", "=", "recurse_count", "+", "1", ")", "try", ":", "with", "salt", ".", "utils", ".", "files", ".", "fopen", "(", "os", ".", "path", ".", "join", "(", "jid_dir", ",", "'jid'", ")", ",", "'wb+'", ")", "as", "fn_", ":", "fn_", ".", "write", "(", "salt", ".", "utils", ".", "stringutils", ".", "to_bytes", "(", "jid", ")", ")", "if", "nocache", ":", "with", "salt", ".", "utils", ".", "files", ".", "fopen", "(", "os", ".", "path", ".", "join", "(", "jid_dir", ",", "'nocache'", ")", ",", "'wb+'", ")", ":", "pass", "except", "IOError", ":", "log", ".", "warning", "(", "'Could not write out jid file for job %s. Retrying.'", ",", "jid", ")", "time", ".", "sleep", "(", "0.1", ")", "return", "prep_jid", "(", "passed_jid", "=", "jid", ",", "nocache", "=", "nocache", ",", "recurse_count", "=", "recurse_count", "+", "1", ")", "return", "jid"], "docstring": "Return a job id and prepare the job id directory.\n\n    This is the function responsible for making sure jids don't collide (unless\n    it is passed a jid).\n    So do what you have to do to make sure that stays the case", "docstring_tokens": ["Return", "a", "job", "id", "and", "prepare", "the", "job", "id", "directory", "."], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/returners/local_cache.py#L88-L130", "partition": "train", "idx": 175623}
{"repo": "croscon/fleaker", "path": "fleaker/config.py", "func_name": "MultiStageConfigurableApp._configure_from_mapping", "original_string": "def _configure_from_mapping(self, item, whitelist_keys=False,\n                                whitelist=None):\n        \"\"\"Configure from a mapping, or dict, like object.\n\n        Args:\n            item (dict):\n                A dict-like object that we can pluck values from.\n\n        Keyword Args:\n            whitelist_keys (bool):\n                Should we whitelist the keys before adding them to the\n                configuration? If no whitelist is provided, we use the\n                pre-existing config keys as a whitelist.\n            whitelist (list[str]):\n                An explicit list of keys that should be allowed. If provided\n                and ``whitelist_keys`` is true, we will use that as our\n                whitelist instead of pre-existing app config keys.\n\n        Returns:\n            fleaker.App:\n                Returns itself.\n        \"\"\"\n        if whitelist is None:\n            whitelist = self.config.keys()\n\n        if whitelist_keys:\n            item = {k: v for k, v in item.items() if k in whitelist}\n\n        self.config.from_mapping(item)\n\n        return self", "language": "python", "code": "def _configure_from_mapping(self, item, whitelist_keys=False,\n                                whitelist=None):\n        \"\"\"Configure from a mapping, or dict, like object.\n\n        Args:\n            item (dict):\n                A dict-like object that we can pluck values from.\n\n        Keyword Args:\n            whitelist_keys (bool):\n                Should we whitelist the keys before adding them to the\n                configuration? If no whitelist is provided, we use the\n                pre-existing config keys as a whitelist.\n            whitelist (list[str]):\n                An explicit list of keys that should be allowed. If provided\n                and ``whitelist_keys`` is true, we will use that as our\n                whitelist instead of pre-existing app config keys.\n\n        Returns:\n            fleaker.App:\n                Returns itself.\n        \"\"\"\n        if whitelist is None:\n            whitelist = self.config.keys()\n\n        if whitelist_keys:\n            item = {k: v for k, v in item.items() if k in whitelist}\n\n        self.config.from_mapping(item)\n\n        return self", "code_tokens": ["def", "_configure_from_mapping", "(", "self", ",", "item", ",", "whitelist_keys", "=", "False", ",", "whitelist", "=", "None", ")", ":", "if", "whitelist", "is", "None", ":", "whitelist", "=", "self", ".", "config", ".", "keys", "(", ")", "if", "whitelist_keys", ":", "item", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "item", ".", "items", "(", ")", "if", "k", "in", "whitelist", "}", "self", ".", "config", ".", "from_mapping", "(", "item", ")", "return", "self"], "docstring": "Configure from a mapping, or dict, like object.\n\n        Args:\n            item (dict):\n                A dict-like object that we can pluck values from.\n\n        Keyword Args:\n            whitelist_keys (bool):\n                Should we whitelist the keys before adding them to the\n                configuration? If no whitelist is provided, we use the\n                pre-existing config keys as a whitelist.\n            whitelist (list[str]):\n                An explicit list of keys that should be allowed. If provided\n                and ``whitelist_keys`` is true, we will use that as our\n                whitelist instead of pre-existing app config keys.\n\n        Returns:\n            fleaker.App:\n                Returns itself.", "docstring_tokens": ["Configure", "from", "a", "mapping", "or", "dict", "like", "object", "."], "sha": "046b026b79c9912bceebb17114bc0c5d2d02e3c7", "url": "https://github.com/croscon/fleaker/blob/046b026b79c9912bceebb17114bc0c5d2d02e3c7/fleaker/config.py#L220-L250", "partition": "train", "idx": 111685}
{"repo": "amol-/dukpy", "path": "dukpy/tsc.py", "func_name": "typescript_compile", "original_string": "def typescript_compile(source):\n    \"\"\"Compiles the given ``source`` from TypeScript to ES5 using TypescriptServices.js\"\"\"\n    with open(TS_COMPILER, 'r') as tsservices_js:\n        return evaljs(\n            (tsservices_js.read(),\n             'ts.transpile(dukpy.tscode, {options});'.format(options=TSC_OPTIONS)),\n            tscode=source\n        )", "language": "python", "code": "def typescript_compile(source):\n    \"\"\"Compiles the given ``source`` from TypeScript to ES5 using TypescriptServices.js\"\"\"\n    with open(TS_COMPILER, 'r') as tsservices_js:\n        return evaljs(\n            (tsservices_js.read(),\n             'ts.transpile(dukpy.tscode, {options});'.format(options=TSC_OPTIONS)),\n            tscode=source\n        )", "code_tokens": ["def", "typescript_compile", "(", "source", ")", ":", "with", "open", "(", "TS_COMPILER", ",", "'r'", ")", "as", "tsservices_js", ":", "return", "evaljs", "(", "(", "tsservices_js", ".", "read", "(", ")", ",", "'ts.transpile(dukpy.tscode, {options});'", ".", "format", "(", "options", "=", "TSC_OPTIONS", ")", ")", ",", "tscode", "=", "source", ")"], "docstring": "Compiles the given ``source`` from TypeScript to ES5 using TypescriptServices.js", "docstring_tokens": ["Compiles", "the", "given", "source", "from", "TypeScript", "to", "ES5", "using", "TypescriptServices", ".", "js"], "sha": "69f56f375a217c9f907499c28dbc964af76feae6", "url": "https://github.com/amol-/dukpy/blob/69f56f375a217c9f907499c28dbc964af76feae6/dukpy/tsc.py#L8-L15", "partition": "train", "idx": 240172}
{"repo": "Fantomas42/django-blog-zinnia", "path": "zinnia/admin/filters.py", "func_name": "RelatedPublishedFilter.lookups", "original_string": "def lookups(self, request, model_admin):\n        \"\"\"\n        Return published objects with the number of entries.\n        \"\"\"\n        active_objects = self.model.published.all().annotate(\n            count_entries_published=Count('entries')).order_by(\n            '-count_entries_published', '-pk')\n        for active_object in active_objects:\n            yield (\n                str(active_object.pk), ungettext_lazy(\n                    '%(item)s (%(count)i entry)',\n                    '%(item)s (%(count)i entries)',\n                    active_object.count_entries_published) % {\n                    'item': smart_text(active_object),\n                    'count': active_object.count_entries_published})", "language": "python", "code": "def lookups(self, request, model_admin):\n        \"\"\"\n        Return published objects with the number of entries.\n        \"\"\"\n        active_objects = self.model.published.all().annotate(\n            count_entries_published=Count('entries')).order_by(\n            '-count_entries_published', '-pk')\n        for active_object in active_objects:\n            yield (\n                str(active_object.pk), ungettext_lazy(\n                    '%(item)s (%(count)i entry)',\n                    '%(item)s (%(count)i entries)',\n                    active_object.count_entries_published) % {\n                    'item': smart_text(active_object),\n                    'count': active_object.count_entries_published})", "code_tokens": ["def", "lookups", "(", "self", ",", "request", ",", "model_admin", ")", ":", "active_objects", "=", "self", ".", "model", ".", "published", ".", "all", "(", ")", ".", "annotate", "(", "count_entries_published", "=", "Count", "(", "'entries'", ")", ")", ".", "order_by", "(", "'-count_entries_published'", ",", "'-pk'", ")", "for", "active_object", "in", "active_objects", ":", "yield", "(", "str", "(", "active_object", ".", "pk", ")", ",", "ungettext_lazy", "(", "'%(item)s (%(count)i entry)'", ",", "'%(item)s (%(count)i entries)'", ",", "active_object", ".", "count_entries_published", ")", "%", "{", "'item'", ":", "smart_text", "(", "active_object", ")", ",", "'count'", ":", "active_object", ".", "count_entries_published", "}", ")"], "docstring": "Return published objects with the number of entries.", "docstring_tokens": ["Return", "published", "objects", "with", "the", "number", "of", "entries", "."], "sha": "b4949304b104a8e1a7a7a0773cbfd024313c3a15", "url": "https://github.com/Fantomas42/django-blog-zinnia/blob/b4949304b104a8e1a7a7a0773cbfd024313c3a15/zinnia/admin/filters.py#L19-L33", "partition": "train", "idx": 148707}
{"repo": "spacetelescope/stsci.tools", "path": "lib/stsci/tools/fileutil.py", "func_name": "untranslateName", "original_string": "def untranslateName(s):\n    \"\"\"Undo Python conversion of CL parameter or variable name.\"\"\"\n\n    s = s.replace('DOT', '.')\n    s = s.replace('DOLLAR', '$')\n    # delete 'PY' at start of name components\n    if s[:2] == 'PY': s = s[2:]\n    s = s.replace('.PY', '.')\n    return s", "language": "python", "code": "def untranslateName(s):\n    \"\"\"Undo Python conversion of CL parameter or variable name.\"\"\"\n\n    s = s.replace('DOT', '.')\n    s = s.replace('DOLLAR', '$')\n    # delete 'PY' at start of name components\n    if s[:2] == 'PY': s = s[2:]\n    s = s.replace('.PY', '.')\n    return s", "code_tokens": ["def", "untranslateName", "(", "s", ")", ":", "s", "=", "s", ".", "replace", "(", "'DOT'", ",", "'.'", ")", "s", "=", "s", ".", "replace", "(", "'DOLLAR'", ",", "'$'", ")", "# delete 'PY' at start of name components", "if", "s", "[", ":", "2", "]", "==", "'PY'", ":", "s", "=", "s", "[", "2", ":", "]", "s", "=", "s", ".", "replace", "(", "'.PY'", ",", "'.'", ")", "return", "s"], "docstring": "Undo Python conversion of CL parameter or variable name.", "docstring_tokens": ["Undo", "Python", "conversion", "of", "CL", "parameter", "or", "variable", "name", "."], "sha": "9a022503ad24ca54ce83331482dfa3ff6de9f403", "url": "https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/fileutil.py#L1291-L1299", "partition": "train", "idx": 193554}
{"repo": "oceanprotocol/squid-py", "path": "squid_py/aquarius/aquarius.py", "func_name": "Aquarius.get_asset_ddo", "original_string": "def get_asset_ddo(self, did):\n        \"\"\"\n        Retrieve asset ddo for a given did.\n\n        :param did: Asset DID string\n        :return: DDO instance\n        \"\"\"\n        response = self.requests_session.get(f'{self.url}/{did}').content\n        if not response:\n            return {}\n        try:\n            parsed_response = json.loads(response)\n        except TypeError:\n            parsed_response = None\n        except ValueError:\n            raise ValueError(response.decode('UTF-8'))\n        if parsed_response is None:\n            return {}\n        return Asset(dictionary=parsed_response)", "language": "python", "code": "def get_asset_ddo(self, did):\n        \"\"\"\n        Retrieve asset ddo for a given did.\n\n        :param did: Asset DID string\n        :return: DDO instance\n        \"\"\"\n        response = self.requests_session.get(f'{self.url}/{did}').content\n        if not response:\n            return {}\n        try:\n            parsed_response = json.loads(response)\n        except TypeError:\n            parsed_response = None\n        except ValueError:\n            raise ValueError(response.decode('UTF-8'))\n        if parsed_response is None:\n            return {}\n        return Asset(dictionary=parsed_response)", "code_tokens": ["def", "get_asset_ddo", "(", "self", ",", "did", ")", ":", "response", "=", "self", ".", "requests_session", ".", "get", "(", "f'{self.url}/{did}'", ")", ".", "content", "if", "not", "response", ":", "return", "{", "}", "try", ":", "parsed_response", "=", "json", ".", "loads", "(", "response", ")", "except", "TypeError", ":", "parsed_response", "=", "None", "except", "ValueError", ":", "raise", "ValueError", "(", "response", ".", "decode", "(", "'UTF-8'", ")", ")", "if", "parsed_response", "is", "None", ":", "return", "{", "}", "return", "Asset", "(", "dictionary", "=", "parsed_response", ")"], "docstring": "Retrieve asset ddo for a given did.\n\n        :param did: Asset DID string\n        :return: DDO instance", "docstring_tokens": ["Retrieve", "asset", "ddo", "for", "a", "given", "did", "."], "sha": "43a5b7431627e4c9ab7382ed9eb8153e96ed4483", "url": "https://github.com/oceanprotocol/squid-py/blob/43a5b7431627e4c9ab7382ed9eb8153e96ed4483/squid_py/aquarius/aquarius.py#L79-L97", "partition": "train", "idx": 235392}
{"repo": "GoogleCloudPlatform/appengine-mapreduce", "path": "python/src/mapreduce/records.py", "func_name": "RecordsWriter._pad_block", "original_string": "def _pad_block(self):\n    \"\"\"Pad block with 0.\n\n    Pad current block with 0. Reader will simply treat these as corrupted\n    record and skip the block.\n\n    This method is idempotent.\n    \"\"\"\n    pad_length = _BLOCK_SIZE - self.__position % _BLOCK_SIZE\n    if pad_length and pad_length != _BLOCK_SIZE:\n      self.__writer.write('\\x00' * pad_length)\n      self.__position += pad_length", "language": "python", "code": "def _pad_block(self):\n    \"\"\"Pad block with 0.\n\n    Pad current block with 0. Reader will simply treat these as corrupted\n    record and skip the block.\n\n    This method is idempotent.\n    \"\"\"\n    pad_length = _BLOCK_SIZE - self.__position % _BLOCK_SIZE\n    if pad_length and pad_length != _BLOCK_SIZE:\n      self.__writer.write('\\x00' * pad_length)\n      self.__position += pad_length", "code_tokens": ["def", "_pad_block", "(", "self", ")", ":", "pad_length", "=", "_BLOCK_SIZE", "-", "self", ".", "__position", "%", "_BLOCK_SIZE", "if", "pad_length", "and", "pad_length", "!=", "_BLOCK_SIZE", ":", "self", ".", "__writer", ".", "write", "(", "'\\x00'", "*", "pad_length", ")", "self", ".", "__position", "+=", "pad_length"], "docstring": "Pad block with 0.\n\n    Pad current block with 0. Reader will simply treat these as corrupted\n    record and skip the block.\n\n    This method is idempotent.", "docstring_tokens": ["Pad", "block", "with", "0", "."], "sha": "2045eb3605b6ecb40c83d11dd5442a89fe5c5dd6", "url": "https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/2045eb3605b6ecb40c83d11dd5442a89fe5c5dd6/python/src/mapreduce/records.py#L213-L224", "partition": "train", "idx": 115753}
{"repo": "xflr6/bitsets", "path": "bitsets/transform.py", "func_name": "packbools", "original_string": "def packbools(bools, dtype='L'):\n    \"\"\"Yield integers concatenating bools in chunks of dtype bit-length.\n\n    >>> list(packbools([False, True, False, True, False, True], 'B'))\n    [42]\n    \"\"\"\n    r = NBITS[dtype]\n    atoms = ATOMS[dtype]\n\n    for chunk in zip_longest(*[iter(bools)] * r, fillvalue=False):\n        yield sum(compress(atoms, chunk))", "language": "python", "code": "def packbools(bools, dtype='L'):\n    \"\"\"Yield integers concatenating bools in chunks of dtype bit-length.\n\n    >>> list(packbools([False, True, False, True, False, True], 'B'))\n    [42]\n    \"\"\"\n    r = NBITS[dtype]\n    atoms = ATOMS[dtype]\n\n    for chunk in zip_longest(*[iter(bools)] * r, fillvalue=False):\n        yield sum(compress(atoms, chunk))", "code_tokens": ["def", "packbools", "(", "bools", ",", "dtype", "=", "'L'", ")", ":", "r", "=", "NBITS", "[", "dtype", "]", "atoms", "=", "ATOMS", "[", "dtype", "]", "for", "chunk", "in", "zip_longest", "(", "*", "[", "iter", "(", "bools", ")", "]", "*", "r", ",", "fillvalue", "=", "False", ")", ":", "yield", "sum", "(", "compress", "(", "atoms", ",", "chunk", ")", ")"], "docstring": "Yield integers concatenating bools in chunks of dtype bit-length.\n\n    >>> list(packbools([False, True, False, True, False, True], 'B'))\n    [42]", "docstring_tokens": ["Yield", "integers", "concatenating", "bools", "in", "chunks", "of", "dtype", "bit", "-", "length", "."], "sha": "ddcfe17e7c7a11f71f1c6764b2cecf7db05d9cdf", "url": "https://github.com/xflr6/bitsets/blob/ddcfe17e7c7a11f71f1c6764b2cecf7db05d9cdf/bitsets/transform.py#L93-L103", "partition": "train", "idx": 3724}
{"repo": "seomoz/qless-py", "path": "qless/workers/__init__.py", "func_name": "Worker.signals", "original_string": "def signals(self, signals=('QUIT', 'USR1', 'USR2')):\n        '''Register our signal handler'''\n        for sig in signals:\n            signal.signal(getattr(signal, 'SIG' + sig), self.handler)", "language": "python", "code": "def signals(self, signals=('QUIT', 'USR1', 'USR2')):\n        '''Register our signal handler'''\n        for sig in signals:\n            signal.signal(getattr(signal, 'SIG' + sig), self.handler)", "code_tokens": ["def", "signals", "(", "self", ",", "signals", "=", "(", "'QUIT'", ",", "'USR1'", ",", "'USR2'", ")", ")", ":", "for", "sig", "in", "signals", ":", "signal", ".", "signal", "(", "getattr", "(", "signal", ",", "'SIG'", "+", "sig", ")", ",", "self", ".", "handler", ")"], "docstring": "Register our signal handler", "docstring_tokens": ["Register", "our", "signal", "handler"], "sha": "3eda4ffcd4c0016c9a7e44f780d6155e1a354dda", "url": "https://github.com/seomoz/qless-py/blob/3eda4ffcd4c0016c9a7e44f780d6155e1a354dda/qless/workers/__init__.py#L175-L178", "partition": "train", "idx": 44774}
{"repo": "nefarioustim/parker", "path": "parker/workers.py", "func_name": "crawler", "original_string": "def crawler(site, uri=None):\n    \"\"\"Crawl URI using site config.\"\"\"\n    config = load_site_config(site)\n    model = _get_model('crawl', config, uri)\n    visited_set, visited_uri_set, consume_set, crawl_set = get_site_sets(\n        site, config\n    )\n\n    if not visited_set.has(model.hash):\n        visited_set.add(model.hash)\n        visited_uri_set.add(model.uri)\n\n        if (\n            model.is_consume_page\n            and not consume_set.has(model.hash)\n        ):\n            consume_set.add(model.hash)\n            consume_q.enqueue(\n                consumer,\n                site,\n                model.uri\n            )\n        else:\n            for crawl_uri in model.uris_to_crawl:\n                if (\n                    not visited_uri_set.has(crawl_uri)\n                    and not crawl_set.has(crawl_uri)\n                ):\n                    crawl_set.add(crawl_uri)\n                    crawl_q.enqueue(\n                        crawler,\n                        site,\n                        crawl_uri\n                    )", "language": "python", "code": "def crawler(site, uri=None):\n    \"\"\"Crawl URI using site config.\"\"\"\n    config = load_site_config(site)\n    model = _get_model('crawl', config, uri)\n    visited_set, visited_uri_set, consume_set, crawl_set = get_site_sets(\n        site, config\n    )\n\n    if not visited_set.has(model.hash):\n        visited_set.add(model.hash)\n        visited_uri_set.add(model.uri)\n\n        if (\n            model.is_consume_page\n            and not consume_set.has(model.hash)\n        ):\n            consume_set.add(model.hash)\n            consume_q.enqueue(\n                consumer,\n                site,\n                model.uri\n            )\n        else:\n            for crawl_uri in model.uris_to_crawl:\n                if (\n                    not visited_uri_set.has(crawl_uri)\n                    and not crawl_set.has(crawl_uri)\n                ):\n                    crawl_set.add(crawl_uri)\n                    crawl_q.enqueue(\n                        crawler,\n                        site,\n                        crawl_uri\n                    )", "code_tokens": ["def", "crawler", "(", "site", ",", "uri", "=", "None", ")", ":", "config", "=", "load_site_config", "(", "site", ")", "model", "=", "_get_model", "(", "'crawl'", ",", "config", ",", "uri", ")", "visited_set", ",", "visited_uri_set", ",", "consume_set", ",", "crawl_set", "=", "get_site_sets", "(", "site", ",", "config", ")", "if", "not", "visited_set", ".", "has", "(", "model", ".", "hash", ")", ":", "visited_set", ".", "add", "(", "model", ".", "hash", ")", "visited_uri_set", ".", "add", "(", "model", ".", "uri", ")", "if", "(", "model", ".", "is_consume_page", "and", "not", "consume_set", ".", "has", "(", "model", ".", "hash", ")", ")", ":", "consume_set", ".", "add", "(", "model", ".", "hash", ")", "consume_q", ".", "enqueue", "(", "consumer", ",", "site", ",", "model", ".", "uri", ")", "else", ":", "for", "crawl_uri", "in", "model", ".", "uris_to_crawl", ":", "if", "(", "not", "visited_uri_set", ".", "has", "(", "crawl_uri", ")", "and", "not", "crawl_set", ".", "has", "(", "crawl_uri", ")", ")", ":", "crawl_set", ".", "add", "(", "crawl_uri", ")", "crawl_q", ".", "enqueue", "(", "crawler", ",", "site", ",", "crawl_uri", ")"], "docstring": "Crawl URI using site config.", "docstring_tokens": ["Crawl", "URI", "using", "site", "config", "."], "sha": "ccc1de1ac6bfb5e0a8cfa4fdebb2f38f2ee027d6", "url": "https://github.com/nefarioustim/parker/blob/ccc1de1ac6bfb5e0a8cfa4fdebb2f38f2ee027d6/parker/workers.py#L34-L67", "partition": "train", "idx": 96468}
{"repo": "maceoutliner/django-fiction-outlines", "path": "fiction_outlines/models.py", "func_name": "Arc.validate_milestones", "original_string": "def validate_milestones(self):\n        '''\n        Reviews the arc element tree to ensure that milestones appear in the right\n        order.\n        '''\n        milestones = self.arc_root_node.get_children().filter(arc_element_type__contains='mile')\n        current_cursor = 0\n        for mile in milestones:\n            seq = mile.milestone_seq\n            if seq < current_cursor:\n                return mile\n            current_cursor = seq\n        return None", "language": "python", "code": "def validate_milestones(self):\n        '''\n        Reviews the arc element tree to ensure that milestones appear in the right\n        order.\n        '''\n        milestones = self.arc_root_node.get_children().filter(arc_element_type__contains='mile')\n        current_cursor = 0\n        for mile in milestones:\n            seq = mile.milestone_seq\n            if seq < current_cursor:\n                return mile\n            current_cursor = seq\n        return None", "code_tokens": ["def", "validate_milestones", "(", "self", ")", ":", "milestones", "=", "self", ".", "arc_root_node", ".", "get_children", "(", ")", ".", "filter", "(", "arc_element_type__contains", "=", "'mile'", ")", "current_cursor", "=", "0", "for", "mile", "in", "milestones", ":", "seq", "=", "mile", ".", "milestone_seq", "if", "seq", "<", "current_cursor", ":", "return", "mile", "current_cursor", "=", "seq", "return", "None"], "docstring": "Reviews the arc element tree to ensure that milestones appear in the right\n        order.", "docstring_tokens": ["Reviews", "the", "arc", "element", "tree", "to", "ensure", "that", "milestones", "appear", "in", "the", "right", "order", "."], "sha": "6c58e356af3fbe7b23557643ba27e46eaef9d4e3", "url": "https://github.com/maceoutliner/django-fiction-outlines/blob/6c58e356af3fbe7b23557643ba27e46eaef9d4e3/fiction_outlines/models.py#L676-L688", "partition": "train", "idx": 93536}
{"repo": "metric-learn/metric-learn", "path": "metric_learn/base_metric.py", "func_name": "MahalanobisMixin.score_pairs", "original_string": "def score_pairs(self, pairs):\n    \"\"\"Returns the learned Mahalanobis distance between pairs.\n\n    This distance is defined as: :math:`d_M(x, x') = \\sqrt{(x-x')^T M (x-x')}`\n    where ``M`` is the learned Mahalanobis matrix, for every pair of points\n    ``x`` and ``x'``. This corresponds to the euclidean distance between\n    embeddings of the points in a new space, obtained through a linear\n    transformation. Indeed, we have also: :math:`d_M(x, x') = \\sqrt{(x_e -\n    x_e')^T (x_e- x_e')}`, with :math:`x_e = L x` (See\n    :class:`MahalanobisMixin`).\n\n    Parameters\n    ----------\n    pairs : array-like, shape=(n_pairs, 2, n_features) or (n_pairs, 2)\n      3D Array of pairs to score, with each row corresponding to two points,\n      for 2D array of indices of pairs if the metric learner uses a\n      preprocessor.\n\n    Returns\n    -------\n    scores: `numpy.ndarray` of shape=(n_pairs,)\n      The learned Mahalanobis distance for every pair.\n\n    See Also\n    --------\n    get_metric : a method that returns a function to compute the metric between\n      two points. The difference with `score_pairs` is that it works on two 1D\n      arrays and cannot use a preprocessor. Besides, the returned function is\n      independent of the metric learner and hence is not modified if the metric\n      learner is.\n\n    :ref:`mahalanobis_distances` : The section of the project documentation\n      that describes Mahalanobis Distances.\n    \"\"\"\n    pairs = check_input(pairs, type_of_inputs='tuples',\n                        preprocessor=self.preprocessor_,\n                        estimator=self, tuple_size=2)\n    pairwise_diffs = self.transform(pairs[:, 1, :] - pairs[:, 0, :])\n    # (for MahalanobisMixin, the embedding is linear so we can just embed the\n    # difference)\n    return np.sqrt(np.sum(pairwise_diffs**2, axis=-1))", "language": "python", "code": "def score_pairs(self, pairs):\n    \"\"\"Returns the learned Mahalanobis distance between pairs.\n\n    This distance is defined as: :math:`d_M(x, x') = \\sqrt{(x-x')^T M (x-x')}`\n    where ``M`` is the learned Mahalanobis matrix, for every pair of points\n    ``x`` and ``x'``. This corresponds to the euclidean distance between\n    embeddings of the points in a new space, obtained through a linear\n    transformation. Indeed, we have also: :math:`d_M(x, x') = \\sqrt{(x_e -\n    x_e')^T (x_e- x_e')}`, with :math:`x_e = L x` (See\n    :class:`MahalanobisMixin`).\n\n    Parameters\n    ----------\n    pairs : array-like, shape=(n_pairs, 2, n_features) or (n_pairs, 2)\n      3D Array of pairs to score, with each row corresponding to two points,\n      for 2D array of indices of pairs if the metric learner uses a\n      preprocessor.\n\n    Returns\n    -------\n    scores: `numpy.ndarray` of shape=(n_pairs,)\n      The learned Mahalanobis distance for every pair.\n\n    See Also\n    --------\n    get_metric : a method that returns a function to compute the metric between\n      two points. The difference with `score_pairs` is that it works on two 1D\n      arrays and cannot use a preprocessor. Besides, the returned function is\n      independent of the metric learner and hence is not modified if the metric\n      learner is.\n\n    :ref:`mahalanobis_distances` : The section of the project documentation\n      that describes Mahalanobis Distances.\n    \"\"\"\n    pairs = check_input(pairs, type_of_inputs='tuples',\n                        preprocessor=self.preprocessor_,\n                        estimator=self, tuple_size=2)\n    pairwise_diffs = self.transform(pairs[:, 1, :] - pairs[:, 0, :])\n    # (for MahalanobisMixin, the embedding is linear so we can just embed the\n    # difference)\n    return np.sqrt(np.sum(pairwise_diffs**2, axis=-1))", "code_tokens": ["def", "score_pairs", "(", "self", ",", "pairs", ")", ":", "pairs", "=", "check_input", "(", "pairs", ",", "type_of_inputs", "=", "'tuples'", ",", "preprocessor", "=", "self", ".", "preprocessor_", ",", "estimator", "=", "self", ",", "tuple_size", "=", "2", ")", "pairwise_diffs", "=", "self", ".", "transform", "(", "pairs", "[", ":", ",", "1", ",", ":", "]", "-", "pairs", "[", ":", ",", "0", ",", ":", "]", ")", "# (for MahalanobisMixin, the embedding is linear so we can just embed the", "# difference)", "return", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "pairwise_diffs", "**", "2", ",", "axis", "=", "-", "1", ")", ")"], "docstring": "Returns the learned Mahalanobis distance between pairs.\n\n    This distance is defined as: :math:`d_M(x, x') = \\sqrt{(x-x')^T M (x-x')}`\n    where ``M`` is the learned Mahalanobis matrix, for every pair of points\n    ``x`` and ``x'``. This corresponds to the euclidean distance between\n    embeddings of the points in a new space, obtained through a linear\n    transformation. Indeed, we have also: :math:`d_M(x, x') = \\sqrt{(x_e -\n    x_e')^T (x_e- x_e')}`, with :math:`x_e = L x` (See\n    :class:`MahalanobisMixin`).\n\n    Parameters\n    ----------\n    pairs : array-like, shape=(n_pairs, 2, n_features) or (n_pairs, 2)\n      3D Array of pairs to score, with each row corresponding to two points,\n      for 2D array of indices of pairs if the metric learner uses a\n      preprocessor.\n\n    Returns\n    -------\n    scores: `numpy.ndarray` of shape=(n_pairs,)\n      The learned Mahalanobis distance for every pair.\n\n    See Also\n    --------\n    get_metric : a method that returns a function to compute the metric between\n      two points. The difference with `score_pairs` is that it works on two 1D\n      arrays and cannot use a preprocessor. Besides, the returned function is\n      independent of the metric learner and hence is not modified if the metric\n      learner is.\n\n    :ref:`mahalanobis_distances` : The section of the project documentation\n      that describes Mahalanobis Distances.", "docstring_tokens": ["Returns", "the", "learned", "Mahalanobis", "distance", "between", "pairs", "."], "sha": "d945df1342c69012608bb70b92520392a0853de6", "url": "https://github.com/metric-learn/metric-learn/blob/d945df1342c69012608bb70b92520392a0853de6/metric_learn/base_metric.py#L179-L219", "partition": "train", "idx": 152615}
{"repo": "googledatalab/pydatalab", "path": "datalab/bigquery/_api.py", "func_name": "Api.tabledata_insert_all", "original_string": "def tabledata_insert_all(self, table_name, rows):\n    \"\"\"Issues a request to insert data into a table.\n\n    Args:\n      table_name: the name of the table as a tuple of components.\n      rows: the data to populate the table, as a list of dictionaries.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._TABLES_PATH % table_name) + \"/insertAll\"\n\n    data = {\n        'kind': 'bigquery#tableDataInsertAllRequest',\n        'rows': rows\n    }\n\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)", "language": "python", "code": "def tabledata_insert_all(self, table_name, rows):\n    \"\"\"Issues a request to insert data into a table.\n\n    Args:\n      table_name: the name of the table as a tuple of components.\n      rows: the data to populate the table, as a list of dictionaries.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n    url = Api._ENDPOINT + (Api._TABLES_PATH % table_name) + \"/insertAll\"\n\n    data = {\n        'kind': 'bigquery#tableDataInsertAllRequest',\n        'rows': rows\n    }\n\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)", "code_tokens": ["def", "tabledata_insert_all", "(", "self", ",", "table_name", ",", "rows", ")", ":", "url", "=", "Api", ".", "_ENDPOINT", "+", "(", "Api", ".", "_TABLES_PATH", "%", "table_name", ")", "+", "\"/insertAll\"", "data", "=", "{", "'kind'", ":", "'bigquery#tableDataInsertAllRequest'", ",", "'rows'", ":", "rows", "}", "return", "datalab", ".", "utils", ".", "Http", ".", "request", "(", "url", ",", "data", "=", "data", ",", "credentials", "=", "self", ".", "_credentials", ")"], "docstring": "Issues a request to insert data into a table.\n\n    Args:\n      table_name: the name of the table as a tuple of components.\n      rows: the data to populate the table, as a list of dictionaries.\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.", "docstring_tokens": ["Issues", "a", "request", "to", "insert", "data", "into", "a", "table", "."], "sha": "d9031901d5bca22fe0d5925d204e6698df9852e1", "url": "https://github.com/googledatalab/pydatalab/blob/d9031901d5bca22fe0d5925d204e6698df9852e1/datalab/bigquery/_api.py#L422-L440", "partition": "train", "idx": 237840}
{"repo": "pennersr/django-allauth", "path": "allauth/socialaccount/providers/oauth/client.py", "func_name": "OAuth.query", "original_string": "def query(self, url, method=\"GET\", params=dict(), headers=dict()):\n        \"\"\"\n        Request a API endpoint at ``url`` with ``params`` being either the\n        POST or GET data.\n        \"\"\"\n        access_token = self._get_at_from_session()\n        oauth = OAuth1(\n            self.consumer_key,\n            client_secret=self.secret_key,\n            resource_owner_key=access_token['oauth_token'],\n            resource_owner_secret=access_token['oauth_token_secret'])\n        response = getattr(requests, method.lower())(url,\n                                                     auth=oauth,\n                                                     headers=headers,\n                                                     params=params)\n        if response.status_code != 200:\n            raise OAuthError(\n                _('No access to private resources at \"%s\".')\n                % get_token_prefix(self.request_token_url))\n\n        return response.text", "language": "python", "code": "def query(self, url, method=\"GET\", params=dict(), headers=dict()):\n        \"\"\"\n        Request a API endpoint at ``url`` with ``params`` being either the\n        POST or GET data.\n        \"\"\"\n        access_token = self._get_at_from_session()\n        oauth = OAuth1(\n            self.consumer_key,\n            client_secret=self.secret_key,\n            resource_owner_key=access_token['oauth_token'],\n            resource_owner_secret=access_token['oauth_token_secret'])\n        response = getattr(requests, method.lower())(url,\n                                                     auth=oauth,\n                                                     headers=headers,\n                                                     params=params)\n        if response.status_code != 200:\n            raise OAuthError(\n                _('No access to private resources at \"%s\".')\n                % get_token_prefix(self.request_token_url))\n\n        return response.text", "code_tokens": ["def", "query", "(", "self", ",", "url", ",", "method", "=", "\"GET\"", ",", "params", "=", "dict", "(", ")", ",", "headers", "=", "dict", "(", ")", ")", ":", "access_token", "=", "self", ".", "_get_at_from_session", "(", ")", "oauth", "=", "OAuth1", "(", "self", ".", "consumer_key", ",", "client_secret", "=", "self", ".", "secret_key", ",", "resource_owner_key", "=", "access_token", "[", "'oauth_token'", "]", ",", "resource_owner_secret", "=", "access_token", "[", "'oauth_token_secret'", "]", ")", "response", "=", "getattr", "(", "requests", ",", "method", ".", "lower", "(", ")", ")", "(", "url", ",", "auth", "=", "oauth", ",", "headers", "=", "headers", ",", "params", "=", "params", ")", "if", "response", ".", "status_code", "!=", "200", ":", "raise", "OAuthError", "(", "_", "(", "'No access to private resources at \"%s\".'", ")", "%", "get_token_prefix", "(", "self", ".", "request_token_url", ")", ")", "return", "response", ".", "text"], "docstring": "Request a API endpoint at ``url`` with ``params`` being either the\n        POST or GET data.", "docstring_tokens": ["Request", "a", "API", "endpoint", "at", "url", "with", "params", "being", "either", "the", "POST", "or", "GET", "data", "."], "sha": "f70cb3d622f992f15fe9b57098e0b328445b664e", "url": "https://github.com/pennersr/django-allauth/blob/f70cb3d622f992f15fe9b57098e0b328445b664e/allauth/socialaccount/providers/oauth/client.py#L180-L200", "partition": "train", "idx": 127201}
{"repo": "spotify/luigi", "path": "luigi/worker.py", "func_name": "Worker.add", "original_string": "def add(self, task, multiprocess=False, processes=0):\n        \"\"\"\n        Add a Task for the worker to check and possibly schedule and run.\n\n        Returns True if task and its dependencies were successfully scheduled or completed before.\n        \"\"\"\n        if self._first_task is None and hasattr(task, 'task_id'):\n            self._first_task = task.task_id\n        self.add_succeeded = True\n        if multiprocess:\n            queue = multiprocessing.Manager().Queue()\n            pool = multiprocessing.Pool(processes=processes if processes > 0 else None)\n        else:\n            queue = DequeQueue()\n            pool = SingleProcessPool()\n        self._validate_task(task)\n        pool.apply_async(check_complete, [task, queue])\n\n        # we track queue size ourselves because len(queue) won't work for multiprocessing\n        queue_size = 1\n        try:\n            seen = {task.task_id}\n            while queue_size:\n                current = queue.get()\n                queue_size -= 1\n                item, is_complete = current\n                for next in self._add(item, is_complete):\n                    if next.task_id not in seen:\n                        self._validate_task(next)\n                        seen.add(next.task_id)\n                        pool.apply_async(check_complete, [next, queue])\n                        queue_size += 1\n        except (KeyboardInterrupt, TaskException):\n            raise\n        except Exception as ex:\n            self.add_succeeded = False\n            formatted_traceback = traceback.format_exc()\n            self._log_unexpected_error(task)\n            task.trigger_event(Event.BROKEN_TASK, task, ex)\n            self._email_unexpected_error(task, formatted_traceback)\n            raise\n        finally:\n            pool.close()\n            pool.join()\n        return self.add_succeeded", "language": "python", "code": "def add(self, task, multiprocess=False, processes=0):\n        \"\"\"\n        Add a Task for the worker to check and possibly schedule and run.\n\n        Returns True if task and its dependencies were successfully scheduled or completed before.\n        \"\"\"\n        if self._first_task is None and hasattr(task, 'task_id'):\n            self._first_task = task.task_id\n        self.add_succeeded = True\n        if multiprocess:\n            queue = multiprocessing.Manager().Queue()\n            pool = multiprocessing.Pool(processes=processes if processes > 0 else None)\n        else:\n            queue = DequeQueue()\n            pool = SingleProcessPool()\n        self._validate_task(task)\n        pool.apply_async(check_complete, [task, queue])\n\n        # we track queue size ourselves because len(queue) won't work for multiprocessing\n        queue_size = 1\n        try:\n            seen = {task.task_id}\n            while queue_size:\n                current = queue.get()\n                queue_size -= 1\n                item, is_complete = current\n                for next in self._add(item, is_complete):\n                    if next.task_id not in seen:\n                        self._validate_task(next)\n                        seen.add(next.task_id)\n                        pool.apply_async(check_complete, [next, queue])\n                        queue_size += 1\n        except (KeyboardInterrupt, TaskException):\n            raise\n        except Exception as ex:\n            self.add_succeeded = False\n            formatted_traceback = traceback.format_exc()\n            self._log_unexpected_error(task)\n            task.trigger_event(Event.BROKEN_TASK, task, ex)\n            self._email_unexpected_error(task, formatted_traceback)\n            raise\n        finally:\n            pool.close()\n            pool.join()\n        return self.add_succeeded", "code_tokens": ["def", "add", "(", "self", ",", "task", ",", "multiprocess", "=", "False", ",", "processes", "=", "0", ")", ":", "if", "self", ".", "_first_task", "is", "None", "and", "hasattr", "(", "task", ",", "'task_id'", ")", ":", "self", ".", "_first_task", "=", "task", ".", "task_id", "self", ".", "add_succeeded", "=", "True", "if", "multiprocess", ":", "queue", "=", "multiprocessing", ".", "Manager", "(", ")", ".", "Queue", "(", ")", "pool", "=", "multiprocessing", ".", "Pool", "(", "processes", "=", "processes", "if", "processes", ">", "0", "else", "None", ")", "else", ":", "queue", "=", "DequeQueue", "(", ")", "pool", "=", "SingleProcessPool", "(", ")", "self", ".", "_validate_task", "(", "task", ")", "pool", ".", "apply_async", "(", "check_complete", ",", "[", "task", ",", "queue", "]", ")", "# we track queue size ourselves because len(queue) won't work for multiprocessing", "queue_size", "=", "1", "try", ":", "seen", "=", "{", "task", ".", "task_id", "}", "while", "queue_size", ":", "current", "=", "queue", ".", "get", "(", ")", "queue_size", "-=", "1", "item", ",", "is_complete", "=", "current", "for", "next", "in", "self", ".", "_add", "(", "item", ",", "is_complete", ")", ":", "if", "next", ".", "task_id", "not", "in", "seen", ":", "self", ".", "_validate_task", "(", "next", ")", "seen", ".", "add", "(", "next", ".", "task_id", ")", "pool", ".", "apply_async", "(", "check_complete", ",", "[", "next", ",", "queue", "]", ")", "queue_size", "+=", "1", "except", "(", "KeyboardInterrupt", ",", "TaskException", ")", ":", "raise", "except", "Exception", "as", "ex", ":", "self", ".", "add_succeeded", "=", "False", "formatted_traceback", "=", "traceback", ".", "format_exc", "(", ")", "self", ".", "_log_unexpected_error", "(", "task", ")", "task", ".", "trigger_event", "(", "Event", ".", "BROKEN_TASK", ",", "task", ",", "ex", ")", "self", ".", "_email_unexpected_error", "(", "task", ",", "formatted_traceback", ")", "raise", "finally", ":", "pool", ".", "close", "(", ")", "pool", ".", "join", "(", ")", "return", "self", ".", "add_succeeded"], "docstring": "Add a Task for the worker to check and possibly schedule and run.\n\n        Returns True if task and its dependencies were successfully scheduled or completed before.", "docstring_tokens": ["Add", "a", "Task", "for", "the", "worker", "to", "check", "and", "possibly", "schedule", "and", "run", "."], "sha": "c5eca1c3c3ee2a7eb612486192a0da146710a1e9", "url": "https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/worker.py#L725-L769", "partition": "train", "idx": 171578}
{"repo": "sdispater/eloquent", "path": "eloquent/orm/relations/has_many_through.py", "func_name": "HasManyThrough._set_join", "original_string": "def _set_join(self, query=None):\n        \"\"\"\n        Set the join clause for the query.\n        \"\"\"\n        if not query:\n            query = self._query\n\n        foreign_key = '%s.%s' % (self._related.get_table(), self._second_key)\n\n        query.join(self._parent.get_table(), self.get_qualified_parent_key_name(), '=', foreign_key)", "language": "python", "code": "def _set_join(self, query=None):\n        \"\"\"\n        Set the join clause for the query.\n        \"\"\"\n        if not query:\n            query = self._query\n\n        foreign_key = '%s.%s' % (self._related.get_table(), self._second_key)\n\n        query.join(self._parent.get_table(), self.get_qualified_parent_key_name(), '=', foreign_key)", "code_tokens": ["def", "_set_join", "(", "self", ",", "query", "=", "None", ")", ":", "if", "not", "query", ":", "query", "=", "self", ".", "_query", "foreign_key", "=", "'%s.%s'", "%", "(", "self", ".", "_related", ".", "get_table", "(", ")", ",", "self", ".", "_second_key", ")", "query", ".", "join", "(", "self", ".", "_parent", ".", "get_table", "(", ")", ",", "self", ".", "get_qualified_parent_key_name", "(", ")", ",", "'='", ",", "foreign_key", ")"], "docstring": "Set the join clause for the query.", "docstring_tokens": ["Set", "the", "join", "clause", "for", "the", "query", "."], "sha": "0638b688d5fd0c1a46b7471dd465eeb4c2f84666", "url": "https://github.com/sdispater/eloquent/blob/0638b688d5fd0c1a46b7471dd465eeb4c2f84666/eloquent/orm/relations/has_many_through.py#L61-L70", "partition": "train", "idx": 178888}
{"repo": "chaoss/grimoirelab-elk", "path": "utils/index_mapping.py", "func_name": "export_items", "original_string": "def export_items(elastic_url, in_index, out_index, elastic_url_out=None,\n                 search_after=False, search_after_value=None, limit=None,\n                 copy=False):\n    \"\"\" Export items from in_index to out_index using the correct mapping \"\"\"\n\n    if not limit:\n        limit = DEFAULT_LIMIT\n\n    if search_after_value:\n        search_after_value_timestamp = int(search_after_value[0])\n        search_after_value_uuid = search_after_value[1]\n        search_after_value = [search_after_value_timestamp, search_after_value_uuid]\n\n    logging.info(\"Exporting items from %s/%s to %s\", elastic_url, in_index, out_index)\n\n    count_res = requests.get('%s/%s/_count' % (elastic_url, in_index))\n    try:\n        count_res.raise_for_status()\n    except requests.exceptions.HTTPError:\n        if count_res.status_code == 404:\n            logging.error(\"The index does not exists: %s\", in_index)\n        else:\n            logging.error(count_res.text)\n        sys.exit(1)\n\n    logging.info(\"Total items to copy: %i\", count_res.json()['count'])\n\n    # Time to upload the items with the correct mapping\n    elastic_in = ElasticSearch(elastic_url, in_index)\n    if not copy:\n        # Create the correct mapping for the data sources detected from in_index\n        ds_mapping = find_mapping(elastic_url, in_index)\n    else:\n        logging.debug('Using the input index mapping')\n        ds_mapping = extract_mapping(elastic_url, in_index)\n\n    if not elastic_url_out:\n        elastic_out = ElasticSearch(elastic_url, out_index, mappings=ds_mapping)\n    else:\n        elastic_out = ElasticSearch(elastic_url_out, out_index, mappings=ds_mapping)\n\n    # Time to just copy from in_index to our_index\n    uid_field = find_uuid(elastic_url, in_index)\n    backend = find_perceval_backend(elastic_url, in_index)\n    if search_after:\n        total = elastic_out.bulk_upload(fetch(elastic_in, backend, limit,\n                                              search_after_value, scroll=False), uid_field)\n    else:\n        total = elastic_out.bulk_upload(fetch(elastic_in, backend, limit), uid_field)\n\n    logging.info(\"Total items copied: %i\", total)", "language": "python", "code": "def export_items(elastic_url, in_index, out_index, elastic_url_out=None,\n                 search_after=False, search_after_value=None, limit=None,\n                 copy=False):\n    \"\"\" Export items from in_index to out_index using the correct mapping \"\"\"\n\n    if not limit:\n        limit = DEFAULT_LIMIT\n\n    if search_after_value:\n        search_after_value_timestamp = int(search_after_value[0])\n        search_after_value_uuid = search_after_value[1]\n        search_after_value = [search_after_value_timestamp, search_after_value_uuid]\n\n    logging.info(\"Exporting items from %s/%s to %s\", elastic_url, in_index, out_index)\n\n    count_res = requests.get('%s/%s/_count' % (elastic_url, in_index))\n    try:\n        count_res.raise_for_status()\n    except requests.exceptions.HTTPError:\n        if count_res.status_code == 404:\n            logging.error(\"The index does not exists: %s\", in_index)\n        else:\n            logging.error(count_res.text)\n        sys.exit(1)\n\n    logging.info(\"Total items to copy: %i\", count_res.json()['count'])\n\n    # Time to upload the items with the correct mapping\n    elastic_in = ElasticSearch(elastic_url, in_index)\n    if not copy:\n        # Create the correct mapping for the data sources detected from in_index\n        ds_mapping = find_mapping(elastic_url, in_index)\n    else:\n        logging.debug('Using the input index mapping')\n        ds_mapping = extract_mapping(elastic_url, in_index)\n\n    if not elastic_url_out:\n        elastic_out = ElasticSearch(elastic_url, out_index, mappings=ds_mapping)\n    else:\n        elastic_out = ElasticSearch(elastic_url_out, out_index, mappings=ds_mapping)\n\n    # Time to just copy from in_index to our_index\n    uid_field = find_uuid(elastic_url, in_index)\n    backend = find_perceval_backend(elastic_url, in_index)\n    if search_after:\n        total = elastic_out.bulk_upload(fetch(elastic_in, backend, limit,\n                                              search_after_value, scroll=False), uid_field)\n    else:\n        total = elastic_out.bulk_upload(fetch(elastic_in, backend, limit), uid_field)\n\n    logging.info(\"Total items copied: %i\", total)", "code_tokens": ["def", "export_items", "(", "elastic_url", ",", "in_index", ",", "out_index", ",", "elastic_url_out", "=", "None", ",", "search_after", "=", "False", ",", "search_after_value", "=", "None", ",", "limit", "=", "None", ",", "copy", "=", "False", ")", ":", "if", "not", "limit", ":", "limit", "=", "DEFAULT_LIMIT", "if", "search_after_value", ":", "search_after_value_timestamp", "=", "int", "(", "search_after_value", "[", "0", "]", ")", "search_after_value_uuid", "=", "search_after_value", "[", "1", "]", "search_after_value", "=", "[", "search_after_value_timestamp", ",", "search_after_value_uuid", "]", "logging", ".", "info", "(", "\"Exporting items from %s/%s to %s\"", ",", "elastic_url", ",", "in_index", ",", "out_index", ")", "count_res", "=", "requests", ".", "get", "(", "'%s/%s/_count'", "%", "(", "elastic_url", ",", "in_index", ")", ")", "try", ":", "count_res", ".", "raise_for_status", "(", ")", "except", "requests", ".", "exceptions", ".", "HTTPError", ":", "if", "count_res", ".", "status_code", "==", "404", ":", "logging", ".", "error", "(", "\"The index does not exists: %s\"", ",", "in_index", ")", "else", ":", "logging", ".", "error", "(", "count_res", ".", "text", ")", "sys", ".", "exit", "(", "1", ")", "logging", ".", "info", "(", "\"Total items to copy: %i\"", ",", "count_res", ".", "json", "(", ")", "[", "'count'", "]", ")", "# Time to upload the items with the correct mapping", "elastic_in", "=", "ElasticSearch", "(", "elastic_url", ",", "in_index", ")", "if", "not", "copy", ":", "# Create the correct mapping for the data sources detected from in_index", "ds_mapping", "=", "find_mapping", "(", "elastic_url", ",", "in_index", ")", "else", ":", "logging", ".", "debug", "(", "'Using the input index mapping'", ")", "ds_mapping", "=", "extract_mapping", "(", "elastic_url", ",", "in_index", ")", "if", "not", "elastic_url_out", ":", "elastic_out", "=", "ElasticSearch", "(", "elastic_url", ",", "out_index", ",", "mappings", "=", "ds_mapping", ")", "else", ":", "elastic_out", "=", "ElasticSearch", "(", "elastic_url_out", ",", "out_index", ",", "mappings", "=", "ds_mapping", ")", "# Time to just copy from in_index to our_index", "uid_field", "=", "find_uuid", "(", "elastic_url", ",", "in_index", ")", "backend", "=", "find_perceval_backend", "(", "elastic_url", ",", "in_index", ")", "if", "search_after", ":", "total", "=", "elastic_out", ".", "bulk_upload", "(", "fetch", "(", "elastic_in", ",", "backend", ",", "limit", ",", "search_after_value", ",", "scroll", "=", "False", ")", ",", "uid_field", ")", "else", ":", "total", "=", "elastic_out", ".", "bulk_upload", "(", "fetch", "(", "elastic_in", ",", "backend", ",", "limit", ")", ",", "uid_field", ")", "logging", ".", "info", "(", "\"Total items copied: %i\"", ",", "total", ")"], "docstring": "Export items from in_index to out_index using the correct mapping", "docstring_tokens": ["Export", "items", "from", "in_index", "to", "out_index", "using", "the", "correct", "mapping"], "sha": "64e08b324b36d9f6909bf705145d6451c8d34e65", "url": "https://github.com/chaoss/grimoirelab-elk/blob/64e08b324b36d9f6909bf705145d6451c8d34e65/utils/index_mapping.py#L297-L347", "partition": "train", "idx": 18278}
{"repo": "baruwa-enterprise/BaruwaAPI", "path": "BaruwaAPI/resource.py", "func_name": "BaruwaAPIClient.update_fallbackserver", "original_string": "def update_fallbackserver(self, serverid, data):\n        \"\"\"Update Fallback server\"\"\"\n        return self.api_call(\n            ENDPOINTS['fallbackservers']['update'],\n            dict(serverid=serverid),\n            body=data)", "language": "python", "code": "def update_fallbackserver(self, serverid, data):\n        \"\"\"Update Fallback server\"\"\"\n        return self.api_call(\n            ENDPOINTS['fallbackservers']['update'],\n            dict(serverid=serverid),\n            body=data)", "code_tokens": ["def", "update_fallbackserver", "(", "self", ",", "serverid", ",", "data", ")", ":", "return", "self", ".", "api_call", "(", "ENDPOINTS", "[", "'fallbackservers'", "]", "[", "'update'", "]", ",", "dict", "(", "serverid", "=", "serverid", ")", ",", "body", "=", "data", ")"], "docstring": "Update Fallback server", "docstring_tokens": ["Update", "Fallback", "server"], "sha": "53335b377ccfd388e42f4f240f181eed72f51180", "url": "https://github.com/baruwa-enterprise/BaruwaAPI/blob/53335b377ccfd388e42f4f240f181eed72f51180/BaruwaAPI/resource.py#L457-L462", "partition": "train", "idx": 85478}
{"repo": "MKLab-ITI/reveal-graph-embedding", "path": "reveal_graph_embedding/datautil/snow_datautil/snow_read_data.py", "func_name": "write_screen_name_to_topics", "original_string": "def write_screen_name_to_topics(filepath, user_label_matrix, node_to_id, id_to_name, label_to_lemma, lemma_to_keyword, separator=\",\"):\n    \"\"\"\n    Writes a user name and associated topic names per row.\n    \"\"\"\n    user_label_matrix = spsp.coo_matrix(user_label_matrix)\n\n    shape = user_label_matrix.shape\n    nnz = user_label_matrix.getnnz()\n\n    row = user_label_matrix.row\n    col = user_label_matrix.col\n    data = user_label_matrix.data\n\n    name_to_topic_set = defaultdict(set)\n\n    for edge in range(row.size):\n        node = row[edge]\n        user_twitter_id = node_to_id[node]\n        name = id_to_name[user_twitter_id]\n\n        label = col[edge]\n        lemma = label_to_lemma[label]\n        # topic = lemma_to_keyword[lemma]\n\n        name_to_topic_set[name].add(lemma)\n\n    with open(filepath, \"w\") as f:\n        # Write metadata.\n        file_row = \"n_rows:\" + separator + str(shape[0]) + separator +\\\n                   \"nnz:\" + separator + str(nnz) + separator +\\\n                   \"\\n\"\n        f.write(file_row)\n\n        for name, topic_set in name_to_topic_set.items():\n            file_row = list()\n            file_row.append(name)\n            file_row.extend(topic_set)\n            file_row = separator.join(file_row) + \"\\n\"\n            f.write(file_row)", "language": "python", "code": "def write_screen_name_to_topics(filepath, user_label_matrix, node_to_id, id_to_name, label_to_lemma, lemma_to_keyword, separator=\",\"):\n    \"\"\"\n    Writes a user name and associated topic names per row.\n    \"\"\"\n    user_label_matrix = spsp.coo_matrix(user_label_matrix)\n\n    shape = user_label_matrix.shape\n    nnz = user_label_matrix.getnnz()\n\n    row = user_label_matrix.row\n    col = user_label_matrix.col\n    data = user_label_matrix.data\n\n    name_to_topic_set = defaultdict(set)\n\n    for edge in range(row.size):\n        node = row[edge]\n        user_twitter_id = node_to_id[node]\n        name = id_to_name[user_twitter_id]\n\n        label = col[edge]\n        lemma = label_to_lemma[label]\n        # topic = lemma_to_keyword[lemma]\n\n        name_to_topic_set[name].add(lemma)\n\n    with open(filepath, \"w\") as f:\n        # Write metadata.\n        file_row = \"n_rows:\" + separator + str(shape[0]) + separator +\\\n                   \"nnz:\" + separator + str(nnz) + separator +\\\n                   \"\\n\"\n        f.write(file_row)\n\n        for name, topic_set in name_to_topic_set.items():\n            file_row = list()\n            file_row.append(name)\n            file_row.extend(topic_set)\n            file_row = separator.join(file_row) + \"\\n\"\n            f.write(file_row)", "code_tokens": ["def", "write_screen_name_to_topics", "(", "filepath", ",", "user_label_matrix", ",", "node_to_id", ",", "id_to_name", ",", "label_to_lemma", ",", "lemma_to_keyword", ",", "separator", "=", "\",\"", ")", ":", "user_label_matrix", "=", "spsp", ".", "coo_matrix", "(", "user_label_matrix", ")", "shape", "=", "user_label_matrix", ".", "shape", "nnz", "=", "user_label_matrix", ".", "getnnz", "(", ")", "row", "=", "user_label_matrix", ".", "row", "col", "=", "user_label_matrix", ".", "col", "data", "=", "user_label_matrix", ".", "data", "name_to_topic_set", "=", "defaultdict", "(", "set", ")", "for", "edge", "in", "range", "(", "row", ".", "size", ")", ":", "node", "=", "row", "[", "edge", "]", "user_twitter_id", "=", "node_to_id", "[", "node", "]", "name", "=", "id_to_name", "[", "user_twitter_id", "]", "label", "=", "col", "[", "edge", "]", "lemma", "=", "label_to_lemma", "[", "label", "]", "# topic = lemma_to_keyword[lemma]", "name_to_topic_set", "[", "name", "]", ".", "add", "(", "lemma", ")", "with", "open", "(", "filepath", ",", "\"w\"", ")", "as", "f", ":", "# Write metadata.", "file_row", "=", "\"n_rows:\"", "+", "separator", "+", "str", "(", "shape", "[", "0", "]", ")", "+", "separator", "+", "\"nnz:\"", "+", "separator", "+", "str", "(", "nnz", ")", "+", "separator", "+", "\"\\n\"", "f", ".", "write", "(", "file_row", ")", "for", "name", ",", "topic_set", "in", "name_to_topic_set", ".", "items", "(", ")", ":", "file_row", "=", "list", "(", ")", "file_row", ".", "append", "(", "name", ")", "file_row", ".", "extend", "(", "topic_set", ")", "file_row", "=", "separator", ".", "join", "(", "file_row", ")", "+", "\"\\n\"", "f", ".", "write", "(", "file_row", ")"], "docstring": "Writes a user name and associated topic names per row.", "docstring_tokens": ["Writes", "a", "user", "name", "and", "associated", "topic", "names", "per", "row", "."], "sha": "eda862687aa5a64b79c6b12de1b4dca6ce986dc8", "url": "https://github.com/MKLab-ITI/reveal-graph-embedding/blob/eda862687aa5a64b79c6b12de1b4dca6ce986dc8/reveal_graph_embedding/datautil/snow_datautil/snow_read_data.py#L178-L216", "partition": "train", "idx": 47451}
{"repo": "Jarn/jarn.mkrelease", "path": "jarn/mkrelease/chdir.py", "func_name": "ChdirStack.push", "original_string": "def push(self, dir):\n        \"\"\"Push cwd on stack and change to 'dir'.\n        \"\"\"\n        self.stack.append(os.getcwd())\n        os.chdir(dir or os.getcwd())", "language": "python", "code": "def push(self, dir):\n        \"\"\"Push cwd on stack and change to 'dir'.\n        \"\"\"\n        self.stack.append(os.getcwd())\n        os.chdir(dir or os.getcwd())", "code_tokens": ["def", "push", "(", "self", ",", "dir", ")", ":", "self", ".", "stack", ".", "append", "(", "os", ".", "getcwd", "(", ")", ")", "os", ".", "chdir", "(", "dir", "or", "os", ".", "getcwd", "(", ")", ")"], "docstring": "Push cwd on stack and change to 'dir'.", "docstring_tokens": ["Push", "cwd", "on", "stack", "and", "change", "to", "dir", "."], "sha": "844377f37a3cdc0a154148790a926f991019ec4a", "url": "https://github.com/Jarn/jarn.mkrelease/blob/844377f37a3cdc0a154148790a926f991019ec4a/jarn/mkrelease/chdir.py#L14-L18", "partition": "train", "idx": 58749}
{"repo": "benoitkugler/abstractDataLibrary", "path": "pyDLib/Core/formats.py", "func_name": "abstractRender.date", "original_string": "def date(objet):\n        \"\"\" abstractRender d'une date datetime.date\"\"\"\n        if objet:\n            return \"{}/{}/{}\".format(objet.day, objet.month, objet.year)\n        return \"\"", "language": "python", "code": "def date(objet):\n        \"\"\" abstractRender d'une date datetime.date\"\"\"\n        if objet:\n            return \"{}/{}/{}\".format(objet.day, objet.month, objet.year)\n        return \"\"", "code_tokens": ["def", "date", "(", "objet", ")", ":", "if", "objet", ":", "return", "\"{}/{}/{}\"", ".", "format", "(", "objet", ".", "day", ",", "objet", ".", "month", ",", "objet", ".", "year", ")", "return", "\"\""], "docstring": "abstractRender d'une date datetime.date", "docstring_tokens": ["abstractRender", "d", "une", "date", "datetime", ".", "date"], "sha": "16be28e99837e40287a63803bbfdf67ac1806b7b", "url": "https://github.com/benoitkugler/abstractDataLibrary/blob/16be28e99837e40287a63803bbfdf67ac1806b7b/pyDLib/Core/formats.py#L250-L254", "partition": "train", "idx": 181793}
{"repo": "arubertoson/maya-launcher", "path": "mayalauncher.py", "func_name": "Config._create_default_config_file", "original_string": "def _create_default_config_file(self):\r\n        \"\"\"\r\n        If config file does not exists create and set default values.\r\n        \"\"\"\r\n        logger.info('Initialize Maya launcher, creating config file...\\n')\r\n        self.add_section(self.DEFAULTS)\r\n        self.add_section(self.PATTERNS)\r\n        self.add_section(self.ENVIRONMENTS)\r\n        self.add_section(self.EXECUTABLES)\r\n        self.set(self.DEFAULTS, 'executable', None)\r\n        self.set(self.DEFAULTS, 'environment', None)\r\n        self.set(self.PATTERNS, 'exclude', ', '.join(self.EXLUDE_PATTERNS))\r\n        self.set(self.PATTERNS, 'icon_ext', ', '.join(self.ICON_EXTENSIONS))\r\n\r\n        self.config_file.parent.mkdir(exist_ok=True)\r\n        self.config_file.touch()\r\n        with self.config_file.open('wb') as f:\r\n            self.write(f)\r\n\r\n        # If this function is run inform the user that a new file has been\r\n        # created.\r\n        sys.exit('Maya launcher has successfully created config file at:\\n'\r\n                 ' \"{}\"'.format(str(self.config_file)))", "language": "python", "code": "def _create_default_config_file(self):\r\n        \"\"\"\r\n        If config file does not exists create and set default values.\r\n        \"\"\"\r\n        logger.info('Initialize Maya launcher, creating config file...\\n')\r\n        self.add_section(self.DEFAULTS)\r\n        self.add_section(self.PATTERNS)\r\n        self.add_section(self.ENVIRONMENTS)\r\n        self.add_section(self.EXECUTABLES)\r\n        self.set(self.DEFAULTS, 'executable', None)\r\n        self.set(self.DEFAULTS, 'environment', None)\r\n        self.set(self.PATTERNS, 'exclude', ', '.join(self.EXLUDE_PATTERNS))\r\n        self.set(self.PATTERNS, 'icon_ext', ', '.join(self.ICON_EXTENSIONS))\r\n\r\n        self.config_file.parent.mkdir(exist_ok=True)\r\n        self.config_file.touch()\r\n        with self.config_file.open('wb') as f:\r\n            self.write(f)\r\n\r\n        # If this function is run inform the user that a new file has been\r\n        # created.\r\n        sys.exit('Maya launcher has successfully created config file at:\\n'\r\n                 ' \"{}\"'.format(str(self.config_file)))", "code_tokens": ["def", "_create_default_config_file", "(", "self", ")", ":", "logger", ".", "info", "(", "'Initialize Maya launcher, creating config file...\\n'", ")", "self", ".", "add_section", "(", "self", ".", "DEFAULTS", ")", "self", ".", "add_section", "(", "self", ".", "PATTERNS", ")", "self", ".", "add_section", "(", "self", ".", "ENVIRONMENTS", ")", "self", ".", "add_section", "(", "self", ".", "EXECUTABLES", ")", "self", ".", "set", "(", "self", ".", "DEFAULTS", ",", "'executable'", ",", "None", ")", "self", ".", "set", "(", "self", ".", "DEFAULTS", ",", "'environment'", ",", "None", ")", "self", ".", "set", "(", "self", ".", "PATTERNS", ",", "'exclude'", ",", "', '", ".", "join", "(", "self", ".", "EXLUDE_PATTERNS", ")", ")", "self", ".", "set", "(", "self", ".", "PATTERNS", ",", "'icon_ext'", ",", "', '", ".", "join", "(", "self", ".", "ICON_EXTENSIONS", ")", ")", "self", ".", "config_file", ".", "parent", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "self", ".", "config_file", ".", "touch", "(", ")", "with", "self", ".", "config_file", ".", "open", "(", "'wb'", ")", "as", "f", ":", "self", ".", "write", "(", "f", ")", "# If this function is run inform the user that a new file has been\r", "# created.\r", "sys", ".", "exit", "(", "'Maya launcher has successfully created config file at:\\n'", "' \"{}\"'", ".", "format", "(", "str", "(", "self", ".", "config_file", ")", ")", ")"], "docstring": "If config file does not exists create and set default values.", "docstring_tokens": ["If", "config", "file", "does", "not", "exists", "create", "and", "set", "default", "values", "."], "sha": "9bd82cce7edf4afb803dd8044107a324e93f197f", "url": "https://github.com/arubertoson/maya-launcher/blob/9bd82cce7edf4afb803dd8044107a324e93f197f/mayalauncher.py#L113-L135", "partition": "train", "idx": 75604}
{"repo": "projectshift/shift-boiler", "path": "boiler/feature/jinja_extensions.py", "func_name": "jinja_extensions_feature", "original_string": "def jinja_extensions_feature(app):\n    \"\"\" Enables custom templating extensions \"\"\"\n\n    # register jinja filters\n    app.jinja_env.globals['momentjs'] = MomentJsFilters\n    app.jinja_env.filters.update(MomentJsFilters().get_filters())\n    app.jinja_env.filters.update(DateFilters().get_filters())\n    app.jinja_env.filters.update(HumanizeFilters().get_filters())\n\n    # register custom jinja functions\n    app.jinja_env.globals.update(dict(\n        asset=functions.asset,\n        dev_proxy=functions.dev_proxy\n    ))", "language": "python", "code": "def jinja_extensions_feature(app):\n    \"\"\" Enables custom templating extensions \"\"\"\n\n    # register jinja filters\n    app.jinja_env.globals['momentjs'] = MomentJsFilters\n    app.jinja_env.filters.update(MomentJsFilters().get_filters())\n    app.jinja_env.filters.update(DateFilters().get_filters())\n    app.jinja_env.filters.update(HumanizeFilters().get_filters())\n\n    # register custom jinja functions\n    app.jinja_env.globals.update(dict(\n        asset=functions.asset,\n        dev_proxy=functions.dev_proxy\n    ))", "code_tokens": ["def", "jinja_extensions_feature", "(", "app", ")", ":", "# register jinja filters", "app", ".", "jinja_env", ".", "globals", "[", "'momentjs'", "]", "=", "MomentJsFilters", "app", ".", "jinja_env", ".", "filters", ".", "update", "(", "MomentJsFilters", "(", ")", ".", "get_filters", "(", ")", ")", "app", ".", "jinja_env", ".", "filters", ".", "update", "(", "DateFilters", "(", ")", ".", "get_filters", "(", ")", ")", "app", ".", "jinja_env", ".", "filters", ".", "update", "(", "HumanizeFilters", "(", ")", ".", "get_filters", "(", ")", ")", "# register custom jinja functions", "app", ".", "jinja_env", ".", "globals", ".", "update", "(", "dict", "(", "asset", "=", "functions", ".", "asset", ",", "dev_proxy", "=", "functions", ".", "dev_proxy", ")", ")"], "docstring": "Enables custom templating extensions", "docstring_tokens": ["Enables", "custom", "templating", "extensions"], "sha": "8e6f3a3e4b9493fb6c8bd16bed160ede153bfb0b", "url": "https://github.com/projectshift/shift-boiler/blob/8e6f3a3e4b9493fb6c8bd16bed160ede153bfb0b/boiler/feature/jinja_extensions.py#L7-L20", "partition": "train", "idx": 13284}
{"repo": "hendrix/hendrix", "path": "hendrix/contrib/concurrency/messaging.py", "func_name": "RecipientManager.remove", "original_string": "def remove(self, transport):\n        \"\"\"\n            removes a transport if a member of this group\n        \"\"\"\n        if transport.uid in self.transports:\n            del (self.transports[transport.uid])", "language": "python", "code": "def remove(self, transport):\n        \"\"\"\n            removes a transport if a member of this group\n        \"\"\"\n        if transport.uid in self.transports:\n            del (self.transports[transport.uid])", "code_tokens": ["def", "remove", "(", "self", ",", "transport", ")", ":", "if", "transport", ".", "uid", "in", "self", ".", "transports", ":", "del", "(", "self", ".", "transports", "[", "transport", ".", "uid", "]", ")"], "docstring": "removes a transport if a member of this group", "docstring_tokens": ["removes", "a", "transport", "if", "a", "member", "of", "this", "group"], "sha": "175af011a7e5822b772bfec0e11a46466bb8688d", "url": "https://github.com/hendrix/hendrix/blob/175af011a7e5822b772bfec0e11a46466bb8688d/hendrix/contrib/concurrency/messaging.py#L41-L46", "partition": "train", "idx": 251351}
{"repo": "Enteee/pdml2flow", "path": "pdml2flow/conf.py", "func_name": "Conf.set", "original_string": "def set(conf):\n        \"\"\"Applies a configuration to the global config object\"\"\"\n        for name, value in conf.items():\n            if value is not None:\n                setattr(Conf, name.upper(), value)", "language": "python", "code": "def set(conf):\n        \"\"\"Applies a configuration to the global config object\"\"\"\n        for name, value in conf.items():\n            if value is not None:\n                setattr(Conf, name.upper(), value)", "code_tokens": ["def", "set", "(", "conf", ")", ":", "for", "name", ",", "value", "in", "conf", ".", "items", "(", ")", ":", "if", "value", "is", "not", "None", ":", "setattr", "(", "Conf", ",", "name", ".", "upper", "(", ")", ",", "value", ")"], "docstring": "Applies a configuration to the global config object", "docstring_tokens": ["Applies", "a", "configuration", "to", "the", "global", "config", "object"], "sha": "bc9efe379b0b2406bfbbbd8e0f678b1f63805c66", "url": "https://github.com/Enteee/pdml2flow/blob/bc9efe379b0b2406bfbbbd8e0f678b1f63805c66/pdml2flow/conf.py#L70-L74", "partition": "train", "idx": 38591}
{"repo": "wonambi-python/wonambi", "path": "wonambi/attr/annotations.py", "func_name": "Annotations.get_stage_for_epoch", "original_string": "def get_stage_for_epoch(self, epoch_start, window_length=None,\n                            attr='stage'):\n        \"\"\"Return stage for one specific epoch.\n\n        Parameters\n        ----------\n        id_epoch : str\n            index of the epoch\n        attr : str, optional\n            'stage' or 'quality'\n\n        Returns\n        -------\n        stage : str\n            description of the stage.\n        \"\"\"\n        for epoch in self.epochs:\n            if epoch['start'] == epoch_start:\n                return epoch[attr]\n\n            if window_length is not None:\n                epoch_length = epoch['end'] - epoch['start']\n                if logical_and(window_length < epoch_length,\n                               0 <= \\\n                               (epoch_start - epoch['start']) < epoch_length):\n                    return epoch[attr]", "language": "python", "code": "def get_stage_for_epoch(self, epoch_start, window_length=None,\n                            attr='stage'):\n        \"\"\"Return stage for one specific epoch.\n\n        Parameters\n        ----------\n        id_epoch : str\n            index of the epoch\n        attr : str, optional\n            'stage' or 'quality'\n\n        Returns\n        -------\n        stage : str\n            description of the stage.\n        \"\"\"\n        for epoch in self.epochs:\n            if epoch['start'] == epoch_start:\n                return epoch[attr]\n\n            if window_length is not None:\n                epoch_length = epoch['end'] - epoch['start']\n                if logical_and(window_length < epoch_length,\n                               0 <= \\\n                               (epoch_start - epoch['start']) < epoch_length):\n                    return epoch[attr]", "code_tokens": ["def", "get_stage_for_epoch", "(", "self", ",", "epoch_start", ",", "window_length", "=", "None", ",", "attr", "=", "'stage'", ")", ":", "for", "epoch", "in", "self", ".", "epochs", ":", "if", "epoch", "[", "'start'", "]", "==", "epoch_start", ":", "return", "epoch", "[", "attr", "]", "if", "window_length", "is", "not", "None", ":", "epoch_length", "=", "epoch", "[", "'end'", "]", "-", "epoch", "[", "'start'", "]", "if", "logical_and", "(", "window_length", "<", "epoch_length", ",", "0", "<=", "(", "epoch_start", "-", "epoch", "[", "'start'", "]", ")", "<", "epoch_length", ")", ":", "return", "epoch", "[", "attr", "]"], "docstring": "Return stage for one specific epoch.\n\n        Parameters\n        ----------\n        id_epoch : str\n            index of the epoch\n        attr : str, optional\n            'stage' or 'quality'\n\n        Returns\n        -------\n        stage : str\n            description of the stage.", "docstring_tokens": ["Return", "stage", "for", "one", "specific", "epoch", "."], "sha": "1d8e3d7e53df8017c199f703bcab582914676e76", "url": "https://github.com/wonambi-python/wonambi/blob/1d8e3d7e53df8017c199f703bcab582914676e76/wonambi/attr/annotations.py#L1166-L1191", "partition": "train", "idx": 23566}
{"repo": "squaresLab/BugZoo", "path": "bugzoo/mgr/bug.py", "func_name": "BugManager.build", "original_string": "def build(self,\n              bug: Bug,\n              force: bool = True,\n              quiet: bool = False\n              ) -> None:\n        \"\"\"\n        Builds the Docker image associated with a given bug.\n\n        See: `BuildManager.build`\n        \"\"\"\n        self.__installation.build.build(bug.image,\n                                        force=force,\n                                        quiet=quiet)", "language": "python", "code": "def build(self,\n              bug: Bug,\n              force: bool = True,\n              quiet: bool = False\n              ) -> None:\n        \"\"\"\n        Builds the Docker image associated with a given bug.\n\n        See: `BuildManager.build`\n        \"\"\"\n        self.__installation.build.build(bug.image,\n                                        force=force,\n                                        quiet=quiet)", "code_tokens": ["def", "build", "(", "self", ",", "bug", ":", "Bug", ",", "force", ":", "bool", "=", "True", ",", "quiet", ":", "bool", "=", "False", ")", "->", "None", ":", "self", ".", "__installation", ".", "build", ".", "build", "(", "bug", ".", "image", ",", "force", "=", "force", ",", "quiet", "=", "quiet", ")"], "docstring": "Builds the Docker image associated with a given bug.\n\n        See: `BuildManager.build`", "docstring_tokens": ["Builds", "the", "Docker", "image", "associated", "with", "a", "given", "bug", "."], "sha": "68664f1977e85b37a78604f7c570382ffae1fa3b", "url": "https://github.com/squaresLab/BugZoo/blob/68664f1977e85b37a78604f7c570382ffae1fa3b/bugzoo/mgr/bug.py#L82-L94", "partition": "train", "idx": 30061}
{"repo": "mitsei/dlkit", "path": "dlkit/json_/assessment/mixins.py", "func_name": "AssessmentSessionSection.is_correct", "original_string": "def is_correct(self, question_id):\n        \"\"\"is the question answered correctly\"\"\"\n        response = self.get_response(question_id=question_id)\n        if response.is_answered():\n            item = self._get_item(response.get_item_id())\n            return item.is_response_correct(response)\n        raise errors.IllegalState()", "language": "python", "code": "def is_correct(self, question_id):\n        \"\"\"is the question answered correctly\"\"\"\n        response = self.get_response(question_id=question_id)\n        if response.is_answered():\n            item = self._get_item(response.get_item_id())\n            return item.is_response_correct(response)\n        raise errors.IllegalState()", "code_tokens": ["def", "is_correct", "(", "self", ",", "question_id", ")", ":", "response", "=", "self", ".", "get_response", "(", "question_id", "=", "question_id", ")", "if", "response", ".", "is_answered", "(", ")", ":", "item", "=", "self", ".", "_get_item", "(", "response", ".", "get_item_id", "(", ")", ")", "return", "item", ".", "is_response_correct", "(", "response", ")", "raise", "errors", ".", "IllegalState", "(", ")"], "docstring": "is the question answered correctly", "docstring_tokens": ["is", "the", "question", "answered", "correctly"], "sha": "445f968a175d61c8d92c0f617a3c17dc1dc7c584", "url": "https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/assessment/mixins.py#L704-L710", "partition": "train", "idx": 81826}
{"repo": "fitnr/censusgeocode", "path": "censusgeocode/censusgeocode.py", "func_name": "CensusGeocode.address", "original_string": "def address(self, street, city=None, state=None, zipcode=None, **kwargs):\n        '''Geocode an address.'''\n        fields = {\n            'street': street,\n            'city': city,\n            'state': state,\n            'zip': zipcode,\n        }\n\n        return self._fetch('address', fields, **kwargs)", "language": "python", "code": "def address(self, street, city=None, state=None, zipcode=None, **kwargs):\n        '''Geocode an address.'''\n        fields = {\n            'street': street,\n            'city': city,\n            'state': state,\n            'zip': zipcode,\n        }\n\n        return self._fetch('address', fields, **kwargs)", "code_tokens": ["def", "address", "(", "self", ",", "street", ",", "city", "=", "None", ",", "state", "=", "None", ",", "zipcode", "=", "None", ",", "*", "*", "kwargs", ")", ":", "fields", "=", "{", "'street'", ":", "street", ",", "'city'", ":", "city", ",", "'state'", ":", "state", ",", "'zip'", ":", "zipcode", ",", "}", "return", "self", ".", "_fetch", "(", "'address'", ",", "fields", ",", "*", "*", "kwargs", ")"], "docstring": "Geocode an address.", "docstring_tokens": ["Geocode", "an", "address", "."], "sha": "9414c331a63fbcfff6b7295cd8935c40ce54c88c", "url": "https://github.com/fitnr/censusgeocode/blob/9414c331a63fbcfff6b7295cd8935c40ce54c88c/censusgeocode/censusgeocode.py#L117-L126", "partition": "train", "idx": 39779}
{"repo": "juju/theblues", "path": "theblues/charmstore.py", "func_name": "CharmStore.entity_readme_url", "original_string": "def entity_readme_url(self, entity_id, channel=None):\n        '''Generate the url path for the readme of an entity.\n\n        @entity_id The id of the entity (i.e. charm, bundle).\n        @param channel Optional channel name.\n        '''\n        url = '{}/{}/readme'.format(self.url, _get_path(entity_id))\n        return _add_channel(url, channel)", "language": "python", "code": "def entity_readme_url(self, entity_id, channel=None):\n        '''Generate the url path for the readme of an entity.\n\n        @entity_id The id of the entity (i.e. charm, bundle).\n        @param channel Optional channel name.\n        '''\n        url = '{}/{}/readme'.format(self.url, _get_path(entity_id))\n        return _add_channel(url, channel)", "code_tokens": ["def", "entity_readme_url", "(", "self", ",", "entity_id", ",", "channel", "=", "None", ")", ":", "url", "=", "'{}/{}/readme'", ".", "format", "(", "self", ".", "url", ",", "_get_path", "(", "entity_id", ")", ")", "return", "_add_channel", "(", "url", ",", "channel", ")"], "docstring": "Generate the url path for the readme of an entity.\n\n        @entity_id The id of the entity (i.e. charm, bundle).\n        @param channel Optional channel name.", "docstring_tokens": ["Generate", "the", "url", "path", "for", "the", "readme", "of", "an", "entity", "."], "sha": "f4431f29e43d04fc32f38f4f86cea45cd4e6ae98", "url": "https://github.com/juju/theblues/blob/f4431f29e43d04fc32f38f4f86cea45cd4e6ae98/theblues/charmstore.py#L217-L224", "partition": "train", "idx": 5550}
{"repo": "twisted/txacme", "path": "src/txacme/client.py", "func_name": "Client._check_regr", "original_string": "def _check_regr(self, regr, new_reg):\n        \"\"\"\n        Check that a registration response contains the registration we were\n        expecting.\n        \"\"\"\n        body = getattr(new_reg, 'body', new_reg)\n        for k, v in body.items():\n            if k == 'resource' or not v:\n                continue\n            if regr.body[k] != v:\n                raise errors.UnexpectedUpdate(regr)\n        if regr.body.key != self.key.public_key():\n            raise errors.UnexpectedUpdate(regr)\n        return regr", "language": "python", "code": "def _check_regr(self, regr, new_reg):\n        \"\"\"\n        Check that a registration response contains the registration we were\n        expecting.\n        \"\"\"\n        body = getattr(new_reg, 'body', new_reg)\n        for k, v in body.items():\n            if k == 'resource' or not v:\n                continue\n            if regr.body[k] != v:\n                raise errors.UnexpectedUpdate(regr)\n        if regr.body.key != self.key.public_key():\n            raise errors.UnexpectedUpdate(regr)\n        return regr", "code_tokens": ["def", "_check_regr", "(", "self", ",", "regr", ",", "new_reg", ")", ":", "body", "=", "getattr", "(", "new_reg", ",", "'body'", ",", "new_reg", ")", "for", "k", ",", "v", "in", "body", ".", "items", "(", ")", ":", "if", "k", "==", "'resource'", "or", "not", "v", ":", "continue", "if", "regr", ".", "body", "[", "k", "]", "!=", "v", ":", "raise", "errors", ".", "UnexpectedUpdate", "(", "regr", ")", "if", "regr", ".", "body", ".", "key", "!=", "self", ".", "key", ".", "public_key", "(", ")", ":", "raise", "errors", ".", "UnexpectedUpdate", "(", "regr", ")", "return", "regr"], "docstring": "Check that a registration response contains the registration we were\n        expecting.", "docstring_tokens": ["Check", "that", "a", "registration", "response", "contains", "the", "registration", "we", "were", "expecting", "."], "sha": "9478381cc63c6d53d14bf8db8407c923f472989a", "url": "https://github.com/twisted/txacme/blob/9478381cc63c6d53d14bf8db8407c923f472989a/src/txacme/client.py#L253-L266", "partition": "train", "idx": 191211}
{"repo": "Alignak-monitoring/alignak", "path": "alignak/objects/config.py", "func_name": "Config.create_objects_for_type", "original_string": "def create_objects_for_type(self, raw_objects, o_type):\n        \"\"\"Generic function to create objects regarding the o_type\n\n        This function create real Alignak objects from the raw data got from the configuration.\n\n        :param raw_objects: Raw objects\n        :type raw_objects: dict\n        :param o_type: the object type we want to create\n        :type o_type: object\n        :return: None\n        \"\"\"\n\n        # Ex: the above code do for timeperiods:\n        # timeperiods = []\n        # for timeperiodcfg in objects['timeperiod']:\n        #    t = Timeperiod(timeperiodcfg)\n        #    timeperiods.append(t)\n        # self.timeperiods = Timeperiods(timeperiods)\n\n        types_creations = self.__class__.types_creations\n        (cls, clss, prop, initial_index, _) = types_creations[o_type]\n\n        # List to store the created objects\n        lst = []\n        try:\n            logger.info(\"- creating '%s' objects\", o_type)\n            for obj_cfg in raw_objects[o_type]:\n                # We create the object\n                my_object = cls(obj_cfg)\n                # and append it to the list\n                lst.append(my_object)\n            if not lst:\n                logger.info(\"  none.\")\n        except KeyError:\n            logger.info(\"  no %s objects in the configuration\", o_type)\n\n        # Create the objects list and set it in our properties\n        setattr(self, prop, clss(lst, initial_index))", "language": "python", "code": "def create_objects_for_type(self, raw_objects, o_type):\n        \"\"\"Generic function to create objects regarding the o_type\n\n        This function create real Alignak objects from the raw data got from the configuration.\n\n        :param raw_objects: Raw objects\n        :type raw_objects: dict\n        :param o_type: the object type we want to create\n        :type o_type: object\n        :return: None\n        \"\"\"\n\n        # Ex: the above code do for timeperiods:\n        # timeperiods = []\n        # for timeperiodcfg in objects['timeperiod']:\n        #    t = Timeperiod(timeperiodcfg)\n        #    timeperiods.append(t)\n        # self.timeperiods = Timeperiods(timeperiods)\n\n        types_creations = self.__class__.types_creations\n        (cls, clss, prop, initial_index, _) = types_creations[o_type]\n\n        # List to store the created objects\n        lst = []\n        try:\n            logger.info(\"- creating '%s' objects\", o_type)\n            for obj_cfg in raw_objects[o_type]:\n                # We create the object\n                my_object = cls(obj_cfg)\n                # and append it to the list\n                lst.append(my_object)\n            if not lst:\n                logger.info(\"  none.\")\n        except KeyError:\n            logger.info(\"  no %s objects in the configuration\", o_type)\n\n        # Create the objects list and set it in our properties\n        setattr(self, prop, clss(lst, initial_index))", "code_tokens": ["def", "create_objects_for_type", "(", "self", ",", "raw_objects", ",", "o_type", ")", ":", "# Ex: the above code do for timeperiods:", "# timeperiods = []", "# for timeperiodcfg in objects['timeperiod']:", "#    t = Timeperiod(timeperiodcfg)", "#    timeperiods.append(t)", "# self.timeperiods = Timeperiods(timeperiods)", "types_creations", "=", "self", ".", "__class__", ".", "types_creations", "(", "cls", ",", "clss", ",", "prop", ",", "initial_index", ",", "_", ")", "=", "types_creations", "[", "o_type", "]", "# List to store the created objects", "lst", "=", "[", "]", "try", ":", "logger", ".", "info", "(", "\"- creating '%s' objects\"", ",", "o_type", ")", "for", "obj_cfg", "in", "raw_objects", "[", "o_type", "]", ":", "# We create the object", "my_object", "=", "cls", "(", "obj_cfg", ")", "# and append it to the list", "lst", ".", "append", "(", "my_object", ")", "if", "not", "lst", ":", "logger", ".", "info", "(", "\"  none.\"", ")", "except", "KeyError", ":", "logger", ".", "info", "(", "\"  no %s objects in the configuration\"", ",", "o_type", ")", "# Create the objects list and set it in our properties", "setattr", "(", "self", ",", "prop", ",", "clss", "(", "lst", ",", "initial_index", ")", ")"], "docstring": "Generic function to create objects regarding the o_type\n\n        This function create real Alignak objects from the raw data got from the configuration.\n\n        :param raw_objects: Raw objects\n        :type raw_objects: dict\n        :param o_type: the object type we want to create\n        :type o_type: object\n        :return: None", "docstring_tokens": ["Generic", "function", "to", "create", "objects", "regarding", "the", "o_type"], "sha": "f3c145207e83159b799d3714e4241399c7740a64", "url": "https://github.com/Alignak-monitoring/alignak/blob/f3c145207e83159b799d3714e4241399c7740a64/alignak/objects/config.py#L1454-L1491", "partition": "train", "idx": 19721}
{"repo": "pyvisa/pyvisa", "path": "pyvisa/ctwrapper/functions.py", "func_name": "move_out_64", "original_string": "def move_out_64(library, session, space, offset, length, data, extended=False):\n    \"\"\"Moves an 64-bit block of data from local memory to the specified address space and offset.\n\n    Corresponds to viMoveOut64* functions of the VISA library.\n\n    :param library: the visa library wrapped by ctypes.\n    :param session: Unique logical identifier to a session.\n    :param space: Specifies the address space. (Constants.*SPACE*)\n    :param offset: Offset (in bytes) of the address or register from which to read.\n    :param length: Number of elements to transfer, where the data width of the elements to transfer\n                   is identical to the source data width.\n    :param data: Data to write to bus.\n    :param extended: Use 64 bits offset independent of the platform.\n    :return: return value of the library call.\n    :rtype: :class:`pyvisa.constants.StatusCode`\n    \"\"\"\n    converted_buffer = (ViUInt64 * length)(*tuple(data))\n    if extended:\n        return library.viMoveOut64Ex(session, space, offset, length, converted_buffer)\n    else:\n        return library.viMoveOut64(session, space, offset, length, converted_buffer)", "language": "python", "code": "def move_out_64(library, session, space, offset, length, data, extended=False):\n    \"\"\"Moves an 64-bit block of data from local memory to the specified address space and offset.\n\n    Corresponds to viMoveOut64* functions of the VISA library.\n\n    :param library: the visa library wrapped by ctypes.\n    :param session: Unique logical identifier to a session.\n    :param space: Specifies the address space. (Constants.*SPACE*)\n    :param offset: Offset (in bytes) of the address or register from which to read.\n    :param length: Number of elements to transfer, where the data width of the elements to transfer\n                   is identical to the source data width.\n    :param data: Data to write to bus.\n    :param extended: Use 64 bits offset independent of the platform.\n    :return: return value of the library call.\n    :rtype: :class:`pyvisa.constants.StatusCode`\n    \"\"\"\n    converted_buffer = (ViUInt64 * length)(*tuple(data))\n    if extended:\n        return library.viMoveOut64Ex(session, space, offset, length, converted_buffer)\n    else:\n        return library.viMoveOut64(session, space, offset, length, converted_buffer)", "code_tokens": ["def", "move_out_64", "(", "library", ",", "session", ",", "space", ",", "offset", ",", "length", ",", "data", ",", "extended", "=", "False", ")", ":", "converted_buffer", "=", "(", "ViUInt64", "*", "length", ")", "(", "*", "tuple", "(", "data", ")", ")", "if", "extended", ":", "return", "library", ".", "viMoveOut64Ex", "(", "session", ",", "space", ",", "offset", ",", "length", ",", "converted_buffer", ")", "else", ":", "return", "library", ".", "viMoveOut64", "(", "session", ",", "space", ",", "offset", ",", "length", ",", "converted_buffer", ")"], "docstring": "Moves an 64-bit block of data from local memory to the specified address space and offset.\n\n    Corresponds to viMoveOut64* functions of the VISA library.\n\n    :param library: the visa library wrapped by ctypes.\n    :param session: Unique logical identifier to a session.\n    :param space: Specifies the address space. (Constants.*SPACE*)\n    :param offset: Offset (in bytes) of the address or register from which to read.\n    :param length: Number of elements to transfer, where the data width of the elements to transfer\n                   is identical to the source data width.\n    :param data: Data to write to bus.\n    :param extended: Use 64 bits offset independent of the platform.\n    :return: return value of the library call.\n    :rtype: :class:`pyvisa.constants.StatusCode`", "docstring_tokens": ["Moves", "an", "64", "-", "bit", "block", "of", "data", "from", "local", "memory", "to", "the", "specified", "address", "space", "and", "offset", "."], "sha": "b8b2d4371e1f00782856aa9176ff1ced6bcb3798", "url": "https://github.com/pyvisa/pyvisa/blob/b8b2d4371e1f00782856aa9176ff1ced6bcb3798/pyvisa/ctwrapper/functions.py#L1166-L1186", "partition": "train", "idx": 228878}
{"repo": "apache/incubator-mxnet", "path": "python/mxnet/symbol/random.py", "func_name": "generalized_negative_binomial", "original_string": "def generalized_negative_binomial(mu=1, alpha=1, shape=_Null, dtype=_Null, **kwargs):\n    \"\"\"Draw random samples from a generalized negative binomial distribution.\n\n    Samples are distributed according to a generalized negative binomial\n    distribution parametrized by *mu* (mean) and *alpha* (dispersion).\n    *alpha* is defined as *1/k* where *k* is the failure limit of the\n    number of unsuccessful experiments (generalized to real numbers).\n    Samples will always be returned as a floating point data type.\n\n    Parameters\n    ----------\n    mu : float or Symbol, optional\n        Mean of the negative binomial distribution.\n    alpha : float or Symbol, optional\n        Alpha (dispersion) parameter of the negative binomial distribution.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `mu` and\n        `alpha` are scalars, output shape will be `(m, n)`. If `mu` and `alpha`\n        are Symbols with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n\n    Returns\n    -------\n    Symbol\n        If input `shape` has dimensions, e.g., `(m, n)`, and `mu` and\n        `alpha` are scalars, returned Symbol will resolve to shape `(m, n)`. If `mu`\n        and `alpha` are Symbols with shape, e.g., `(x, y)`, returned Symbol will resolve\n        to shape `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    \"\"\"\n    return _random_helper(_internal._random_generalized_negative_binomial,\n                          _internal._sample_generalized_negative_binomial,\n                          [mu, alpha], shape, dtype, kwargs)", "language": "python", "code": "def generalized_negative_binomial(mu=1, alpha=1, shape=_Null, dtype=_Null, **kwargs):\n    \"\"\"Draw random samples from a generalized negative binomial distribution.\n\n    Samples are distributed according to a generalized negative binomial\n    distribution parametrized by *mu* (mean) and *alpha* (dispersion).\n    *alpha* is defined as *1/k* where *k* is the failure limit of the\n    number of unsuccessful experiments (generalized to real numbers).\n    Samples will always be returned as a floating point data type.\n\n    Parameters\n    ----------\n    mu : float or Symbol, optional\n        Mean of the negative binomial distribution.\n    alpha : float or Symbol, optional\n        Alpha (dispersion) parameter of the negative binomial distribution.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `mu` and\n        `alpha` are scalars, output shape will be `(m, n)`. If `mu` and `alpha`\n        are Symbols with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n\n    Returns\n    -------\n    Symbol\n        If input `shape` has dimensions, e.g., `(m, n)`, and `mu` and\n        `alpha` are scalars, returned Symbol will resolve to shape `(m, n)`. If `mu`\n        and `alpha` are Symbols with shape, e.g., `(x, y)`, returned Symbol will resolve\n        to shape `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    \"\"\"\n    return _random_helper(_internal._random_generalized_negative_binomial,\n                          _internal._sample_generalized_negative_binomial,\n                          [mu, alpha], shape, dtype, kwargs)", "code_tokens": ["def", "generalized_negative_binomial", "(", "mu", "=", "1", ",", "alpha", "=", "1", ",", "shape", "=", "_Null", ",", "dtype", "=", "_Null", ",", "*", "*", "kwargs", ")", ":", "return", "_random_helper", "(", "_internal", ".", "_random_generalized_negative_binomial", ",", "_internal", ".", "_sample_generalized_negative_binomial", ",", "[", "mu", ",", "alpha", "]", ",", "shape", ",", "dtype", ",", "kwargs", ")"], "docstring": "Draw random samples from a generalized negative binomial distribution.\n\n    Samples are distributed according to a generalized negative binomial\n    distribution parametrized by *mu* (mean) and *alpha* (dispersion).\n    *alpha* is defined as *1/k* where *k* is the failure limit of the\n    number of unsuccessful experiments (generalized to real numbers).\n    Samples will always be returned as a floating point data type.\n\n    Parameters\n    ----------\n    mu : float or Symbol, optional\n        Mean of the negative binomial distribution.\n    alpha : float or Symbol, optional\n        Alpha (dispersion) parameter of the negative binomial distribution.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `mu` and\n        `alpha` are scalars, output shape will be `(m, n)`. If `mu` and `alpha`\n        are Symbols with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n\n    Returns\n    -------\n    Symbol\n        If input `shape` has dimensions, e.g., `(m, n)`, and `mu` and\n        `alpha` are scalars, returned Symbol will resolve to shape `(m, n)`. If `mu`\n        and `alpha` are Symbols with shape, e.g., `(x, y)`, returned Symbol will resolve\n        to shape `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.", "docstring_tokens": ["Draw", "random", "samples", "from", "a", "generalized", "negative", "binomial", "distribution", "."], "sha": "1af29e9c060a4c7d60eeaacba32afdb9a7775ba7", "url": "https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/symbol/random.py#L248-L281", "partition": "train", "idx": 163848}
{"repo": "saltstack/salt", "path": "salt/modules/grub_legacy.py", "func_name": "conf", "original_string": "def conf():\n    '''\n    Parse GRUB conf file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' grub.conf\n    '''\n    stanza = ''\n    stanzas = []\n    in_stanza = False\n    ret = {}\n    pos = 0\n    try:\n        with salt.utils.files.fopen(_detect_conf(), 'r') as _fp:\n            for line in _fp:\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.startswith('#'):\n                    continue\n                if line.startswith('\\n'):\n                    in_stanza = False\n                    if 'title' in stanza:\n                        stanza += 'order {0}'.format(pos)\n                        pos += 1\n                        stanzas.append(stanza)\n                    stanza = ''\n                    continue\n                if line.strip().startswith('title'):\n                    if in_stanza:\n                        stanza += 'order {0}'.format(pos)\n                        pos += 1\n                        stanzas.append(stanza)\n                        stanza = ''\n                    else:\n                        in_stanza = True\n                if in_stanza:\n                    stanza += line\n                if not in_stanza:\n                    key, value = _parse_line(line)\n                    ret[key] = value\n            if in_stanza:\n                if not line.endswith('\\n'):\n                    line += '\\n'\n                stanza += line\n                stanza += 'order {0}'.format(pos)\n                pos += 1\n                stanzas.append(stanza)\n    except (IOError, OSError) as exc:\n        msg = \"Could not read grub config: {0}\"\n        raise CommandExecutionError(msg.format(exc))\n\n    ret['stanzas'] = []\n    for stanza in stanzas:\n        mydict = {}\n        for line in stanza.strip().splitlines():\n            key, value = _parse_line(line)\n            mydict[key] = value\n        ret['stanzas'].append(mydict)\n    return ret", "language": "python", "code": "def conf():\n    '''\n    Parse GRUB conf file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' grub.conf\n    '''\n    stanza = ''\n    stanzas = []\n    in_stanza = False\n    ret = {}\n    pos = 0\n    try:\n        with salt.utils.files.fopen(_detect_conf(), 'r') as _fp:\n            for line in _fp:\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.startswith('#'):\n                    continue\n                if line.startswith('\\n'):\n                    in_stanza = False\n                    if 'title' in stanza:\n                        stanza += 'order {0}'.format(pos)\n                        pos += 1\n                        stanzas.append(stanza)\n                    stanza = ''\n                    continue\n                if line.strip().startswith('title'):\n                    if in_stanza:\n                        stanza += 'order {0}'.format(pos)\n                        pos += 1\n                        stanzas.append(stanza)\n                        stanza = ''\n                    else:\n                        in_stanza = True\n                if in_stanza:\n                    stanza += line\n                if not in_stanza:\n                    key, value = _parse_line(line)\n                    ret[key] = value\n            if in_stanza:\n                if not line.endswith('\\n'):\n                    line += '\\n'\n                stanza += line\n                stanza += 'order {0}'.format(pos)\n                pos += 1\n                stanzas.append(stanza)\n    except (IOError, OSError) as exc:\n        msg = \"Could not read grub config: {0}\"\n        raise CommandExecutionError(msg.format(exc))\n\n    ret['stanzas'] = []\n    for stanza in stanzas:\n        mydict = {}\n        for line in stanza.strip().splitlines():\n            key, value = _parse_line(line)\n            mydict[key] = value\n        ret['stanzas'].append(mydict)\n    return ret", "code_tokens": ["def", "conf", "(", ")", ":", "stanza", "=", "''", "stanzas", "=", "[", "]", "in_stanza", "=", "False", "ret", "=", "{", "}", "pos", "=", "0", "try", ":", "with", "salt", ".", "utils", ".", "files", ".", "fopen", "(", "_detect_conf", "(", ")", ",", "'r'", ")", "as", "_fp", ":", "for", "line", "in", "_fp", ":", "line", "=", "salt", ".", "utils", ".", "stringutils", ".", "to_unicode", "(", "line", ")", "if", "line", ".", "startswith", "(", "'#'", ")", ":", "continue", "if", "line", ".", "startswith", "(", "'\\n'", ")", ":", "in_stanza", "=", "False", "if", "'title'", "in", "stanza", ":", "stanza", "+=", "'order {0}'", ".", "format", "(", "pos", ")", "pos", "+=", "1", "stanzas", ".", "append", "(", "stanza", ")", "stanza", "=", "''", "continue", "if", "line", ".", "strip", "(", ")", ".", "startswith", "(", "'title'", ")", ":", "if", "in_stanza", ":", "stanza", "+=", "'order {0}'", ".", "format", "(", "pos", ")", "pos", "+=", "1", "stanzas", ".", "append", "(", "stanza", ")", "stanza", "=", "''", "else", ":", "in_stanza", "=", "True", "if", "in_stanza", ":", "stanza", "+=", "line", "if", "not", "in_stanza", ":", "key", ",", "value", "=", "_parse_line", "(", "line", ")", "ret", "[", "key", "]", "=", "value", "if", "in_stanza", ":", "if", "not", "line", ".", "endswith", "(", "'\\n'", ")", ":", "line", "+=", "'\\n'", "stanza", "+=", "line", "stanza", "+=", "'order {0}'", ".", "format", "(", "pos", ")", "pos", "+=", "1", "stanzas", ".", "append", "(", "stanza", ")", "except", "(", "IOError", ",", "OSError", ")", "as", "exc", ":", "msg", "=", "\"Could not read grub config: {0}\"", "raise", "CommandExecutionError", "(", "msg", ".", "format", "(", "exc", ")", ")", "ret", "[", "'stanzas'", "]", "=", "[", "]", "for", "stanza", "in", "stanzas", ":", "mydict", "=", "{", "}", "for", "line", "in", "stanza", ".", "strip", "(", ")", ".", "splitlines", "(", ")", ":", "key", ",", "value", "=", "_parse_line", "(", "line", ")", "mydict", "[", "key", "]", "=", "value", "ret", "[", "'stanzas'", "]", ".", "append", "(", "mydict", ")", "return", "ret"], "docstring": "Parse GRUB conf file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' grub.conf", "docstring_tokens": ["Parse", "GRUB", "conf", "file"], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/grub_legacy.py#L55-L115", "partition": "train", "idx": 122690}
{"repo": "saltstack/salt", "path": "salt/grains/minion_process.py", "func_name": "_username", "original_string": "def _username():\n    '''\n    Grain for the minion username\n    '''\n    if pwd:\n        username = pwd.getpwuid(os.getuid()).pw_name\n    else:\n        username = getpass.getuser()\n\n    return username", "language": "python", "code": "def _username():\n    '''\n    Grain for the minion username\n    '''\n    if pwd:\n        username = pwd.getpwuid(os.getuid()).pw_name\n    else:\n        username = getpass.getuser()\n\n    return username", "code_tokens": ["def", "_username", "(", ")", ":", "if", "pwd", ":", "username", "=", "pwd", ".", "getpwuid", "(", "os", ".", "getuid", "(", ")", ")", ".", "pw_name", "else", ":", "username", "=", "getpass", ".", "getuser", "(", ")", "return", "username"], "docstring": "Grain for the minion username", "docstring_tokens": ["Grain", "for", "the", "minion", "username"], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/grains/minion_process.py#L34-L43", "partition": "train", "idx": 176972}
{"repo": "iterative/dvc", "path": "dvc/utils/compat.py", "func_name": "csv_reader", "original_string": "def csv_reader(unicode_csv_data, dialect=None, **kwargs):\n    \"\"\"csv.reader doesn't support Unicode input, so need to use some tricks\n    to work around this.\n\n    Source: https://docs.python.org/2/library/csv.html#csv-examples\n    \"\"\"\n    import csv\n\n    dialect = dialect or csv.excel\n\n    if is_py3:\n        # Python3 supports encoding by default, so just return the object\n        for row in csv.reader(unicode_csv_data, dialect=dialect, **kwargs):\n            yield [cell for cell in row]\n\n    else:\n        # csv.py doesn't do Unicode; encode temporarily as UTF-8:\n        reader = csv.reader(\n            utf_8_encoder(unicode_csv_data), dialect=dialect, **kwargs\n        )\n        for row in reader:\n            # decode UTF-8 back to Unicode, cell by cell:\n            yield [unicode(cell, \"utf-8\") for cell in row]", "language": "python", "code": "def csv_reader(unicode_csv_data, dialect=None, **kwargs):\n    \"\"\"csv.reader doesn't support Unicode input, so need to use some tricks\n    to work around this.\n\n    Source: https://docs.python.org/2/library/csv.html#csv-examples\n    \"\"\"\n    import csv\n\n    dialect = dialect or csv.excel\n\n    if is_py3:\n        # Python3 supports encoding by default, so just return the object\n        for row in csv.reader(unicode_csv_data, dialect=dialect, **kwargs):\n            yield [cell for cell in row]\n\n    else:\n        # csv.py doesn't do Unicode; encode temporarily as UTF-8:\n        reader = csv.reader(\n            utf_8_encoder(unicode_csv_data), dialect=dialect, **kwargs\n        )\n        for row in reader:\n            # decode UTF-8 back to Unicode, cell by cell:\n            yield [unicode(cell, \"utf-8\") for cell in row]", "code_tokens": ["def", "csv_reader", "(", "unicode_csv_data", ",", "dialect", "=", "None", ",", "*", "*", "kwargs", ")", ":", "import", "csv", "dialect", "=", "dialect", "or", "csv", ".", "excel", "if", "is_py3", ":", "# Python3 supports encoding by default, so just return the object", "for", "row", "in", "csv", ".", "reader", "(", "unicode_csv_data", ",", "dialect", "=", "dialect", ",", "*", "*", "kwargs", ")", ":", "yield", "[", "cell", "for", "cell", "in", "row", "]", "else", ":", "# csv.py doesn't do Unicode; encode temporarily as UTF-8:", "reader", "=", "csv", ".", "reader", "(", "utf_8_encoder", "(", "unicode_csv_data", ")", ",", "dialect", "=", "dialect", ",", "*", "*", "kwargs", ")", "for", "row", "in", "reader", ":", "# decode UTF-8 back to Unicode, cell by cell:", "yield", "[", "unicode", "(", "cell", ",", "\"utf-8\"", ")", "for", "cell", "in", "row", "]"], "docstring": "csv.reader doesn't support Unicode input, so need to use some tricks\n    to work around this.\n\n    Source: https://docs.python.org/2/library/csv.html#csv-examples", "docstring_tokens": ["csv", ".", "reader", "doesn", "t", "support", "Unicode", "input", "so", "need", "to", "use", "some", "tricks", "to", "work", "around", "this", "."], "sha": "8bb21261e34c9632453e09090de7ebe50e38d341", "url": "https://github.com/iterative/dvc/blob/8bb21261e34c9632453e09090de7ebe50e38d341/dvc/utils/compat.py#L30-L52", "partition": "train", "idx": 169963}
{"repo": "MrYsLab/pymata-aio", "path": "examples/sparkfun_redbot/sparkfun_experiments/Exp6_LineFollowing_IRSensors.py", "func_name": "signal_handler", "original_string": "def signal_handler(sig, frame):\n    \"\"\"Helper method to shutdown the RedBot if Ctrl-c is pressed\"\"\"\n    print('\\nYou pressed Ctrl+C')\n    if board is not None:\n        board.send_reset()\n        board.shutdown()\n    sys.exit(0)", "language": "python", "code": "def signal_handler(sig, frame):\n    \"\"\"Helper method to shutdown the RedBot if Ctrl-c is pressed\"\"\"\n    print('\\nYou pressed Ctrl+C')\n    if board is not None:\n        board.send_reset()\n        board.shutdown()\n    sys.exit(0)", "code_tokens": ["def", "signal_handler", "(", "sig", ",", "frame", ")", ":", "print", "(", "'\\nYou pressed Ctrl+C'", ")", "if", "board", "is", "not", "None", ":", "board", ".", "send_reset", "(", ")", "board", ".", "shutdown", "(", ")", "sys", ".", "exit", "(", "0", ")"], "docstring": "Helper method to shutdown the RedBot if Ctrl-c is pressed", "docstring_tokens": ["Helper", "method", "to", "shutdown", "the", "RedBot", "if", "Ctrl", "-", "c", "is", "pressed"], "sha": "015081a4628b9d47dfe3f8d6c698ff903f107810", "url": "https://github.com/MrYsLab/pymata-aio/blob/015081a4628b9d47dfe3f8d6c698ff903f107810/examples/sparkfun_redbot/sparkfun_experiments/Exp6_LineFollowing_IRSensors.py#L45-L51", "partition": "train", "idx": 140808}
{"repo": "softlayer/softlayer-python", "path": "SoftLayer/managers/firewall.py", "func_name": "FirewallManager.edit_dedicated_fwl_rules", "original_string": "def edit_dedicated_fwl_rules(self, firewall_id, rules):\n        \"\"\"Edit the rules for dedicated firewall.\n\n        :param integer firewall_id: the instance ID of the dedicated firewall\n        :param list rules: the rules to be pushed on the firewall as defined by\n                           SoftLayer_Network_Firewall_Update_Request_Rule\n        \"\"\"\n\n        mask = ('mask[networkVlan[firewallInterfaces'\n                '[firewallContextAccessControlLists]]]')\n        svc = self.client['Network_Vlan_Firewall']\n        fwl = svc.getObject(id=firewall_id, mask=mask)\n        network_vlan = fwl['networkVlan']\n\n        for fwl1 in network_vlan['firewallInterfaces']:\n            if fwl1['name'] == 'inside':\n                continue\n\n            for control_list in fwl1['firewallContextAccessControlLists']:\n                if control_list['direction'] == 'out':\n                    continue\n                fwl_ctx_acl_id = control_list['id']\n\n        template = {'firewallContextAccessControlListId': fwl_ctx_acl_id,\n                    'rules': rules}\n\n        svc = self.client['Network_Firewall_Update_Request']\n        return svc.createObject(template)", "language": "python", "code": "def edit_dedicated_fwl_rules(self, firewall_id, rules):\n        \"\"\"Edit the rules for dedicated firewall.\n\n        :param integer firewall_id: the instance ID of the dedicated firewall\n        :param list rules: the rules to be pushed on the firewall as defined by\n                           SoftLayer_Network_Firewall_Update_Request_Rule\n        \"\"\"\n\n        mask = ('mask[networkVlan[firewallInterfaces'\n                '[firewallContextAccessControlLists]]]')\n        svc = self.client['Network_Vlan_Firewall']\n        fwl = svc.getObject(id=firewall_id, mask=mask)\n        network_vlan = fwl['networkVlan']\n\n        for fwl1 in network_vlan['firewallInterfaces']:\n            if fwl1['name'] == 'inside':\n                continue\n\n            for control_list in fwl1['firewallContextAccessControlLists']:\n                if control_list['direction'] == 'out':\n                    continue\n                fwl_ctx_acl_id = control_list['id']\n\n        template = {'firewallContextAccessControlListId': fwl_ctx_acl_id,\n                    'rules': rules}\n\n        svc = self.client['Network_Firewall_Update_Request']\n        return svc.createObject(template)", "code_tokens": ["def", "edit_dedicated_fwl_rules", "(", "self", ",", "firewall_id", ",", "rules", ")", ":", "mask", "=", "(", "'mask[networkVlan[firewallInterfaces'", "'[firewallContextAccessControlLists]]]'", ")", "svc", "=", "self", ".", "client", "[", "'Network_Vlan_Firewall'", "]", "fwl", "=", "svc", ".", "getObject", "(", "id", "=", "firewall_id", ",", "mask", "=", "mask", ")", "network_vlan", "=", "fwl", "[", "'networkVlan'", "]", "for", "fwl1", "in", "network_vlan", "[", "'firewallInterfaces'", "]", ":", "if", "fwl1", "[", "'name'", "]", "==", "'inside'", ":", "continue", "for", "control_list", "in", "fwl1", "[", "'firewallContextAccessControlLists'", "]", ":", "if", "control_list", "[", "'direction'", "]", "==", "'out'", ":", "continue", "fwl_ctx_acl_id", "=", "control_list", "[", "'id'", "]", "template", "=", "{", "'firewallContextAccessControlListId'", ":", "fwl_ctx_acl_id", ",", "'rules'", ":", "rules", "}", "svc", "=", "self", ".", "client", "[", "'Network_Firewall_Update_Request'", "]", "return", "svc", ".", "createObject", "(", "template", ")"], "docstring": "Edit the rules for dedicated firewall.\n\n        :param integer firewall_id: the instance ID of the dedicated firewall\n        :param list rules: the rules to be pushed on the firewall as defined by\n                           SoftLayer_Network_Firewall_Update_Request_Rule", "docstring_tokens": ["Edit", "the", "rules", "for", "dedicated", "firewall", "."], "sha": "9f181be08cc3668353b05a6de0cb324f52cff6fa", "url": "https://github.com/softlayer/softlayer-python/blob/9f181be08cc3668353b05a6de0cb324f52cff6fa/SoftLayer/managers/firewall.py#L252-L279", "partition": "train", "idx": 234496}
{"repo": "SpriteLink/NIPAP", "path": "nipap-www/nipapwww/controllers/xhr.py", "func_name": "XhrController.search_prefix", "original_string": "def search_prefix(self):\n        \"\"\" Search prefixes. Does not yet incorporate all the functions of the\n            search_prefix API function due to difficulties with transferring\n            a complete 'dict-to-sql' encoded data structure.\n\n            Instead, a list of prefix attributes can be given which will be\n            matched with the 'equals' operator if notheing else is specified. If\n            multiple attributes are given, they will be combined with the 'and'\n            operator. Currently, it is not possible to specify different\n            operators for different attributes.\n        \"\"\"\n\n        # extract operator\n        if 'operator' in request.json:\n            operator = request.json['operator']\n        else:\n            operator = 'equals'\n\n        # fetch attributes from request.json\n        attr = XhrController.extract_prefix_attr(request.json)\n\n        # build query dict\n        n = 0\n        q = {}\n        for key, val in attr.items():\n            if n == 0:\n                q = {\n                    'operator': operator,\n                    'val1': key,\n                    'val2': val\n                }\n            else:\n                q = {\n                    'operator': 'and',\n                    'val1': {\n                        'operator': operator,\n                        'val1': key,\n                        'val2': val\n                    },\n                    'val2': q\n                }\n            n += 1\n\n        # extract search options\n        search_opts = {}\n        if 'children_depth' in request.json:\n            search_opts['children_depth'] = request.json['children_depth']\n        if 'parents_depth' in request.json:\n            search_opts['parents_depth'] = request.json['parents_depth']\n        if 'include_neighbors' in request.json:\n            search_opts['include_neighbors'] = request.json['include_neighbors']\n        if 'max_result' in request.json:\n            search_opts['max_result'] = request.json['max_result']\n        if 'offset' in request.json:\n            search_opts['offset'] = request.json['offset']\n\n        try:\n            result = Prefix.search(q, search_opts)\n        except NipapError, e:\n            return json.dumps({'error': 1, 'message': e.args, 'type': type(e).__name__})\n\n        return json.dumps(result, cls=NipapJSONEncoder)", "language": "python", "code": "def search_prefix(self):\n        \"\"\" Search prefixes. Does not yet incorporate all the functions of the\n            search_prefix API function due to difficulties with transferring\n            a complete 'dict-to-sql' encoded data structure.\n\n            Instead, a list of prefix attributes can be given which will be\n            matched with the 'equals' operator if notheing else is specified. If\n            multiple attributes are given, they will be combined with the 'and'\n            operator. Currently, it is not possible to specify different\n            operators for different attributes.\n        \"\"\"\n\n        # extract operator\n        if 'operator' in request.json:\n            operator = request.json['operator']\n        else:\n            operator = 'equals'\n\n        # fetch attributes from request.json\n        attr = XhrController.extract_prefix_attr(request.json)\n\n        # build query dict\n        n = 0\n        q = {}\n        for key, val in attr.items():\n            if n == 0:\n                q = {\n                    'operator': operator,\n                    'val1': key,\n                    'val2': val\n                }\n            else:\n                q = {\n                    'operator': 'and',\n                    'val1': {\n                        'operator': operator,\n                        'val1': key,\n                        'val2': val\n                    },\n                    'val2': q\n                }\n            n += 1\n\n        # extract search options\n        search_opts = {}\n        if 'children_depth' in request.json:\n            search_opts['children_depth'] = request.json['children_depth']\n        if 'parents_depth' in request.json:\n            search_opts['parents_depth'] = request.json['parents_depth']\n        if 'include_neighbors' in request.json:\n            search_opts['include_neighbors'] = request.json['include_neighbors']\n        if 'max_result' in request.json:\n            search_opts['max_result'] = request.json['max_result']\n        if 'offset' in request.json:\n            search_opts['offset'] = request.json['offset']\n\n        try:\n            result = Prefix.search(q, search_opts)\n        except NipapError, e:\n            return json.dumps({'error': 1, 'message': e.args, 'type': type(e).__name__})\n\n        return json.dumps(result, cls=NipapJSONEncoder)", "code_tokens": ["def", "search_prefix", "(", "self", ")", ":", "# extract operator", "if", "'operator'", "in", "request", ".", "json", ":", "operator", "=", "request", ".", "json", "[", "'operator'", "]", "else", ":", "operator", "=", "'equals'", "# fetch attributes from request.json", "attr", "=", "XhrController", ".", "extract_prefix_attr", "(", "request", ".", "json", ")", "# build query dict", "n", "=", "0", "q", "=", "{", "}", "for", "key", ",", "val", "in", "attr", ".", "items", "(", ")", ":", "if", "n", "==", "0", ":", "q", "=", "{", "'operator'", ":", "operator", ",", "'val1'", ":", "key", ",", "'val2'", ":", "val", "}", "else", ":", "q", "=", "{", "'operator'", ":", "'and'", ",", "'val1'", ":", "{", "'operator'", ":", "operator", ",", "'val1'", ":", "key", ",", "'val2'", ":", "val", "}", ",", "'val2'", ":", "q", "}", "n", "+=", "1", "# extract search options", "search_opts", "=", "{", "}", "if", "'children_depth'", "in", "request", ".", "json", ":", "search_opts", "[", "'children_depth'", "]", "=", "request", ".", "json", "[", "'children_depth'", "]", "if", "'parents_depth'", "in", "request", ".", "json", ":", "search_opts", "[", "'parents_depth'", "]", "=", "request", ".", "json", "[", "'parents_depth'", "]", "if", "'include_neighbors'", "in", "request", ".", "json", ":", "search_opts", "[", "'include_neighbors'", "]", "=", "request", ".", "json", "[", "'include_neighbors'", "]", "if", "'max_result'", "in", "request", ".", "json", ":", "search_opts", "[", "'max_result'", "]", "=", "request", ".", "json", "[", "'max_result'", "]", "if", "'offset'", "in", "request", ".", "json", ":", "search_opts", "[", "'offset'", "]", "=", "request", ".", "json", "[", "'offset'", "]", "try", ":", "result", "=", "Prefix", ".", "search", "(", "q", ",", "search_opts", ")", "except", "NipapError", ",", "e", ":", "return", "json", ".", "dumps", "(", "{", "'error'", ":", "1", ",", "'message'", ":", "e", ".", "args", ",", "'type'", ":", "type", "(", "e", ")", ".", "__name__", "}", ")", "return", "json", ".", "dumps", "(", "result", ",", "cls", "=", "NipapJSONEncoder", ")"], "docstring": "Search prefixes. Does not yet incorporate all the functions of the\n            search_prefix API function due to difficulties with transferring\n            a complete 'dict-to-sql' encoded data structure.\n\n            Instead, a list of prefix attributes can be given which will be\n            matched with the 'equals' operator if notheing else is specified. If\n            multiple attributes are given, they will be combined with the 'and'\n            operator. Currently, it is not possible to specify different\n            operators for different attributes.", "docstring_tokens": ["Search", "prefixes", ".", "Does", "not", "yet", "incorporate", "all", "the", "functions", "of", "the", "search_prefix", "API", "function", "due", "to", "difficulties", "with", "transferring", "a", "complete", "dict", "-", "to", "-", "sql", "encoded", "data", "structure", "."], "sha": "f96069f11ab952d80b13cab06e0528f2d24b3de9", "url": "https://github.com/SpriteLink/NIPAP/blob/f96069f11ab952d80b13cab06e0528f2d24b3de9/nipap-www/nipapwww/controllers/xhr.py#L349-L410", "partition": "train", "idx": 140419}
{"repo": "DataONEorg/d1_python", "path": "gmn/src/d1_gmn/app/db_filter.py", "func_name": "add_redact_annotation", "original_string": "def add_redact_annotation(request, query):\n    \"\"\"Flag LogEntry records that require ``ipAddress`` and ``subject`` fields to be\n    redacted before being returned to the client.\n\n    Only trusted subjects and subjects with ``write`` or ``changePermission`` on a\n    SciObj receive unredacted ``ipAddress`` and ``subject`` in LogEntry records for the\n    associated SciObj.\n\n    Subjects with only ``read`` access receive redacted records.\n\n    \"\"\"\n    return query.annotate(\n        redact=django.db.models.Exists(\n            d1_gmn.app.models.Permission.objects.filter(\n                sciobj=django.db.models.OuterRef('sciobj'),\n                subject__subject__in=request.all_subjects_set,\n                level__gte=d1_gmn.app.auth.WRITE_LEVEL,\n            ),\n            negated=True,\n        )\n    )", "language": "python", "code": "def add_redact_annotation(request, query):\n    \"\"\"Flag LogEntry records that require ``ipAddress`` and ``subject`` fields to be\n    redacted before being returned to the client.\n\n    Only trusted subjects and subjects with ``write`` or ``changePermission`` on a\n    SciObj receive unredacted ``ipAddress`` and ``subject`` in LogEntry records for the\n    associated SciObj.\n\n    Subjects with only ``read`` access receive redacted records.\n\n    \"\"\"\n    return query.annotate(\n        redact=django.db.models.Exists(\n            d1_gmn.app.models.Permission.objects.filter(\n                sciobj=django.db.models.OuterRef('sciobj'),\n                subject__subject__in=request.all_subjects_set,\n                level__gte=d1_gmn.app.auth.WRITE_LEVEL,\n            ),\n            negated=True,\n        )\n    )", "code_tokens": ["def", "add_redact_annotation", "(", "request", ",", "query", ")", ":", "return", "query", ".", "annotate", "(", "redact", "=", "django", ".", "db", ".", "models", ".", "Exists", "(", "d1_gmn", ".", "app", ".", "models", ".", "Permission", ".", "objects", ".", "filter", "(", "sciobj", "=", "django", ".", "db", ".", "models", ".", "OuterRef", "(", "'sciobj'", ")", ",", "subject__subject__in", "=", "request", ".", "all_subjects_set", ",", "level__gte", "=", "d1_gmn", ".", "app", ".", "auth", ".", "WRITE_LEVEL", ",", ")", ",", "negated", "=", "True", ",", ")", ")"], "docstring": "Flag LogEntry records that require ``ipAddress`` and ``subject`` fields to be\n    redacted before being returned to the client.\n\n    Only trusted subjects and subjects with ``write`` or ``changePermission`` on a\n    SciObj receive unredacted ``ipAddress`` and ``subject`` in LogEntry records for the\n    associated SciObj.\n\n    Subjects with only ``read`` access receive redacted records.", "docstring_tokens": ["Flag", "LogEntry", "records", "that", "require", "ipAddress", "and", "subject", "fields", "to", "be", "redacted", "before", "being", "returned", "to", "the", "client", "."], "sha": "3ac4d4f3ca052d3e8641a6a329cab526c8ddcb0d", "url": "https://github.com/DataONEorg/d1_python/blob/3ac4d4f3ca052d3e8641a6a329cab526c8ddcb0d/gmn/src/d1_gmn/app/db_filter.py#L61-L81", "partition": "train", "idx": 45037}
{"repo": "RPi-Distro/python-sense-hat", "path": "sense_hat/sense_hat.py", "func_name": "SenseHat.get_temperature_from_pressure", "original_string": "def get_temperature_from_pressure(self):\n        \"\"\"\n        Returns the temperature in Celsius from the pressure sensor\n        \"\"\"\n\n        self._init_pressure()  # Ensure pressure sensor is initialised\n        temp = 0\n        data = self._pressure.pressureRead()\n        if (data[2]):  # Temp valid\n            temp = data[3]\n        return temp", "language": "python", "code": "def get_temperature_from_pressure(self):\n        \"\"\"\n        Returns the temperature in Celsius from the pressure sensor\n        \"\"\"\n\n        self._init_pressure()  # Ensure pressure sensor is initialised\n        temp = 0\n        data = self._pressure.pressureRead()\n        if (data[2]):  # Temp valid\n            temp = data[3]\n        return temp", "code_tokens": ["def", "get_temperature_from_pressure", "(", "self", ")", ":", "self", ".", "_init_pressure", "(", ")", "# Ensure pressure sensor is initialised", "temp", "=", "0", "data", "=", "self", ".", "_pressure", ".", "pressureRead", "(", ")", "if", "(", "data", "[", "2", "]", ")", ":", "# Temp valid", "temp", "=", "data", "[", "3", "]", "return", "temp"], "docstring": "Returns the temperature in Celsius from the pressure sensor", "docstring_tokens": ["Returns", "the", "temperature", "in", "Celsius", "from", "the", "pressure", "sensor"], "sha": "9a37f0923ce8dbde69514c3b8d58d30de01c9ee7", "url": "https://github.com/RPi-Distro/python-sense-hat/blob/9a37f0923ce8dbde69514c3b8d58d30de01c9ee7/sense_hat/sense_hat.py#L589-L599", "partition": "train", "idx": 198300}
{"repo": "tensorflow/mesh", "path": "mesh_tensorflow/ops.py", "func_name": "conv2d_with_blocks", "original_string": "def conv2d_with_blocks(\n    conv_input,\n    conv_filter,\n    strides,\n    padding,\n    h_blocks_dim=None,\n    w_blocks_dim=None,\n    name=None):\n  \"\"\"conv2d operation with spatial partitioning.\n\n  Spatial partitioning is implemented by decomposing the image into blocks.\n  Block dimensions represented as h_blocks_dim and w_blocks_dim can be split\n  along the mesh axis. If split, then we do a halo exchange where each block\n  receives the part of the image from its left and right neighbors necessary to\n  do the convolution. Exchange can involve complete or partial blocks depending\n  on the filter height and width.\n\n  Currently, only \"SAME\" padding with dilation rate of 1 is supported.\n\n  Args:\n    conv_input: a Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, in_channels_dim]\n    conv_filter: a Tensor of shape\n      [filter_height, filter_width, in_channels_dim, out_channels_dim]\n    strides: A list of ints. 1-D tensor of length 4.\n    padding: string, \"SAME\". The type of padding algorithm to use.\n      Valid is not currently supported.\n    h_blocks_dim: Dimension representing number of height blocks.\n    w_blocks_dim: Dimension representing number of height blocks.\n    name: A name for the operation (optional).\n\n  Returns:\n    A Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, out_channels_dim]\n  \"\"\"\n  filter_h_dim, filter_w_dim = conv_filter.shape.dims[:2]\n  assert filter_h_dim.size % 2 == 1\n  assert filter_w_dim.size % 2 == 1\n  h_dim, w_dim = conv_input.shape.dims[-3:-1]\n\n  # If h_blocks_dim and w_blocks_dim is not split, directly call conv2d.\n  if h_blocks_dim is None and w_blocks_dim is None:\n    return conv2d(conv_input, conv_filter, strides, padding, name)\n\n  # Padding 'VALID' is not supported yet.\n  if padding != \"SAME\":\n    raise NotImplementedError(\"conv2d_with_blocks requires padding=SAME\")\n\n  # Halo exchange for h_blocks and w_blocks.\n  for blocks_dim, block_size_dim, halo_size in [\n      (h_blocks_dim, h_dim, filter_h_dim.size // 2),\n      (w_blocks_dim, w_dim, filter_w_dim.size // 2)]:\n    if halo_size > 0:\n      if blocks_dim is not None:\n        conv_input = halo_exchange(\n            conv_input, blocks_dim, block_size_dim, halo_size)\n      else:\n        conv_input = pad(\n            conv_input, [halo_size, halo_size], block_size_dim.name)\n  return conv2d(conv_input, conv_filter, strides, \"VALID\", name)", "language": "python", "code": "def conv2d_with_blocks(\n    conv_input,\n    conv_filter,\n    strides,\n    padding,\n    h_blocks_dim=None,\n    w_blocks_dim=None,\n    name=None):\n  \"\"\"conv2d operation with spatial partitioning.\n\n  Spatial partitioning is implemented by decomposing the image into blocks.\n  Block dimensions represented as h_blocks_dim and w_blocks_dim can be split\n  along the mesh axis. If split, then we do a halo exchange where each block\n  receives the part of the image from its left and right neighbors necessary to\n  do the convolution. Exchange can involve complete or partial blocks depending\n  on the filter height and width.\n\n  Currently, only \"SAME\" padding with dilation rate of 1 is supported.\n\n  Args:\n    conv_input: a Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, in_channels_dim]\n    conv_filter: a Tensor of shape\n      [filter_height, filter_width, in_channels_dim, out_channels_dim]\n    strides: A list of ints. 1-D tensor of length 4.\n    padding: string, \"SAME\". The type of padding algorithm to use.\n      Valid is not currently supported.\n    h_blocks_dim: Dimension representing number of height blocks.\n    w_blocks_dim: Dimension representing number of height blocks.\n    name: A name for the operation (optional).\n\n  Returns:\n    A Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, out_channels_dim]\n  \"\"\"\n  filter_h_dim, filter_w_dim = conv_filter.shape.dims[:2]\n  assert filter_h_dim.size % 2 == 1\n  assert filter_w_dim.size % 2 == 1\n  h_dim, w_dim = conv_input.shape.dims[-3:-1]\n\n  # If h_blocks_dim and w_blocks_dim is not split, directly call conv2d.\n  if h_blocks_dim is None and w_blocks_dim is None:\n    return conv2d(conv_input, conv_filter, strides, padding, name)\n\n  # Padding 'VALID' is not supported yet.\n  if padding != \"SAME\":\n    raise NotImplementedError(\"conv2d_with_blocks requires padding=SAME\")\n\n  # Halo exchange for h_blocks and w_blocks.\n  for blocks_dim, block_size_dim, halo_size in [\n      (h_blocks_dim, h_dim, filter_h_dim.size // 2),\n      (w_blocks_dim, w_dim, filter_w_dim.size // 2)]:\n    if halo_size > 0:\n      if blocks_dim is not None:\n        conv_input = halo_exchange(\n            conv_input, blocks_dim, block_size_dim, halo_size)\n      else:\n        conv_input = pad(\n            conv_input, [halo_size, halo_size], block_size_dim.name)\n  return conv2d(conv_input, conv_filter, strides, \"VALID\", name)", "code_tokens": ["def", "conv2d_with_blocks", "(", "conv_input", ",", "conv_filter", ",", "strides", ",", "padding", ",", "h_blocks_dim", "=", "None", ",", "w_blocks_dim", "=", "None", ",", "name", "=", "None", ")", ":", "filter_h_dim", ",", "filter_w_dim", "=", "conv_filter", ".", "shape", ".", "dims", "[", ":", "2", "]", "assert", "filter_h_dim", ".", "size", "%", "2", "==", "1", "assert", "filter_w_dim", ".", "size", "%", "2", "==", "1", "h_dim", ",", "w_dim", "=", "conv_input", ".", "shape", ".", "dims", "[", "-", "3", ":", "-", "1", "]", "# If h_blocks_dim and w_blocks_dim is not split, directly call conv2d.", "if", "h_blocks_dim", "is", "None", "and", "w_blocks_dim", "is", "None", ":", "return", "conv2d", "(", "conv_input", ",", "conv_filter", ",", "strides", ",", "padding", ",", "name", ")", "# Padding 'VALID' is not supported yet.", "if", "padding", "!=", "\"SAME\"", ":", "raise", "NotImplementedError", "(", "\"conv2d_with_blocks requires padding=SAME\"", ")", "# Halo exchange for h_blocks and w_blocks.", "for", "blocks_dim", ",", "block_size_dim", ",", "halo_size", "in", "[", "(", "h_blocks_dim", ",", "h_dim", ",", "filter_h_dim", ".", "size", "//", "2", ")", ",", "(", "w_blocks_dim", ",", "w_dim", ",", "filter_w_dim", ".", "size", "//", "2", ")", "]", ":", "if", "halo_size", ">", "0", ":", "if", "blocks_dim", "is", "not", "None", ":", "conv_input", "=", "halo_exchange", "(", "conv_input", ",", "blocks_dim", ",", "block_size_dim", ",", "halo_size", ")", "else", ":", "conv_input", "=", "pad", "(", "conv_input", ",", "[", "halo_size", ",", "halo_size", "]", ",", "block_size_dim", ".", "name", ")", "return", "conv2d", "(", "conv_input", ",", "conv_filter", ",", "strides", ",", "\"VALID\"", ",", "name", ")"], "docstring": "conv2d operation with spatial partitioning.\n\n  Spatial partitioning is implemented by decomposing the image into blocks.\n  Block dimensions represented as h_blocks_dim and w_blocks_dim can be split\n  along the mesh axis. If split, then we do a halo exchange where each block\n  receives the part of the image from its left and right neighbors necessary to\n  do the convolution. Exchange can involve complete or partial blocks depending\n  on the filter height and width.\n\n  Currently, only \"SAME\" padding with dilation rate of 1 is supported.\n\n  Args:\n    conv_input: a Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, in_channels_dim]\n    conv_filter: a Tensor of shape\n      [filter_height, filter_width, in_channels_dim, out_channels_dim]\n    strides: A list of ints. 1-D tensor of length 4.\n    padding: string, \"SAME\". The type of padding algorithm to use.\n      Valid is not currently supported.\n    h_blocks_dim: Dimension representing number of height blocks.\n    w_blocks_dim: Dimension representing number of height blocks.\n    name: A name for the operation (optional).\n\n  Returns:\n    A Tensor of shape\n      [batch, h_blocks_dim, w_blocks_dim, h_dim, w_dim, out_channels_dim]", "docstring_tokens": ["conv2d", "operation", "with", "spatial", "partitioning", "."], "sha": "3921196e5e43302e820da0a87329f25d7e2a3016", "url": "https://github.com/tensorflow/mesh/blob/3921196e5e43302e820da0a87329f25d7e2a3016/mesh_tensorflow/ops.py#L5049-L5108", "partition": "train", "idx": 222752}
{"repo": "spacetelescope/stsci.tools", "path": "lib/stsci/tools/fileutil.py", "func_name": "defvar", "original_string": "def defvar(varname):\n    \"\"\"Returns true if CL variable is defined.\"\"\"\n\n    if 'pyraf' in sys.modules:\n        #ONLY if pyraf is already loaded, import iraf into the namespace\n        from pyraf import iraf\n    else:\n        # else set iraf to None so it knows to not use iraf's environment\n        iraf = None\n\n    if iraf:\n        _irafdef = iraf.envget(varname)\n    else:\n        _irafdef = 0\n    return varname in _varDict or varname in os.environ or _irafdef", "language": "python", "code": "def defvar(varname):\n    \"\"\"Returns true if CL variable is defined.\"\"\"\n\n    if 'pyraf' in sys.modules:\n        #ONLY if pyraf is already loaded, import iraf into the namespace\n        from pyraf import iraf\n    else:\n        # else set iraf to None so it knows to not use iraf's environment\n        iraf = None\n\n    if iraf:\n        _irafdef = iraf.envget(varname)\n    else:\n        _irafdef = 0\n    return varname in _varDict or varname in os.environ or _irafdef", "code_tokens": ["def", "defvar", "(", "varname", ")", ":", "if", "'pyraf'", "in", "sys", ".", "modules", ":", "#ONLY if pyraf is already loaded, import iraf into the namespace", "from", "pyraf", "import", "iraf", "else", ":", "# else set iraf to None so it knows to not use iraf's environment", "iraf", "=", "None", "if", "iraf", ":", "_irafdef", "=", "iraf", ".", "envget", "(", "varname", ")", "else", ":", "_irafdef", "=", "0", "return", "varname", "in", "_varDict", "or", "varname", "in", "os", ".", "environ", "or", "_irafdef"], "docstring": "Returns true if CL variable is defined.", "docstring_tokens": ["Returns", "true", "if", "CL", "variable", "is", "defined", "."], "sha": "9a022503ad24ca54ce83331482dfa3ff6de9f403", "url": "https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/fileutil.py#L1364-L1378", "partition": "train", "idx": 193557}
{"repo": "mayfield/cellulario", "path": "cellulario/iocell.py", "func_name": "IOCell.cleanup_event_loop", "original_string": "def cleanup_event_loop(self):\n        \"\"\" Cleanup an event loop and close it down forever. \"\"\"\n        for task in asyncio.Task.all_tasks(loop=self.loop):\n            if self.debug:\n                warnings.warn('Cancelling task: %s' % task)\n            task._log_destroy_pending = False\n            task.cancel()\n        self.loop.close()\n        self.loop.set_exception_handler(self.loop_exception_handler_save)\n        self.loop_exception_handler_save = None\n        self.loop_policy = None\n        self.loop = None", "language": "python", "code": "def cleanup_event_loop(self):\n        \"\"\" Cleanup an event loop and close it down forever. \"\"\"\n        for task in asyncio.Task.all_tasks(loop=self.loop):\n            if self.debug:\n                warnings.warn('Cancelling task: %s' % task)\n            task._log_destroy_pending = False\n            task.cancel()\n        self.loop.close()\n        self.loop.set_exception_handler(self.loop_exception_handler_save)\n        self.loop_exception_handler_save = None\n        self.loop_policy = None\n        self.loop = None", "code_tokens": ["def", "cleanup_event_loop", "(", "self", ")", ":", "for", "task", "in", "asyncio", ".", "Task", ".", "all_tasks", "(", "loop", "=", "self", ".", "loop", ")", ":", "if", "self", ".", "debug", ":", "warnings", ".", "warn", "(", "'Cancelling task: %s'", "%", "task", ")", "task", ".", "_log_destroy_pending", "=", "False", "task", ".", "cancel", "(", ")", "self", ".", "loop", ".", "close", "(", ")", "self", ".", "loop", ".", "set_exception_handler", "(", "self", ".", "loop_exception_handler_save", ")", "self", ".", "loop_exception_handler_save", "=", "None", "self", ".", "loop_policy", "=", "None", "self", ".", "loop", "=", "None"], "docstring": "Cleanup an event loop and close it down forever.", "docstring_tokens": ["Cleanup", "an", "event", "loop", "and", "close", "it", "down", "forever", "."], "sha": "e9dc10532a0357bc90ebaa2655b36822f9249673", "url": "https://github.com/mayfield/cellulario/blob/e9dc10532a0357bc90ebaa2655b36822f9249673/cellulario/iocell.py#L99-L110", "partition": "train", "idx": 94487}
{"repo": "spyder-ide/spyder", "path": "spyder/app/mainwindow.py", "func_name": "MainWindow.reset_window_layout", "original_string": "def reset_window_layout(self):\r\n        \"\"\"Reset window layout to default\"\"\"\r\n        answer = QMessageBox.warning(self, _(\"Warning\"),\r\n                     _(\"Window layout will be reset to default settings: \"\r\n                       \"this affects window position, size and dockwidgets.\\n\"\r\n                       \"Do you want to continue?\"),\r\n                     QMessageBox.Yes | QMessageBox.No)\r\n        if answer == QMessageBox.Yes:\r\n            self.setup_layout(default=True)", "language": "python", "code": "def reset_window_layout(self):\r\n        \"\"\"Reset window layout to default\"\"\"\r\n        answer = QMessageBox.warning(self, _(\"Warning\"),\r\n                     _(\"Window layout will be reset to default settings: \"\r\n                       \"this affects window position, size and dockwidgets.\\n\"\r\n                       \"Do you want to continue?\"),\r\n                     QMessageBox.Yes | QMessageBox.No)\r\n        if answer == QMessageBox.Yes:\r\n            self.setup_layout(default=True)", "code_tokens": ["def", "reset_window_layout", "(", "self", ")", ":", "answer", "=", "QMessageBox", ".", "warning", "(", "self", ",", "_", "(", "\"Warning\"", ")", ",", "_", "(", "\"Window layout will be reset to default settings: \"", "\"this affects window position, size and dockwidgets.\\n\"", "\"Do you want to continue?\"", ")", ",", "QMessageBox", ".", "Yes", "|", "QMessageBox", ".", "No", ")", "if", "answer", "==", "QMessageBox", ".", "Yes", ":", "self", ".", "setup_layout", "(", "default", "=", "True", ")"], "docstring": "Reset window layout to default", "docstring_tokens": ["Reset", "window", "layout", "to", "default"], "sha": "f76836ce1b924bcc4efd3f74f2960d26a4e528e0", "url": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/app/mainwindow.py#L1903-L1911", "partition": "train", "idx": 170791}
{"repo": "gbowerman/azurerm", "path": "azurerm/insightsrp.py", "func_name": "list_metric_defs_for_resource", "original_string": "def list_metric_defs_for_resource(access_token, subscription_id, resource_group,\n                                  resource_provider, resource_type, resource_name):\n    '''List the monitoring metric definitions for a resource.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        resource_provider (str): Type of resource provider.\n        resource_type (str): Type of resource.\n        resource_name (str): Name of resource.\n\n    Returns:\n        HTTP response. JSON body of metric definitions.\n    '''\n    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/resourceGroups/', resource_group,\n                        '/providers/', resource_provider,\n                        '/', resource_type,\n                        '/', resource_name,\n                        '/providers/microsoft.insights',\n                        '/metricdefinitions?api-version=', INSIGHTS_METRICS_API])\n    return do_get(endpoint, access_token)", "language": "python", "code": "def list_metric_defs_for_resource(access_token, subscription_id, resource_group,\n                                  resource_provider, resource_type, resource_name):\n    '''List the monitoring metric definitions for a resource.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        resource_provider (str): Type of resource provider.\n        resource_type (str): Type of resource.\n        resource_name (str): Name of resource.\n\n    Returns:\n        HTTP response. JSON body of metric definitions.\n    '''\n    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/resourceGroups/', resource_group,\n                        '/providers/', resource_provider,\n                        '/', resource_type,\n                        '/', resource_name,\n                        '/providers/microsoft.insights',\n                        '/metricdefinitions?api-version=', INSIGHTS_METRICS_API])\n    return do_get(endpoint, access_token)", "code_tokens": ["def", "list_metric_defs_for_resource", "(", "access_token", ",", "subscription_id", ",", "resource_group", ",", "resource_provider", ",", "resource_type", ",", "resource_name", ")", ":", "endpoint", "=", "''", ".", "join", "(", "[", "get_rm_endpoint", "(", ")", ",", "'/subscriptions/'", ",", "subscription_id", ",", "'/resourceGroups/'", ",", "resource_group", ",", "'/providers/'", ",", "resource_provider", ",", "'/'", ",", "resource_type", ",", "'/'", ",", "resource_name", ",", "'/providers/microsoft.insights'", ",", "'/metricdefinitions?api-version='", ",", "INSIGHTS_METRICS_API", "]", ")", "return", "do_get", "(", "endpoint", ",", "access_token", ")"], "docstring": "List the monitoring metric definitions for a resource.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        resource_provider (str): Type of resource provider.\n        resource_type (str): Type of resource.\n        resource_name (str): Name of resource.\n\n    Returns:\n        HTTP response. JSON body of metric definitions.", "docstring_tokens": ["List", "the", "monitoring", "metric", "definitions", "for", "a", "resource", "."], "sha": "79d40431d3b13f8a36aadbff5029888383d72674", "url": "https://github.com/gbowerman/azurerm/blob/79d40431d3b13f8a36aadbff5029888383d72674/azurerm/insightsrp.py#L136-L159", "partition": "train", "idx": 110297}
{"repo": "brainiak/brainiak", "path": "brainiak/utils/fmrisim.py", "func_name": "apply_signal", "original_string": "def apply_signal(signal_function,\n                 volume_signal,\n                 ):\n    \"\"\"Combine the signal volume with its timecourse\n\n    Apply the convolution of the HRF and stimulus time course to the\n    volume.\n\n    Parameters\n    ----------\n\n    signal_function : timepoint by timecourse array, float\n        The timecourse of the signal over time. If there is only one column\n        then the same timecourse is applied to all non-zero voxels in\n        volume_signal. If there is more than one column then each column is\n        paired with a non-zero voxel in the volume_signal (a 3d numpy array\n        generated in generate_signal).\n\n    volume_signal : multi dimensional array, float\n        The volume containing the signal to be convolved with the same\n        dimensions as the output volume. The elements in volume_signal\n        indicate how strong each signal in signal_function are modulated by\n        in the output volume\n\n\n    Returns\n    ----------\n    signal : multidimensional array, float\n        The convolved signal volume with the same 3d as volume signal and\n        the same 4th dimension as signal_function\n\n    \"\"\"\n\n    # How many timecourses are there within the signal_function\n    timepoints = signal_function.shape[0]\n    timecourses = signal_function.shape[1]\n\n    # Preset volume\n    signal = np.zeros([volume_signal.shape[0], volume_signal.shape[\n        1], volume_signal.shape[2], timepoints])\n\n    # Find all the non-zero voxels in the brain\n    idxs = np.where(volume_signal != 0)\n    if timecourses == 1:\n        # If there is only one time course supplied then duplicate it for\n        # every voxel\n        signal_function = np.matlib.repmat(signal_function, 1, len(idxs[0]))\n\n    elif len(idxs[0]) != timecourses:\n        raise IndexError('The number of non-zero voxels in the volume and '\n                         'the number of timecourses does not match. Aborting')\n\n    # For each coordinate with a non zero voxel, fill in the timecourse for\n    # that voxel\n    for idx_counter in range(len(idxs[0])):\n        x = idxs[0][idx_counter]\n        y = idxs[1][idx_counter]\n        z = idxs[2][idx_counter]\n\n        # Pull out the function for this voxel\n        signal_function_temp = signal_function[:, idx_counter]\n\n        # Multiply the voxel value by the function timecourse\n        signal[x, y, z, :] = volume_signal[x, y, z] * signal_function_temp\n\n    return signal", "language": "python", "code": "def apply_signal(signal_function,\n                 volume_signal,\n                 ):\n    \"\"\"Combine the signal volume with its timecourse\n\n    Apply the convolution of the HRF and stimulus time course to the\n    volume.\n\n    Parameters\n    ----------\n\n    signal_function : timepoint by timecourse array, float\n        The timecourse of the signal over time. If there is only one column\n        then the same timecourse is applied to all non-zero voxels in\n        volume_signal. If there is more than one column then each column is\n        paired with a non-zero voxel in the volume_signal (a 3d numpy array\n        generated in generate_signal).\n\n    volume_signal : multi dimensional array, float\n        The volume containing the signal to be convolved with the same\n        dimensions as the output volume. The elements in volume_signal\n        indicate how strong each signal in signal_function are modulated by\n        in the output volume\n\n\n    Returns\n    ----------\n    signal : multidimensional array, float\n        The convolved signal volume with the same 3d as volume signal and\n        the same 4th dimension as signal_function\n\n    \"\"\"\n\n    # How many timecourses are there within the signal_function\n    timepoints = signal_function.shape[0]\n    timecourses = signal_function.shape[1]\n\n    # Preset volume\n    signal = np.zeros([volume_signal.shape[0], volume_signal.shape[\n        1], volume_signal.shape[2], timepoints])\n\n    # Find all the non-zero voxels in the brain\n    idxs = np.where(volume_signal != 0)\n    if timecourses == 1:\n        # If there is only one time course supplied then duplicate it for\n        # every voxel\n        signal_function = np.matlib.repmat(signal_function, 1, len(idxs[0]))\n\n    elif len(idxs[0]) != timecourses:\n        raise IndexError('The number of non-zero voxels in the volume and '\n                         'the number of timecourses does not match. Aborting')\n\n    # For each coordinate with a non zero voxel, fill in the timecourse for\n    # that voxel\n    for idx_counter in range(len(idxs[0])):\n        x = idxs[0][idx_counter]\n        y = idxs[1][idx_counter]\n        z = idxs[2][idx_counter]\n\n        # Pull out the function for this voxel\n        signal_function_temp = signal_function[:, idx_counter]\n\n        # Multiply the voxel value by the function timecourse\n        signal[x, y, z, :] = volume_signal[x, y, z] * signal_function_temp\n\n    return signal", "code_tokens": ["def", "apply_signal", "(", "signal_function", ",", "volume_signal", ",", ")", ":", "# How many timecourses are there within the signal_function", "timepoints", "=", "signal_function", ".", "shape", "[", "0", "]", "timecourses", "=", "signal_function", ".", "shape", "[", "1", "]", "# Preset volume", "signal", "=", "np", ".", "zeros", "(", "[", "volume_signal", ".", "shape", "[", "0", "]", ",", "volume_signal", ".", "shape", "[", "1", "]", ",", "volume_signal", ".", "shape", "[", "2", "]", ",", "timepoints", "]", ")", "# Find all the non-zero voxels in the brain", "idxs", "=", "np", ".", "where", "(", "volume_signal", "!=", "0", ")", "if", "timecourses", "==", "1", ":", "# If there is only one time course supplied then duplicate it for", "# every voxel", "signal_function", "=", "np", ".", "matlib", ".", "repmat", "(", "signal_function", ",", "1", ",", "len", "(", "idxs", "[", "0", "]", ")", ")", "elif", "len", "(", "idxs", "[", "0", "]", ")", "!=", "timecourses", ":", "raise", "IndexError", "(", "'The number of non-zero voxels in the volume and '", "'the number of timecourses does not match. Aborting'", ")", "# For each coordinate with a non zero voxel, fill in the timecourse for", "# that voxel", "for", "idx_counter", "in", "range", "(", "len", "(", "idxs", "[", "0", "]", ")", ")", ":", "x", "=", "idxs", "[", "0", "]", "[", "idx_counter", "]", "y", "=", "idxs", "[", "1", "]", "[", "idx_counter", "]", "z", "=", "idxs", "[", "2", "]", "[", "idx_counter", "]", "# Pull out the function for this voxel", "signal_function_temp", "=", "signal_function", "[", ":", ",", "idx_counter", "]", "# Multiply the voxel value by the function timecourse", "signal", "[", "x", ",", "y", ",", "z", ",", ":", "]", "=", "volume_signal", "[", "x", ",", "y", ",", "z", "]", "*", "signal_function_temp", "return", "signal"], "docstring": "Combine the signal volume with its timecourse\n\n    Apply the convolution of the HRF and stimulus time course to the\n    volume.\n\n    Parameters\n    ----------\n\n    signal_function : timepoint by timecourse array, float\n        The timecourse of the signal over time. If there is only one column\n        then the same timecourse is applied to all non-zero voxels in\n        volume_signal. If there is more than one column then each column is\n        paired with a non-zero voxel in the volume_signal (a 3d numpy array\n        generated in generate_signal).\n\n    volume_signal : multi dimensional array, float\n        The volume containing the signal to be convolved with the same\n        dimensions as the output volume. The elements in volume_signal\n        indicate how strong each signal in signal_function are modulated by\n        in the output volume\n\n\n    Returns\n    ----------\n    signal : multidimensional array, float\n        The convolved signal volume with the same 3d as volume signal and\n        the same 4th dimension as signal_function", "docstring_tokens": ["Combine", "the", "signal", "volume", "with", "its", "timecourse"], "sha": "408f12dec2ff56559a26873a848a09e4c8facfeb", "url": "https://github.com/brainiak/brainiak/blob/408f12dec2ff56559a26873a848a09e4c8facfeb/brainiak/utils/fmrisim.py#L896-L961", "partition": "train", "idx": 204421}
{"repo": "slarse/clanimtk", "path": "clanimtk/core.py", "func_name": "Animation.reset", "original_string": "def reset(self):\n        \"\"\"Reset the current animation generator.\"\"\"\n        animation_gen = self._frame_function(*self._animation_args,\n                                             **self._animation_kwargs)\n        self._current_generator = itertools.cycle(\n            util.concatechain(animation_gen, self._back_up_generator))", "language": "python", "code": "def reset(self):\n        \"\"\"Reset the current animation generator.\"\"\"\n        animation_gen = self._frame_function(*self._animation_args,\n                                             **self._animation_kwargs)\n        self._current_generator = itertools.cycle(\n            util.concatechain(animation_gen, self._back_up_generator))", "code_tokens": ["def", "reset", "(", "self", ")", ":", "animation_gen", "=", "self", ".", "_frame_function", "(", "*", "self", ".", "_animation_args", ",", "*", "*", "self", ".", "_animation_kwargs", ")", "self", ".", "_current_generator", "=", "itertools", ".", "cycle", "(", "util", ".", "concatechain", "(", "animation_gen", ",", "self", ".", "_back_up_generator", ")", ")"], "docstring": "Reset the current animation generator.", "docstring_tokens": ["Reset", "the", "current", "animation", "generator", "."], "sha": "cb93d2e914c3ecc4e0007745ff4d546318cf3902", "url": "https://github.com/slarse/clanimtk/blob/cb93d2e914c3ecc4e0007745ff4d546318cf3902/clanimtk/core.py#L212-L217", "partition": "train", "idx": 96344}
{"repo": "gem/oq-engine", "path": "openquake/hazardlib/gsim/lin_lee_2008.py", "func_name": "LinLee2008SInter._compute_mean", "original_string": "def _compute_mean(self, C, mag, rhypo, hypo_depth, mean, idx):\n        \"\"\"\n        Compute mean value according to equations 10 and 11 page 226.\n        \"\"\"\n        mean[idx] = (C['C1'] + C['C2'] * mag + C['C3'] * np.log(rhypo[idx] +\n                     C['C4'] * np.exp(C['C5'] * mag)) + C['C6'] * hypo_depth)", "language": "python", "code": "def _compute_mean(self, C, mag, rhypo, hypo_depth, mean, idx):\n        \"\"\"\n        Compute mean value according to equations 10 and 11 page 226.\n        \"\"\"\n        mean[idx] = (C['C1'] + C['C2'] * mag + C['C3'] * np.log(rhypo[idx] +\n                     C['C4'] * np.exp(C['C5'] * mag)) + C['C6'] * hypo_depth)", "code_tokens": ["def", "_compute_mean", "(", "self", ",", "C", ",", "mag", ",", "rhypo", ",", "hypo_depth", ",", "mean", ",", "idx", ")", ":", "mean", "[", "idx", "]", "=", "(", "C", "[", "'C1'", "]", "+", "C", "[", "'C2'", "]", "*", "mag", "+", "C", "[", "'C3'", "]", "*", "np", ".", "log", "(", "rhypo", "[", "idx", "]", "+", "C", "[", "'C4'", "]", "*", "np", ".", "exp", "(", "C", "[", "'C5'", "]", "*", "mag", ")", ")", "+", "C", "[", "'C6'", "]", "*", "hypo_depth", ")"], "docstring": "Compute mean value according to equations 10 and 11 page 226.", "docstring_tokens": ["Compute", "mean", "value", "according", "to", "equations", "10", "and", "11", "page", "226", "."], "sha": "8294553a0b8aba33fd96437a35065d03547d0040", "url": "https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hazardlib/gsim/lin_lee_2008.py#L102-L107", "partition": "train", "idx": 213963}
{"repo": "dswah/pyGAM", "path": "pygam/utils.py", "func_name": "check_array", "original_string": "def check_array(array, force_2d=False, n_feats=None, ndim=None,\n                min_samples=1, name='Input data', verbose=True):\n    \"\"\"\n    tool to perform basic data validation.\n    called by check_X and check_y.\n\n    ensures that data:\n    - is ndim dimensional\n    - contains float-compatible data-types\n    - has at least min_samples\n    - has n_feats\n    - is finite\n\n    Parameters\n    ----------\n    array : array-like\n    force_2d : boolean, default: False\n        whether to force a 2d array. Setting to True forces ndim = 2\n    n_feats : int, default: None\n              represents number of features that the array should have.\n              not enforced if n_feats is None.\n    ndim : int default: None\n        number of dimensions expected in the array\n    min_samples : int, default: 1\n    name : str, default: 'Input data'\n        name to use when referring to the array\n    verbose : bool, default: True\n        whether to print warnings\n\n    Returns\n    -------\n    array : validated array\n    \"\"\"\n    # make array\n    if force_2d:\n        array = make_2d(array, verbose=verbose)\n        ndim = 2\n    else:\n        array = np.array(array)\n\n    # cast to float\n    dtype = array.dtype\n    if dtype.kind not in ['i', 'f']:\n        try:\n            array = array.astype('float')\n        except ValueError as e:\n            raise ValueError('{} must be type int or float, '\\\n                             'but found type: {}\\n'\\\n                             'Try transforming data with a LabelEncoder first.'\\\n                             .format(name, dtype.type))\n\n    # check finite\n    if not(np.isfinite(array).all()):\n        raise ValueError('{} must not contain Inf nor NaN'.format(name))\n\n    # check ndim\n    if ndim is not None:\n        if array.ndim != ndim:\n            raise ValueError('{} must have {} dimensions. '\\\n                             'found shape {}'.format(name, ndim, array.shape))\n\n    # check n_feats\n    if n_feats is not None:\n        m = array.shape[1]\n        if m != n_feats:\n           raise ValueError('{} must have {} features, '\\\n                            'but found {}'.format(name, n_feats, m))\n\n    # minimum samples\n    n = array.shape[0]\n    if n < min_samples:\n        raise ValueError('{} should have at least {} samples, '\\\n                         'but found {}'.format(name, min_samples, n))\n\n    return array", "language": "python", "code": "def check_array(array, force_2d=False, n_feats=None, ndim=None,\n                min_samples=1, name='Input data', verbose=True):\n    \"\"\"\n    tool to perform basic data validation.\n    called by check_X and check_y.\n\n    ensures that data:\n    - is ndim dimensional\n    - contains float-compatible data-types\n    - has at least min_samples\n    - has n_feats\n    - is finite\n\n    Parameters\n    ----------\n    array : array-like\n    force_2d : boolean, default: False\n        whether to force a 2d array. Setting to True forces ndim = 2\n    n_feats : int, default: None\n              represents number of features that the array should have.\n              not enforced if n_feats is None.\n    ndim : int default: None\n        number of dimensions expected in the array\n    min_samples : int, default: 1\n    name : str, default: 'Input data'\n        name to use when referring to the array\n    verbose : bool, default: True\n        whether to print warnings\n\n    Returns\n    -------\n    array : validated array\n    \"\"\"\n    # make array\n    if force_2d:\n        array = make_2d(array, verbose=verbose)\n        ndim = 2\n    else:\n        array = np.array(array)\n\n    # cast to float\n    dtype = array.dtype\n    if dtype.kind not in ['i', 'f']:\n        try:\n            array = array.astype('float')\n        except ValueError as e:\n            raise ValueError('{} must be type int or float, '\\\n                             'but found type: {}\\n'\\\n                             'Try transforming data with a LabelEncoder first.'\\\n                             .format(name, dtype.type))\n\n    # check finite\n    if not(np.isfinite(array).all()):\n        raise ValueError('{} must not contain Inf nor NaN'.format(name))\n\n    # check ndim\n    if ndim is not None:\n        if array.ndim != ndim:\n            raise ValueError('{} must have {} dimensions. '\\\n                             'found shape {}'.format(name, ndim, array.shape))\n\n    # check n_feats\n    if n_feats is not None:\n        m = array.shape[1]\n        if m != n_feats:\n           raise ValueError('{} must have {} features, '\\\n                            'but found {}'.format(name, n_feats, m))\n\n    # minimum samples\n    n = array.shape[0]\n    if n < min_samples:\n        raise ValueError('{} should have at least {} samples, '\\\n                         'but found {}'.format(name, min_samples, n))\n\n    return array", "code_tokens": ["def", "check_array", "(", "array", ",", "force_2d", "=", "False", ",", "n_feats", "=", "None", ",", "ndim", "=", "None", ",", "min_samples", "=", "1", ",", "name", "=", "'Input data'", ",", "verbose", "=", "True", ")", ":", "# make array", "if", "force_2d", ":", "array", "=", "make_2d", "(", "array", ",", "verbose", "=", "verbose", ")", "ndim", "=", "2", "else", ":", "array", "=", "np", ".", "array", "(", "array", ")", "# cast to float", "dtype", "=", "array", ".", "dtype", "if", "dtype", ".", "kind", "not", "in", "[", "'i'", ",", "'f'", "]", ":", "try", ":", "array", "=", "array", ".", "astype", "(", "'float'", ")", "except", "ValueError", "as", "e", ":", "raise", "ValueError", "(", "'{} must be type int or float, '", "'but found type: {}\\n'", "'Try transforming data with a LabelEncoder first.'", ".", "format", "(", "name", ",", "dtype", ".", "type", ")", ")", "# check finite", "if", "not", "(", "np", ".", "isfinite", "(", "array", ")", ".", "all", "(", ")", ")", ":", "raise", "ValueError", "(", "'{} must not contain Inf nor NaN'", ".", "format", "(", "name", ")", ")", "# check ndim", "if", "ndim", "is", "not", "None", ":", "if", "array", ".", "ndim", "!=", "ndim", ":", "raise", "ValueError", "(", "'{} must have {} dimensions. '", "'found shape {}'", ".", "format", "(", "name", ",", "ndim", ",", "array", ".", "shape", ")", ")", "# check n_feats", "if", "n_feats", "is", "not", "None", ":", "m", "=", "array", ".", "shape", "[", "1", "]", "if", "m", "!=", "n_feats", ":", "raise", "ValueError", "(", "'{} must have {} features, '", "'but found {}'", ".", "format", "(", "name", ",", "n_feats", ",", "m", ")", ")", "# minimum samples", "n", "=", "array", ".", "shape", "[", "0", "]", "if", "n", "<", "min_samples", ":", "raise", "ValueError", "(", "'{} should have at least {} samples, '", "'but found {}'", ".", "format", "(", "name", ",", "min_samples", ",", "n", ")", ")", "return", "array"], "docstring": "tool to perform basic data validation.\n    called by check_X and check_y.\n\n    ensures that data:\n    - is ndim dimensional\n    - contains float-compatible data-types\n    - has at least min_samples\n    - has n_feats\n    - is finite\n\n    Parameters\n    ----------\n    array : array-like\n    force_2d : boolean, default: False\n        whether to force a 2d array. Setting to True forces ndim = 2\n    n_feats : int, default: None\n              represents number of features that the array should have.\n              not enforced if n_feats is None.\n    ndim : int default: None\n        number of dimensions expected in the array\n    min_samples : int, default: 1\n    name : str, default: 'Input data'\n        name to use when referring to the array\n    verbose : bool, default: True\n        whether to print warnings\n\n    Returns\n    -------\n    array : validated array", "docstring_tokens": ["tool", "to", "perform", "basic", "data", "validation", ".", "called", "by", "check_X", "and", "check_y", "."], "sha": "b3e5c3cd580f0a3ad69f9372861624f67760c325", "url": "https://github.com/dswah/pyGAM/blob/b3e5c3cd580f0a3ad69f9372861624f67760c325/pygam/utils.py#L118-L192", "partition": "train", "idx": 220439}
{"repo": "72squared/redpipe", "path": "redpipe/keyspaces.py", "func_name": "String.append", "original_string": "def append(self, name, value):\n        \"\"\"\n        Appends the string ``value`` to the value at ``key``. If ``key``\n        doesn't already exist, create it with a value of ``value``.\n        Returns the new length of the value at ``key``.\n\n        :param name: str     the name of the redis key\n        :param value: str\n        :return: Future()\n        \"\"\"\n        with self.pipe as pipe:\n            return pipe.append(self.redis_key(name),\n                               self.valueparse.encode(value))", "language": "python", "code": "def append(self, name, value):\n        \"\"\"\n        Appends the string ``value`` to the value at ``key``. If ``key``\n        doesn't already exist, create it with a value of ``value``.\n        Returns the new length of the value at ``key``.\n\n        :param name: str     the name of the redis key\n        :param value: str\n        :return: Future()\n        \"\"\"\n        with self.pipe as pipe:\n            return pipe.append(self.redis_key(name),\n                               self.valueparse.encode(value))", "code_tokens": ["def", "append", "(", "self", ",", "name", ",", "value", ")", ":", "with", "self", ".", "pipe", "as", "pipe", ":", "return", "pipe", ".", "append", "(", "self", ".", "redis_key", "(", "name", ")", ",", "self", ".", "valueparse", ".", "encode", "(", "value", ")", ")"], "docstring": "Appends the string ``value`` to the value at ``key``. If ``key``\n        doesn't already exist, create it with a value of ``value``.\n        Returns the new length of the value at ``key``.\n\n        :param name: str     the name of the redis key\n        :param value: str\n        :return: Future()", "docstring_tokens": ["Appends", "the", "string", "value", "to", "the", "value", "at", "key", ".", "If", "key", "doesn", "t", "already", "exist", "create", "it", "with", "a", "value", "of", "value", ".", "Returns", "the", "new", "length", "of", "the", "value", "at", "key", "."], "sha": "e6ee518bc9f3e2fee323c8c53d08997799bd9b1b", "url": "https://github.com/72squared/redpipe/blob/e6ee518bc9f3e2fee323c8c53d08997799bd9b1b/redpipe/keyspaces.py#L553-L565", "partition": "train", "idx": 73471}
{"repo": "python-diamond/Diamond", "path": "src/diamond/handler/Handler.py", "func_name": "Handler._reset_errors", "original_string": "def _reset_errors(self, msg=None):\n        \"\"\"\n        Resets the logging throttle cache, so the next error is emitted\n        regardless of the value in `self.server_error_interval`\n\n        :param msg: if present, only this key is reset. Otherwise, the whole\n            cache is cleaned.\n        \"\"\"\n        if msg is not None and msg in self._errors:\n            del self._errors[msg]\n        else:\n            self._errors = {}", "language": "python", "code": "def _reset_errors(self, msg=None):\n        \"\"\"\n        Resets the logging throttle cache, so the next error is emitted\n        regardless of the value in `self.server_error_interval`\n\n        :param msg: if present, only this key is reset. Otherwise, the whole\n            cache is cleaned.\n        \"\"\"\n        if msg is not None and msg in self._errors:\n            del self._errors[msg]\n        else:\n            self._errors = {}", "code_tokens": ["def", "_reset_errors", "(", "self", ",", "msg", "=", "None", ")", ":", "if", "msg", "is", "not", "None", "and", "msg", "in", "self", ".", "_errors", ":", "del", "self", ".", "_errors", "[", "msg", "]", "else", ":", "self", ".", "_errors", "=", "{", "}"], "docstring": "Resets the logging throttle cache, so the next error is emitted\n        regardless of the value in `self.server_error_interval`\n\n        :param msg: if present, only this key is reset. Otherwise, the whole\n            cache is cleaned.", "docstring_tokens": ["Resets", "the", "logging", "throttle", "cache", "so", "the", "next", "error", "is", "emitted", "regardless", "of", "the", "value", "in", "self", ".", "server_error_interval"], "sha": "0f3eb04327d6d3ed5e53a9967d6c9d2c09714a47", "url": "https://github.com/python-diamond/Diamond/blob/0f3eb04327d6d3ed5e53a9967d6c9d2c09714a47/src/diamond/handler/Handler.py#L140-L151", "partition": "train", "idx": 217208}
{"repo": "GoogleCloudPlatform/appengine-gcs-client", "path": "python/src/cloudstorage/errors.py", "func_name": "check_status", "original_string": "def check_status(status, expected, path, headers=None,\n                 resp_headers=None, body=None, extras=None):\n  \"\"\"Check HTTP response status is expected.\n\n  Args:\n    status: HTTP response status. int.\n    expected: a list of expected statuses. A list of ints.\n    path: filename or a path prefix.\n    headers: HTTP request headers.\n    resp_headers: HTTP response headers.\n    body: HTTP response body.\n    extras: extra info to be logged verbatim if error occurs.\n\n  Raises:\n    AuthorizationError: if authorization failed.\n    NotFoundError: if an object that's expected to exist doesn't.\n    TimeoutError: if HTTP request timed out.\n    ServerError: if server experienced some errors.\n    FatalError: if any other unexpected errors occurred.\n  \"\"\"\n  if status in expected:\n    return\n\n  msg = ('Expect status %r from Google Storage. But got status %d.\\n'\n         'Path: %r.\\n'\n         'Request headers: %r.\\n'\n         'Response headers: %r.\\n'\n         'Body: %r.\\n'\n         'Extra info: %r.\\n' %\n         (expected, status, path, headers, resp_headers, body, extras))\n\n  if status == httplib.UNAUTHORIZED:\n    raise AuthorizationError(msg)\n  elif status == httplib.FORBIDDEN:\n    raise ForbiddenError(msg)\n  elif status == httplib.NOT_FOUND:\n    raise NotFoundError(msg)\n  elif status == httplib.REQUEST_TIMEOUT:\n    raise TimeoutError(msg)\n  elif status == httplib.REQUESTED_RANGE_NOT_SATISFIABLE:\n    raise InvalidRange(msg)\n  elif (status == httplib.OK and 308 in expected and\n        httplib.OK not in expected):\n    raise FileClosedError(msg)\n  elif status >= 500:\n    raise ServerError(msg)\n  else:\n    raise FatalError(msg)", "language": "python", "code": "def check_status(status, expected, path, headers=None,\n                 resp_headers=None, body=None, extras=None):\n  \"\"\"Check HTTP response status is expected.\n\n  Args:\n    status: HTTP response status. int.\n    expected: a list of expected statuses. A list of ints.\n    path: filename or a path prefix.\n    headers: HTTP request headers.\n    resp_headers: HTTP response headers.\n    body: HTTP response body.\n    extras: extra info to be logged verbatim if error occurs.\n\n  Raises:\n    AuthorizationError: if authorization failed.\n    NotFoundError: if an object that's expected to exist doesn't.\n    TimeoutError: if HTTP request timed out.\n    ServerError: if server experienced some errors.\n    FatalError: if any other unexpected errors occurred.\n  \"\"\"\n  if status in expected:\n    return\n\n  msg = ('Expect status %r from Google Storage. But got status %d.\\n'\n         'Path: %r.\\n'\n         'Request headers: %r.\\n'\n         'Response headers: %r.\\n'\n         'Body: %r.\\n'\n         'Extra info: %r.\\n' %\n         (expected, status, path, headers, resp_headers, body, extras))\n\n  if status == httplib.UNAUTHORIZED:\n    raise AuthorizationError(msg)\n  elif status == httplib.FORBIDDEN:\n    raise ForbiddenError(msg)\n  elif status == httplib.NOT_FOUND:\n    raise NotFoundError(msg)\n  elif status == httplib.REQUEST_TIMEOUT:\n    raise TimeoutError(msg)\n  elif status == httplib.REQUESTED_RANGE_NOT_SATISFIABLE:\n    raise InvalidRange(msg)\n  elif (status == httplib.OK and 308 in expected and\n        httplib.OK not in expected):\n    raise FileClosedError(msg)\n  elif status >= 500:\n    raise ServerError(msg)\n  else:\n    raise FatalError(msg)", "code_tokens": ["def", "check_status", "(", "status", ",", "expected", ",", "path", ",", "headers", "=", "None", ",", "resp_headers", "=", "None", ",", "body", "=", "None", ",", "extras", "=", "None", ")", ":", "if", "status", "in", "expected", ":", "return", "msg", "=", "(", "'Expect status %r from Google Storage. But got status %d.\\n'", "'Path: %r.\\n'", "'Request headers: %r.\\n'", "'Response headers: %r.\\n'", "'Body: %r.\\n'", "'Extra info: %r.\\n'", "%", "(", "expected", ",", "status", ",", "path", ",", "headers", ",", "resp_headers", ",", "body", ",", "extras", ")", ")", "if", "status", "==", "httplib", ".", "UNAUTHORIZED", ":", "raise", "AuthorizationError", "(", "msg", ")", "elif", "status", "==", "httplib", ".", "FORBIDDEN", ":", "raise", "ForbiddenError", "(", "msg", ")", "elif", "status", "==", "httplib", ".", "NOT_FOUND", ":", "raise", "NotFoundError", "(", "msg", ")", "elif", "status", "==", "httplib", ".", "REQUEST_TIMEOUT", ":", "raise", "TimeoutError", "(", "msg", ")", "elif", "status", "==", "httplib", ".", "REQUESTED_RANGE_NOT_SATISFIABLE", ":", "raise", "InvalidRange", "(", "msg", ")", "elif", "(", "status", "==", "httplib", ".", "OK", "and", "308", "in", "expected", "and", "httplib", ".", "OK", "not", "in", "expected", ")", ":", "raise", "FileClosedError", "(", "msg", ")", "elif", "status", ">=", "500", ":", "raise", "ServerError", "(", "msg", ")", "else", ":", "raise", "FatalError", "(", "msg", ")"], "docstring": "Check HTTP response status is expected.\n\n  Args:\n    status: HTTP response status. int.\n    expected: a list of expected statuses. A list of ints.\n    path: filename or a path prefix.\n    headers: HTTP request headers.\n    resp_headers: HTTP response headers.\n    body: HTTP response body.\n    extras: extra info to be logged verbatim if error occurs.\n\n  Raises:\n    AuthorizationError: if authorization failed.\n    NotFoundError: if an object that's expected to exist doesn't.\n    TimeoutError: if HTTP request timed out.\n    ServerError: if server experienced some errors.\n    FatalError: if any other unexpected errors occurred.", "docstring_tokens": ["Check", "HTTP", "response", "status", "is", "expected", "."], "sha": "d11078331ecd915d753c886e96a80133599f3f98", "url": "https://github.com/GoogleCloudPlatform/appengine-gcs-client/blob/d11078331ecd915d753c886e96a80133599f3f98/python/src/cloudstorage/errors.py#L96-L143", "partition": "train", "idx": 212241}
{"repo": "Kozea/wdb", "path": "client/wdb/__init__.py", "func_name": "Wdb.set_trace", "original_string": "def set_trace(self, frame=None, break_=True):\n        \"\"\"Break at current state\"\"\"\n        # We are already tracing, do nothing\n        trace_log.info(\n            'Setting trace %s (stepping %s) (current_trace: %s)' % (\n                pretty_frame(frame or sys._getframe().f_back), self.stepping,\n                sys.gettrace()\n            )\n        )\n        if self.stepping or self.closed:\n            return\n        self.reset()\n        trace = (\n            self.trace_dispatch\n            if trace_log.level >= 30 else self.trace_debug_dispatch\n        )\n        trace_frame = frame = frame or sys._getframe().f_back\n        while frame:\n            frame.f_trace = trace\n            frame = frame.f_back\n        self.state = Step(trace_frame) if break_ else Running(trace_frame)\n        sys.settrace(trace)", "language": "python", "code": "def set_trace(self, frame=None, break_=True):\n        \"\"\"Break at current state\"\"\"\n        # We are already tracing, do nothing\n        trace_log.info(\n            'Setting trace %s (stepping %s) (current_trace: %s)' % (\n                pretty_frame(frame or sys._getframe().f_back), self.stepping,\n                sys.gettrace()\n            )\n        )\n        if self.stepping or self.closed:\n            return\n        self.reset()\n        trace = (\n            self.trace_dispatch\n            if trace_log.level >= 30 else self.trace_debug_dispatch\n        )\n        trace_frame = frame = frame or sys._getframe().f_back\n        while frame:\n            frame.f_trace = trace\n            frame = frame.f_back\n        self.state = Step(trace_frame) if break_ else Running(trace_frame)\n        sys.settrace(trace)", "code_tokens": ["def", "set_trace", "(", "self", ",", "frame", "=", "None", ",", "break_", "=", "True", ")", ":", "# We are already tracing, do nothing", "trace_log", ".", "info", "(", "'Setting trace %s (stepping %s) (current_trace: %s)'", "%", "(", "pretty_frame", "(", "frame", "or", "sys", ".", "_getframe", "(", ")", ".", "f_back", ")", ",", "self", ".", "stepping", ",", "sys", ".", "gettrace", "(", ")", ")", ")", "if", "self", ".", "stepping", "or", "self", ".", "closed", ":", "return", "self", ".", "reset", "(", ")", "trace", "=", "(", "self", ".", "trace_dispatch", "if", "trace_log", ".", "level", ">=", "30", "else", "self", ".", "trace_debug_dispatch", ")", "trace_frame", "=", "frame", "=", "frame", "or", "sys", ".", "_getframe", "(", ")", ".", "f_back", "while", "frame", ":", "frame", ".", "f_trace", "=", "trace", "frame", "=", "frame", ".", "f_back", "self", ".", "state", "=", "Step", "(", "trace_frame", ")", "if", "break_", "else", "Running", "(", "trace_frame", ")", "sys", ".", "settrace", "(", "trace", ")"], "docstring": "Break at current state", "docstring_tokens": ["Break", "at", "current", "state"], "sha": "6af7901b02e866d76f8b0a697a8c078e5b70d1aa", "url": "https://github.com/Kozea/wdb/blob/6af7901b02e866d76f8b0a697a8c078e5b70d1aa/client/wdb/__init__.py#L389-L410", "partition": "train", "idx": 207179}
{"repo": "datascopeanalytics/traces", "path": "traces/timeseries.py", "func_name": "TimeSeries.moving_average", "original_string": "def moving_average(self, sampling_period,\n                       window_size=None,\n                       start=None, end=None,\n                       placement='center',\n                       pandas=False):\n        \"\"\"Averaging over regular intervals\n        \"\"\"\n        start, end, mask = self._check_boundaries(start, end)\n\n        # default to sampling_period if not given\n        if window_size is None:\n            window_size = sampling_period\n\n        sampling_period = \\\n            self._check_regularization(start, end, sampling_period)\n\n        # convert to datetime if the times are datetimes\n        full_window = window_size * 1.  # convert to float if int or do nothing\n        half_window = full_window / 2.  # divide by 2\n        if (isinstance(start, datetime.datetime) and\n                not isinstance(full_window, datetime.timedelta)):\n            half_window = datetime.timedelta(seconds=half_window)\n            full_window = datetime.timedelta(seconds=full_window)\n\n        result = []\n        current_time = start\n        while current_time <= end:\n\n            if placement == 'center':\n                window_start = current_time - half_window\n                window_end = current_time + half_window\n            elif placement == 'left':\n                window_start = current_time\n                window_end = current_time + full_window\n            elif placement == 'right':\n                window_start = current_time - full_window\n                window_end = current_time\n            else:\n                msg = 'unknown placement \"{}\"'.format(placement)\n                raise ValueError(msg)\n\n            # calculate mean over window and add (t, v) tuple to list\n            try:\n                mean = self.mean(window_start, window_end)\n            except TypeError as e:\n                if 'NoneType' in str(e):\n                    mean = None\n                else:\n                    raise e\n            result.append((current_time, mean))\n\n            current_time += sampling_period\n\n        # convert to pandas Series if pandas=True\n        if pandas:\n\n            try:\n                import pandas as pd\n            except ImportError:\n                msg = \"can't have pandas=True if pandas is not installed\"\n                raise ImportError(msg)\n\n            result = pd.Series(\n                [v for t, v in result],\n                index=[t for t, v in result],\n            )\n\n        return result", "language": "python", "code": "def moving_average(self, sampling_period,\n                       window_size=None,\n                       start=None, end=None,\n                       placement='center',\n                       pandas=False):\n        \"\"\"Averaging over regular intervals\n        \"\"\"\n        start, end, mask = self._check_boundaries(start, end)\n\n        # default to sampling_period if not given\n        if window_size is None:\n            window_size = sampling_period\n\n        sampling_period = \\\n            self._check_regularization(start, end, sampling_period)\n\n        # convert to datetime if the times are datetimes\n        full_window = window_size * 1.  # convert to float if int or do nothing\n        half_window = full_window / 2.  # divide by 2\n        if (isinstance(start, datetime.datetime) and\n                not isinstance(full_window, datetime.timedelta)):\n            half_window = datetime.timedelta(seconds=half_window)\n            full_window = datetime.timedelta(seconds=full_window)\n\n        result = []\n        current_time = start\n        while current_time <= end:\n\n            if placement == 'center':\n                window_start = current_time - half_window\n                window_end = current_time + half_window\n            elif placement == 'left':\n                window_start = current_time\n                window_end = current_time + full_window\n            elif placement == 'right':\n                window_start = current_time - full_window\n                window_end = current_time\n            else:\n                msg = 'unknown placement \"{}\"'.format(placement)\n                raise ValueError(msg)\n\n            # calculate mean over window and add (t, v) tuple to list\n            try:\n                mean = self.mean(window_start, window_end)\n            except TypeError as e:\n                if 'NoneType' in str(e):\n                    mean = None\n                else:\n                    raise e\n            result.append((current_time, mean))\n\n            current_time += sampling_period\n\n        # convert to pandas Series if pandas=True\n        if pandas:\n\n            try:\n                import pandas as pd\n            except ImportError:\n                msg = \"can't have pandas=True if pandas is not installed\"\n                raise ImportError(msg)\n\n            result = pd.Series(\n                [v for t, v in result],\n                index=[t for t, v in result],\n            )\n\n        return result", "code_tokens": ["def", "moving_average", "(", "self", ",", "sampling_period", ",", "window_size", "=", "None", ",", "start", "=", "None", ",", "end", "=", "None", ",", "placement", "=", "'center'", ",", "pandas", "=", "False", ")", ":", "start", ",", "end", ",", "mask", "=", "self", ".", "_check_boundaries", "(", "start", ",", "end", ")", "# default to sampling_period if not given", "if", "window_size", "is", "None", ":", "window_size", "=", "sampling_period", "sampling_period", "=", "self", ".", "_check_regularization", "(", "start", ",", "end", ",", "sampling_period", ")", "# convert to datetime if the times are datetimes", "full_window", "=", "window_size", "*", "1.", "# convert to float if int or do nothing", "half_window", "=", "full_window", "/", "2.", "# divide by 2", "if", "(", "isinstance", "(", "start", ",", "datetime", ".", "datetime", ")", "and", "not", "isinstance", "(", "full_window", ",", "datetime", ".", "timedelta", ")", ")", ":", "half_window", "=", "datetime", ".", "timedelta", "(", "seconds", "=", "half_window", ")", "full_window", "=", "datetime", ".", "timedelta", "(", "seconds", "=", "full_window", ")", "result", "=", "[", "]", "current_time", "=", "start", "while", "current_time", "<=", "end", ":", "if", "placement", "==", "'center'", ":", "window_start", "=", "current_time", "-", "half_window", "window_end", "=", "current_time", "+", "half_window", "elif", "placement", "==", "'left'", ":", "window_start", "=", "current_time", "window_end", "=", "current_time", "+", "full_window", "elif", "placement", "==", "'right'", ":", "window_start", "=", "current_time", "-", "full_window", "window_end", "=", "current_time", "else", ":", "msg", "=", "'unknown placement \"{}\"'", ".", "format", "(", "placement", ")", "raise", "ValueError", "(", "msg", ")", "# calculate mean over window and add (t, v) tuple to list", "try", ":", "mean", "=", "self", ".", "mean", "(", "window_start", ",", "window_end", ")", "except", "TypeError", "as", "e", ":", "if", "'NoneType'", "in", "str", "(", "e", ")", ":", "mean", "=", "None", "else", ":", "raise", "e", "result", ".", "append", "(", "(", "current_time", ",", "mean", ")", ")", "current_time", "+=", "sampling_period", "# convert to pandas Series if pandas=True", "if", "pandas", ":", "try", ":", "import", "pandas", "as", "pd", "except", "ImportError", ":", "msg", "=", "\"can't have pandas=True if pandas is not installed\"", "raise", "ImportError", "(", "msg", ")", "result", "=", "pd", ".", "Series", "(", "[", "v", "for", "t", ",", "v", "in", "result", "]", ",", "index", "=", "[", "t", "for", "t", ",", "v", "in", "result", "]", ",", ")", "return", "result"], "docstring": "Averaging over regular intervals", "docstring_tokens": ["Averaging", "over", "regular", "intervals"], "sha": "420611151a05fea88a07bc5200fefffdc37cc95b", "url": "https://github.com/datascopeanalytics/traces/blob/420611151a05fea88a07bc5200fefffdc37cc95b/traces/timeseries.py#L435-L502", "partition": "train", "idx": 205094}
{"repo": "log2timeline/plaso", "path": "plaso/multi_processing/engine.py", "func_name": "MultiProcessEngine._TerminateProcess", "original_string": "def _TerminateProcess(self, process):\n    \"\"\"Terminate a process.\n\n    Args:\n      process (MultiProcessBaseProcess): process to terminate.\n    \"\"\"\n    pid = process.pid\n    logger.warning('Terminating process: (PID: {0:d}).'.format(pid))\n    process.terminate()\n\n    # Wait for the process to exit.\n    process.join(timeout=self._PROCESS_JOIN_TIMEOUT)\n\n    if process.is_alive():\n      logger.warning('Killing process: (PID: {0:d}).'.format(pid))\n      self._KillProcess(pid)", "language": "python", "code": "def _TerminateProcess(self, process):\n    \"\"\"Terminate a process.\n\n    Args:\n      process (MultiProcessBaseProcess): process to terminate.\n    \"\"\"\n    pid = process.pid\n    logger.warning('Terminating process: (PID: {0:d}).'.format(pid))\n    process.terminate()\n\n    # Wait for the process to exit.\n    process.join(timeout=self._PROCESS_JOIN_TIMEOUT)\n\n    if process.is_alive():\n      logger.warning('Killing process: (PID: {0:d}).'.format(pid))\n      self._KillProcess(pid)", "code_tokens": ["def", "_TerminateProcess", "(", "self", ",", "process", ")", ":", "pid", "=", "process", ".", "pid", "logger", ".", "warning", "(", "'Terminating process: (PID: {0:d}).'", ".", "format", "(", "pid", ")", ")", "process", ".", "terminate", "(", ")", "# Wait for the process to exit.", "process", ".", "join", "(", "timeout", "=", "self", ".", "_PROCESS_JOIN_TIMEOUT", ")", "if", "process", ".", "is_alive", "(", ")", ":", "logger", ".", "warning", "(", "'Killing process: (PID: {0:d}).'", ".", "format", "(", "pid", ")", ")", "self", ".", "_KillProcess", "(", "pid", ")"], "docstring": "Terminate a process.\n\n    Args:\n      process (MultiProcessBaseProcess): process to terminate.", "docstring_tokens": ["Terminate", "a", "process", "."], "sha": "9c564698d2da3ffbe23607a3c54c0582ea18a6cc", "url": "https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/multi_processing/engine.py#L417-L432", "partition": "train", "idx": 147206}
{"repo": "titusjan/argos", "path": "argos/inspector/pgplugins/pgctis.py", "func_name": "PgAxisRangeCti.setTargetRange", "original_string": "def setTargetRange(self, targetRange, padding=None):\n        \"\"\" Sets the range of the target.\n        \"\"\"\n        # viewBox.setRange doesn't accept an axis number :-(\n        if self.axisNumber == X_AXIS:\n            xRange, yRange = targetRange, None\n        else:\n            xRange, yRange = None, targetRange\n\n        # Do not set disableAutoRange to True in setRange; it triggers 'one last' auto range.\n        # This is why the viewBox' autorange must be False at construction.\n        self.viewBox.setRange(xRange = xRange, yRange=yRange, padding=padding,\n                              update=False, disableAutoRange=False)", "language": "python", "code": "def setTargetRange(self, targetRange, padding=None):\n        \"\"\" Sets the range of the target.\n        \"\"\"\n        # viewBox.setRange doesn't accept an axis number :-(\n        if self.axisNumber == X_AXIS:\n            xRange, yRange = targetRange, None\n        else:\n            xRange, yRange = None, targetRange\n\n        # Do not set disableAutoRange to True in setRange; it triggers 'one last' auto range.\n        # This is why the viewBox' autorange must be False at construction.\n        self.viewBox.setRange(xRange = xRange, yRange=yRange, padding=padding,\n                              update=False, disableAutoRange=False)", "code_tokens": ["def", "setTargetRange", "(", "self", ",", "targetRange", ",", "padding", "=", "None", ")", ":", "# viewBox.setRange doesn't accept an axis number :-(", "if", "self", ".", "axisNumber", "==", "X_AXIS", ":", "xRange", ",", "yRange", "=", "targetRange", ",", "None", "else", ":", "xRange", ",", "yRange", "=", "None", ",", "targetRange", "# Do not set disableAutoRange to True in setRange; it triggers 'one last' auto range.", "# This is why the viewBox' autorange must be False at construction.", "self", ".", "viewBox", ".", "setRange", "(", "xRange", "=", "xRange", ",", "yRange", "=", "yRange", ",", "padding", "=", "padding", ",", "update", "=", "False", ",", "disableAutoRange", "=", "False", ")"], "docstring": "Sets the range of the target.", "docstring_tokens": ["Sets", "the", "range", "of", "the", "target", "."], "sha": "20d0a3cae26c36ea789a5d219c02ca7df21279dd", "url": "https://github.com/titusjan/argos/blob/20d0a3cae26c36ea789a5d219c02ca7df21279dd/argos/inspector/pgplugins/pgctis.py#L428-L440", "partition": "train", "idx": 109479}
{"repo": "pip-services3-python/pip-services3-commons-python", "path": "pip_services3_commons/data/AnyValueArray.py", "func_name": "AnyValueArray.contains", "original_string": "def contains(self, value):\n        \"\"\"\n        Checks if this array contains a value.\n        The check uses direct comparison between elements and the specified value.\n\n        :param value: a value to be checked\n\n        :return: true if this array contains the value or false otherwise.\n        \"\"\"\n        str_value = StringConverter.to_nullable_string(value)\n\n        for element in self:\n            str_element = StringConverter.to_string(element)\n\n            if str_value == None and str_element == None:\n                return True\n            if str_value == None or str_element == None:\n                continue\n            \n            if str_value == str_element:\n                return True\n\n        return False", "language": "python", "code": "def contains(self, value):\n        \"\"\"\n        Checks if this array contains a value.\n        The check uses direct comparison between elements and the specified value.\n\n        :param value: a value to be checked\n\n        :return: true if this array contains the value or false otherwise.\n        \"\"\"\n        str_value = StringConverter.to_nullable_string(value)\n\n        for element in self:\n            str_element = StringConverter.to_string(element)\n\n            if str_value == None and str_element == None:\n                return True\n            if str_value == None or str_element == None:\n                continue\n            \n            if str_value == str_element:\n                return True\n\n        return False", "code_tokens": ["def", "contains", "(", "self", ",", "value", ")", ":", "str_value", "=", "StringConverter", ".", "to_nullable_string", "(", "value", ")", "for", "element", "in", "self", ":", "str_element", "=", "StringConverter", ".", "to_string", "(", "element", ")", "if", "str_value", "==", "None", "and", "str_element", "==", "None", ":", "return", "True", "if", "str_value", "==", "None", "or", "str_element", "==", "None", ":", "continue", "if", "str_value", "==", "str_element", ":", "return", "True", "return", "False"], "docstring": "Checks if this array contains a value.\n        The check uses direct comparison between elements and the specified value.\n\n        :param value: a value to be checked\n\n        :return: true if this array contains the value or false otherwise.", "docstring_tokens": ["Checks", "if", "this", "array", "contains", "a", "value", ".", "The", "check", "uses", "direct", "comparison", "between", "elements", "and", "the", "specified", "value", "."], "sha": "22cbbb3e91e49717f65c083d36147fdb07ba9e3b", "url": "https://github.com/pip-services3-python/pip-services3-commons-python/blob/22cbbb3e91e49717f65c083d36147fdb07ba9e3b/pip_services3_commons/data/AnyValueArray.py#L367-L389", "partition": "train", "idx": 90576}
{"repo": "omaraboumrad/mastool", "path": "mastool/helpers.py", "func_name": "target_names", "original_string": "def target_names(targets):\n    \"\"\"Retrieves the target names\"\"\"\n    names = []\n    for entry in targets:\n        if isinstance(entry, ast.Name):\n            names.append(entry.id)\n        elif isinstance(entry, ast.Tuple):\n            for element in entry.elts:\n                if isinstance(element, ast.Name):\n                    names.append(element.id)\n\n    return names", "language": "python", "code": "def target_names(targets):\n    \"\"\"Retrieves the target names\"\"\"\n    names = []\n    for entry in targets:\n        if isinstance(entry, ast.Name):\n            names.append(entry.id)\n        elif isinstance(entry, ast.Tuple):\n            for element in entry.elts:\n                if isinstance(element, ast.Name):\n                    names.append(element.id)\n\n    return names", "code_tokens": ["def", "target_names", "(", "targets", ")", ":", "names", "=", "[", "]", "for", "entry", "in", "targets", ":", "if", "isinstance", "(", "entry", ",", "ast", ".", "Name", ")", ":", "names", ".", "append", "(", "entry", ".", "id", ")", "elif", "isinstance", "(", "entry", ",", "ast", ".", "Tuple", ")", ":", "for", "element", "in", "entry", ".", "elts", ":", "if", "isinstance", "(", "element", ",", "ast", ".", "Name", ")", ":", "names", ".", "append", "(", "element", ".", "id", ")", "return", "names"], "docstring": "Retrieves the target names", "docstring_tokens": ["Retrieves", "the", "target", "names"], "sha": "0ec566de6717d03c6ec61affe5d1e9ff8d7e6ebd", "url": "https://github.com/omaraboumrad/mastool/blob/0ec566de6717d03c6ec61affe5d1e9ff8d7e6ebd/mastool/helpers.py#L35-L46", "partition": "train", "idx": 84706}
{"repo": "django-danceschool/django-danceschool", "path": "danceschool/private_lessons/views.py", "func_name": "PrivateLessonStudentInfoView.dispatch", "original_string": "def dispatch(self,request,*args,**kwargs):\n        '''\n        Handle the session data passed by the prior view.\n        '''\n\n        lessonSession = request.session.get(PRIVATELESSON_VALIDATION_STR,{})\n\n        try:\n            self.lesson = PrivateLessonEvent.objects.get(id=lessonSession.get('lesson'))\n        except (ValueError, ObjectDoesNotExist):\n            messages.error(request,_('Invalid lesson identifier passed to sign-up form.'))\n            return HttpResponseRedirect(reverse('bookPrivateLesson'))\n\n        expiry = parse_datetime(lessonSession.get('expiry',''),)\n        if not expiry or expiry < timezone.now():\n            messages.info(request,_('Your registration session has expired. Please try again.'))\n            return HttpResponseRedirect(reverse('bookPrivateLesson'))\n\n        self.payAtDoor = lessonSession.get('payAtDoor',False)\n        return super(PrivateLessonStudentInfoView,self).dispatch(request,*args,**kwargs)", "language": "python", "code": "def dispatch(self,request,*args,**kwargs):\n        '''\n        Handle the session data passed by the prior view.\n        '''\n\n        lessonSession = request.session.get(PRIVATELESSON_VALIDATION_STR,{})\n\n        try:\n            self.lesson = PrivateLessonEvent.objects.get(id=lessonSession.get('lesson'))\n        except (ValueError, ObjectDoesNotExist):\n            messages.error(request,_('Invalid lesson identifier passed to sign-up form.'))\n            return HttpResponseRedirect(reverse('bookPrivateLesson'))\n\n        expiry = parse_datetime(lessonSession.get('expiry',''),)\n        if not expiry or expiry < timezone.now():\n            messages.info(request,_('Your registration session has expired. Please try again.'))\n            return HttpResponseRedirect(reverse('bookPrivateLesson'))\n\n        self.payAtDoor = lessonSession.get('payAtDoor',False)\n        return super(PrivateLessonStudentInfoView,self).dispatch(request,*args,**kwargs)", "code_tokens": ["def", "dispatch", "(", "self", ",", "request", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "lessonSession", "=", "request", ".", "session", ".", "get", "(", "PRIVATELESSON_VALIDATION_STR", ",", "{", "}", ")", "try", ":", "self", ".", "lesson", "=", "PrivateLessonEvent", ".", "objects", ".", "get", "(", "id", "=", "lessonSession", ".", "get", "(", "'lesson'", ")", ")", "except", "(", "ValueError", ",", "ObjectDoesNotExist", ")", ":", "messages", ".", "error", "(", "request", ",", "_", "(", "'Invalid lesson identifier passed to sign-up form.'", ")", ")", "return", "HttpResponseRedirect", "(", "reverse", "(", "'bookPrivateLesson'", ")", ")", "expiry", "=", "parse_datetime", "(", "lessonSession", ".", "get", "(", "'expiry'", ",", "''", ")", ",", ")", "if", "not", "expiry", "or", "expiry", "<", "timezone", ".", "now", "(", ")", ":", "messages", ".", "info", "(", "request", ",", "_", "(", "'Your registration session has expired. Please try again.'", ")", ")", "return", "HttpResponseRedirect", "(", "reverse", "(", "'bookPrivateLesson'", ")", ")", "self", ".", "payAtDoor", "=", "lessonSession", ".", "get", "(", "'payAtDoor'", ",", "False", ")", "return", "super", "(", "PrivateLessonStudentInfoView", ",", "self", ")", ".", "dispatch", "(", "request", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Handle the session data passed by the prior view.", "docstring_tokens": ["Handle", "the", "session", "data", "passed", "by", "the", "prior", "view", "."], "sha": "bb08cbf39017a812a5a94bdb4ea34170bf1a30ba", "url": "https://github.com/django-danceschool/django-danceschool/blob/bb08cbf39017a812a5a94bdb4ea34170bf1a30ba/danceschool/private_lessons/views.py#L308-L327", "partition": "train", "idx": 236461}
{"repo": "ska-sa/montblanc", "path": "montblanc/impl/rime/tensorflow/queue_wrapper.py", "func_name": "_get_queue_types", "original_string": "def _get_queue_types(fed_arrays, data_sources):\n    \"\"\"\n    Given a list of arrays to feed in fed_arrays, return\n    a list of associated queue types, obtained from tuples\n    in the data_sources dictionary\n    \"\"\"\n    try:\n        return [data_sources[n].dtype for n in fed_arrays]\n    except KeyError as e:\n        raise ValueError(\"Array '{k}' has no data source!\"\n            .format(k=e.message)), None, sys.exc_info()[2]", "language": "python", "code": "def _get_queue_types(fed_arrays, data_sources):\n    \"\"\"\n    Given a list of arrays to feed in fed_arrays, return\n    a list of associated queue types, obtained from tuples\n    in the data_sources dictionary\n    \"\"\"\n    try:\n        return [data_sources[n].dtype for n in fed_arrays]\n    except KeyError as e:\n        raise ValueError(\"Array '{k}' has no data source!\"\n            .format(k=e.message)), None, sys.exc_info()[2]", "code_tokens": ["def", "_get_queue_types", "(", "fed_arrays", ",", "data_sources", ")", ":", "try", ":", "return", "[", "data_sources", "[", "n", "]", ".", "dtype", "for", "n", "in", "fed_arrays", "]", "except", "KeyError", "as", "e", ":", "raise", "ValueError", "(", "\"Array '{k}' has no data source!\"", ".", "format", "(", "k", "=", "e", ".", "message", ")", ")", ",", "None", ",", "sys", ".", "exc_info", "(", ")", "[", "2", "]"], "docstring": "Given a list of arrays to feed in fed_arrays, return\n    a list of associated queue types, obtained from tuples\n    in the data_sources dictionary", "docstring_tokens": ["Given", "a", "list", "of", "arrays", "to", "feed", "in", "fed_arrays", "return", "a", "list", "of", "associated", "queue", "types", "obtained", "from", "tuples", "in", "the", "data_sources", "dictionary"], "sha": "8a2e742e7500bcc6196489b735f87b233075dd2d", "url": "https://github.com/ska-sa/montblanc/blob/8a2e742e7500bcc6196489b735f87b233075dd2d/montblanc/impl/rime/tensorflow/queue_wrapper.py#L8-L18", "partition": "train", "idx": 187918}
{"repo": "edx/edx-celeryutils", "path": "celery_utils/logged_task.py", "func_name": "LoggedTask.on_retry", "original_string": "def on_retry(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"\n        Capture the exception that caused the task to be retried, if any.\n        \"\"\"\n        super(LoggedTask, self).on_retry(exc, task_id, args, kwargs, einfo)\n        log.warning('[{}] retried due to {}'.format(task_id, getattr(einfo, 'traceback', None)))", "language": "python", "code": "def on_retry(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"\n        Capture the exception that caused the task to be retried, if any.\n        \"\"\"\n        super(LoggedTask, self).on_retry(exc, task_id, args, kwargs, einfo)\n        log.warning('[{}] retried due to {}'.format(task_id, getattr(einfo, 'traceback', None)))", "code_tokens": ["def", "on_retry", "(", "self", ",", "exc", ",", "task_id", ",", "args", ",", "kwargs", ",", "einfo", ")", ":", "super", "(", "LoggedTask", ",", "self", ")", ".", "on_retry", "(", "exc", ",", "task_id", ",", "args", ",", "kwargs", ",", "einfo", ")", "log", ".", "warning", "(", "'[{}] retried due to {}'", ".", "format", "(", "task_id", ",", "getattr", "(", "einfo", ",", "'traceback'", ",", "None", ")", ")", ")"], "docstring": "Capture the exception that caused the task to be retried, if any.", "docstring_tokens": ["Capture", "the", "exception", "that", "caused", "the", "task", "to", "be", "retried", "if", "any", "."], "sha": "d8745f5f0929ad154fad779a19fbefe7f51e9498", "url": "https://github.com/edx/edx-celeryutils/blob/d8745f5f0929ad154fad779a19fbefe7f51e9498/celery_utils/logged_task.py#L35-L40", "partition": "train", "idx": 12680}
{"repo": "pandas-dev/pandas", "path": "pandas/core/apply.py", "func_name": "FrameColumnApply.wrap_results_for_axis", "original_string": "def wrap_results_for_axis(self):\n        \"\"\" return the results for the columns \"\"\"\n        results = self.results\n\n        # we have requested to expand\n        if self.result_type == 'expand':\n            result = self.infer_to_same_shape()\n\n        # we have a non-series and don't want inference\n        elif not isinstance(results[0], ABCSeries):\n            from pandas import Series\n            result = Series(results)\n            result.index = self.res_index\n\n        # we may want to infer results\n        else:\n            result = self.infer_to_same_shape()\n\n        return result", "language": "python", "code": "def wrap_results_for_axis(self):\n        \"\"\" return the results for the columns \"\"\"\n        results = self.results\n\n        # we have requested to expand\n        if self.result_type == 'expand':\n            result = self.infer_to_same_shape()\n\n        # we have a non-series and don't want inference\n        elif not isinstance(results[0], ABCSeries):\n            from pandas import Series\n            result = Series(results)\n            result.index = self.res_index\n\n        # we may want to infer results\n        else:\n            result = self.infer_to_same_shape()\n\n        return result", "code_tokens": ["def", "wrap_results_for_axis", "(", "self", ")", ":", "results", "=", "self", ".", "results", "# we have requested to expand", "if", "self", ".", "result_type", "==", "'expand'", ":", "result", "=", "self", ".", "infer_to_same_shape", "(", ")", "# we have a non-series and don't want inference", "elif", "not", "isinstance", "(", "results", "[", "0", "]", ",", "ABCSeries", ")", ":", "from", "pandas", "import", "Series", "result", "=", "Series", "(", "results", ")", "result", ".", "index", "=", "self", ".", "res_index", "# we may want to infer results", "else", ":", "result", "=", "self", ".", "infer_to_same_shape", "(", ")", "return", "result"], "docstring": "return the results for the columns", "docstring_tokens": ["return", "the", "results", "for", "the", "columns"], "sha": "9feb3ad92cc0397a04b665803a49299ee7aa1037", "url": "https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/apply.py#L377-L395", "partition": "train", "idx": 160420}
{"repo": "CZ-NIC/yangson", "path": "yangson/schemanode.py", "func_name": "InternalNode.filter_children", "original_string": "def filter_children(self, ctype: ContentType = None) -> List[SchemaNode]:\n        \"\"\"Return receiver's children based on content type.\n\n        Args:\n            ctype: Content type.\n        \"\"\"\n        if ctype is None:\n            ctype = self.content_type()\n        return [c for c in self.children if\n                not isinstance(c, (RpcActionNode, NotificationNode)) and\n                c.content_type().value & ctype.value != 0]", "language": "python", "code": "def filter_children(self, ctype: ContentType = None) -> List[SchemaNode]:\n        \"\"\"Return receiver's children based on content type.\n\n        Args:\n            ctype: Content type.\n        \"\"\"\n        if ctype is None:\n            ctype = self.content_type()\n        return [c for c in self.children if\n                not isinstance(c, (RpcActionNode, NotificationNode)) and\n                c.content_type().value & ctype.value != 0]", "code_tokens": ["def", "filter_children", "(", "self", ",", "ctype", ":", "ContentType", "=", "None", ")", "->", "List", "[", "SchemaNode", "]", ":", "if", "ctype", "is", "None", ":", "ctype", "=", "self", ".", "content_type", "(", ")", "return", "[", "c", "for", "c", "in", "self", ".", "children", "if", "not", "isinstance", "(", "c", ",", "(", "RpcActionNode", ",", "NotificationNode", ")", ")", "and", "c", ".", "content_type", "(", ")", ".", "value", "&", "ctype", ".", "value", "!=", "0", "]"], "docstring": "Return receiver's children based on content type.\n\n        Args:\n            ctype: Content type.", "docstring_tokens": ["Return", "receiver", "s", "children", "based", "on", "content", "type", "."], "sha": "a4b9464041fa8b28f6020a420ababf18fddf5d4a", "url": "https://github.com/CZ-NIC/yangson/blob/a4b9464041fa8b28f6020a420ababf18fddf5d4a/yangson/schemanode.py#L403-L413", "partition": "train", "idx": 119725}
{"repo": "openfisca/openfisca-web-api", "path": "openfisca_web_api/urls.py", "func_name": "make_router", "original_string": "def make_router(*routings):\n    \"\"\"Return a WSGI application that dispatches requests to controllers \"\"\"\n    routes = []\n    for routing in routings:\n        methods, regex, app = routing[:3]\n        if isinstance(methods, basestring):\n            methods = (methods,)\n        vars = routing[3] if len(routing) >= 4 else {}\n        routes.append((methods, re.compile(unicode(regex)), app, vars))\n\n    def router(environ, start_response):\n        \"\"\"Dispatch request to controllers.\"\"\"\n        req = webob.Request(environ)\n        split_path_info = req.path_info.split('/')\n        if split_path_info[0]:\n            # When path_info doesn't start with a \"/\" this is an error or a attack => Reject request.\n            # An example of an URL with such a invalid path_info: http://127.0.0.1http%3A//127.0.0.1%3A80/result?...\n            ctx = contexts.Ctx(req)\n            headers = wsgihelpers.handle_cross_origin_resource_sharing(ctx)\n            return wsgihelpers.respond_json(ctx,\n                dict(\n                    apiVersion = 1,\n                    error = dict(\n                        code = 400,  # Bad Request\n                        message = ctx._(u\"Invalid path: {0}\").format(req.path_info),\n                        ),\n                    ),\n                headers = headers,\n                )(environ, start_response)\n        for methods, regex, app, vars in routes:\n            match = regex.match(req.path_info)\n            if match is not None:\n                if methods is not None and req.method not in methods:\n                    ctx = contexts.Ctx(req)\n                    headers = wsgihelpers.handle_cross_origin_resource_sharing(ctx)\n                    return wsgihelpers.respond_json(ctx,\n                        dict(\n                            apiVersion = 1,\n                            error = dict(\n                                code = 405,\n                                message = ctx._(u\"You cannot use HTTP {} to access this URL. Use one of {}.\").format(\n                                    req.method, methods),\n                                ),\n                            ),\n                        headers = headers,\n                        )(environ, start_response)\n                if getattr(req, 'urlvars', None) is None:\n                    req.urlvars = {}\n                req.urlvars.update(match.groupdict())\n                req.urlvars.update(vars)\n                req.script_name += req.path_info[:match.end()]\n                req.path_info = req.path_info[match.end():]\n                return app(req.environ, start_response)\n        ctx = contexts.Ctx(req)\n        headers = wsgihelpers.handle_cross_origin_resource_sharing(ctx)\n        return wsgihelpers.respond_json(ctx,\n            dict(\n                apiVersion = 1,\n                error = dict(\n                    code = 404,  # Not Found\n                    message = ctx._(u\"Path not found: {0}\").format(req.path_info),\n                    ),\n                ),\n            headers = headers,\n            )(environ, start_response)\n\n    return router", "language": "python", "code": "def make_router(*routings):\n    \"\"\"Return a WSGI application that dispatches requests to controllers \"\"\"\n    routes = []\n    for routing in routings:\n        methods, regex, app = routing[:3]\n        if isinstance(methods, basestring):\n            methods = (methods,)\n        vars = routing[3] if len(routing) >= 4 else {}\n        routes.append((methods, re.compile(unicode(regex)), app, vars))\n\n    def router(environ, start_response):\n        \"\"\"Dispatch request to controllers.\"\"\"\n        req = webob.Request(environ)\n        split_path_info = req.path_info.split('/')\n        if split_path_info[0]:\n            # When path_info doesn't start with a \"/\" this is an error or a attack => Reject request.\n            # An example of an URL with such a invalid path_info: http://127.0.0.1http%3A//127.0.0.1%3A80/result?...\n            ctx = contexts.Ctx(req)\n            headers = wsgihelpers.handle_cross_origin_resource_sharing(ctx)\n            return wsgihelpers.respond_json(ctx,\n                dict(\n                    apiVersion = 1,\n                    error = dict(\n                        code = 400,  # Bad Request\n                        message = ctx._(u\"Invalid path: {0}\").format(req.path_info),\n                        ),\n                    ),\n                headers = headers,\n                )(environ, start_response)\n        for methods, regex, app, vars in routes:\n            match = regex.match(req.path_info)\n            if match is not None:\n                if methods is not None and req.method not in methods:\n                    ctx = contexts.Ctx(req)\n                    headers = wsgihelpers.handle_cross_origin_resource_sharing(ctx)\n                    return wsgihelpers.respond_json(ctx,\n                        dict(\n                            apiVersion = 1,\n                            error = dict(\n                                code = 405,\n                                message = ctx._(u\"You cannot use HTTP {} to access this URL. Use one of {}.\").format(\n                                    req.method, methods),\n                                ),\n                            ),\n                        headers = headers,\n                        )(environ, start_response)\n                if getattr(req, 'urlvars', None) is None:\n                    req.urlvars = {}\n                req.urlvars.update(match.groupdict())\n                req.urlvars.update(vars)\n                req.script_name += req.path_info[:match.end()]\n                req.path_info = req.path_info[match.end():]\n                return app(req.environ, start_response)\n        ctx = contexts.Ctx(req)\n        headers = wsgihelpers.handle_cross_origin_resource_sharing(ctx)\n        return wsgihelpers.respond_json(ctx,\n            dict(\n                apiVersion = 1,\n                error = dict(\n                    code = 404,  # Not Found\n                    message = ctx._(u\"Path not found: {0}\").format(req.path_info),\n                    ),\n                ),\n            headers = headers,\n            )(environ, start_response)\n\n    return router", "code_tokens": ["def", "make_router", "(", "*", "routings", ")", ":", "routes", "=", "[", "]", "for", "routing", "in", "routings", ":", "methods", ",", "regex", ",", "app", "=", "routing", "[", ":", "3", "]", "if", "isinstance", "(", "methods", ",", "basestring", ")", ":", "methods", "=", "(", "methods", ",", ")", "vars", "=", "routing", "[", "3", "]", "if", "len", "(", "routing", ")", ">=", "4", "else", "{", "}", "routes", ".", "append", "(", "(", "methods", ",", "re", ".", "compile", "(", "unicode", "(", "regex", ")", ")", ",", "app", ",", "vars", ")", ")", "def", "router", "(", "environ", ",", "start_response", ")", ":", "\"\"\"Dispatch request to controllers.\"\"\"", "req", "=", "webob", ".", "Request", "(", "environ", ")", "split_path_info", "=", "req", ".", "path_info", ".", "split", "(", "'/'", ")", "if", "split_path_info", "[", "0", "]", ":", "# When path_info doesn't start with a \"/\" this is an error or a attack => Reject request.", "# An example of an URL with such a invalid path_info: http://127.0.0.1http%3A//127.0.0.1%3A80/result?...", "ctx", "=", "contexts", ".", "Ctx", "(", "req", ")", "headers", "=", "wsgihelpers", ".", "handle_cross_origin_resource_sharing", "(", "ctx", ")", "return", "wsgihelpers", ".", "respond_json", "(", "ctx", ",", "dict", "(", "apiVersion", "=", "1", ",", "error", "=", "dict", "(", "code", "=", "400", ",", "# Bad Request", "message", "=", "ctx", ".", "_", "(", "u\"Invalid path: {0}\"", ")", ".", "format", "(", "req", ".", "path_info", ")", ",", ")", ",", ")", ",", "headers", "=", "headers", ",", ")", "(", "environ", ",", "start_response", ")", "for", "methods", ",", "regex", ",", "app", ",", "vars", "in", "routes", ":", "match", "=", "regex", ".", "match", "(", "req", ".", "path_info", ")", "if", "match", "is", "not", "None", ":", "if", "methods", "is", "not", "None", "and", "req", ".", "method", "not", "in", "methods", ":", "ctx", "=", "contexts", ".", "Ctx", "(", "req", ")", "headers", "=", "wsgihelpers", ".", "handle_cross_origin_resource_sharing", "(", "ctx", ")", "return", "wsgihelpers", ".", "respond_json", "(", "ctx", ",", "dict", "(", "apiVersion", "=", "1", ",", "error", "=", "dict", "(", "code", "=", "405", ",", "message", "=", "ctx", ".", "_", "(", "u\"You cannot use HTTP {} to access this URL. Use one of {}.\"", ")", ".", "format", "(", "req", ".", "method", ",", "methods", ")", ",", ")", ",", ")", ",", "headers", "=", "headers", ",", ")", "(", "environ", ",", "start_response", ")", "if", "getattr", "(", "req", ",", "'urlvars'", ",", "None", ")", "is", "None", ":", "req", ".", "urlvars", "=", "{", "}", "req", ".", "urlvars", ".", "update", "(", "match", ".", "groupdict", "(", ")", ")", "req", ".", "urlvars", ".", "update", "(", "vars", ")", "req", ".", "script_name", "+=", "req", ".", "path_info", "[", ":", "match", ".", "end", "(", ")", "]", "req", ".", "path_info", "=", "req", ".", "path_info", "[", "match", ".", "end", "(", ")", ":", "]", "return", "app", "(", "req", ".", "environ", ",", "start_response", ")", "ctx", "=", "contexts", ".", "Ctx", "(", "req", ")", "headers", "=", "wsgihelpers", ".", "handle_cross_origin_resource_sharing", "(", "ctx", ")", "return", "wsgihelpers", ".", "respond_json", "(", "ctx", ",", "dict", "(", "apiVersion", "=", "1", ",", "error", "=", "dict", "(", "code", "=", "404", ",", "# Not Found", "message", "=", "ctx", ".", "_", "(", "u\"Path not found: {0}\"", ")", ".", "format", "(", "req", ".", "path_info", ")", ",", ")", ",", ")", ",", "headers", "=", "headers", ",", ")", "(", "environ", ",", "start_response", ")", "return", "router"], "docstring": "Return a WSGI application that dispatches requests to controllers", "docstring_tokens": ["Return", "a", "WSGI", "application", "that", "dispatches", "requests", "to", "controllers"], "sha": "d1cd3bfacac338e80bb0df7e0465b65649dd893b", "url": "https://github.com/openfisca/openfisca-web-api/blob/d1cd3bfacac338e80bb0df7e0465b65649dd893b/openfisca_web_api/urls.py#L63-L129", "partition": "train", "idx": 72702}
{"repo": "djm/python-scrapyd-api", "path": "scrapyd_api/wrapper.py", "func_name": "ScrapydAPI.delete_version", "original_string": "def delete_version(self, project, version):\n        \"\"\"\n        Deletes a specific version of a project. First class, maps to\n        Scrapyd's delete version endpoint.\n        \"\"\"\n        url = self._build_url(constants.DELETE_VERSION_ENDPOINT)\n        data = {\n            'project': project,\n            'version': version\n        }\n        self.client.post(url, data=data, timeout=self.timeout)\n        return True", "language": "python", "code": "def delete_version(self, project, version):\n        \"\"\"\n        Deletes a specific version of a project. First class, maps to\n        Scrapyd's delete version endpoint.\n        \"\"\"\n        url = self._build_url(constants.DELETE_VERSION_ENDPOINT)\n        data = {\n            'project': project,\n            'version': version\n        }\n        self.client.post(url, data=data, timeout=self.timeout)\n        return True", "code_tokens": ["def", "delete_version", "(", "self", ",", "project", ",", "version", ")", ":", "url", "=", "self", ".", "_build_url", "(", "constants", ".", "DELETE_VERSION_ENDPOINT", ")", "data", "=", "{", "'project'", ":", "project", ",", "'version'", ":", "version", "}", "self", ".", "client", ".", "post", "(", "url", ",", "data", "=", "data", ",", "timeout", "=", "self", ".", "timeout", ")", "return", "True"], "docstring": "Deletes a specific version of a project. First class, maps to\n        Scrapyd's delete version endpoint.", "docstring_tokens": ["Deletes", "a", "specific", "version", "of", "a", "project", ".", "First", "class", "maps", "to", "Scrapyd", "s", "delete", "version", "endpoint", "."], "sha": "42f287cf83c3a5bd46795f4f85cce02a56829921", "url": "https://github.com/djm/python-scrapyd-api/blob/42f287cf83c3a5bd46795f4f85cce02a56829921/scrapyd_api/wrapper.py#L107-L118", "partition": "train", "idx": 212673}
{"repo": "aio-libs/aiohttp", "path": "aiohttp/http_writer.py", "func_name": "StreamWriter.write", "original_string": "async def write(self, chunk: bytes,\n                    *, drain: bool=True, LIMIT: int=0x10000) -> None:\n        \"\"\"Writes chunk of data to a stream.\n\n        write_eof() indicates end of stream.\n        writer can't be used after write_eof() method being called.\n        write() return drain future.\n        \"\"\"\n        if self._on_chunk_sent is not None:\n            await self._on_chunk_sent(chunk)\n\n        if self._compress is not None:\n            chunk = self._compress.compress(chunk)\n            if not chunk:\n                return\n\n        if self.length is not None:\n            chunk_len = len(chunk)\n            if self.length >= chunk_len:\n                self.length = self.length - chunk_len\n            else:\n                chunk = chunk[:self.length]\n                self.length = 0\n                if not chunk:\n                    return\n\n        if chunk:\n            if self.chunked:\n                chunk_len_pre = ('%x\\r\\n' % len(chunk)).encode('ascii')\n                chunk = chunk_len_pre + chunk + b'\\r\\n'\n\n            self._write(chunk)\n\n            if self.buffer_size > LIMIT and drain:\n                self.buffer_size = 0\n                await self.drain()", "language": "python", "code": "async def write(self, chunk: bytes,\n                    *, drain: bool=True, LIMIT: int=0x10000) -> None:\n        \"\"\"Writes chunk of data to a stream.\n\n        write_eof() indicates end of stream.\n        writer can't be used after write_eof() method being called.\n        write() return drain future.\n        \"\"\"\n        if self._on_chunk_sent is not None:\n            await self._on_chunk_sent(chunk)\n\n        if self._compress is not None:\n            chunk = self._compress.compress(chunk)\n            if not chunk:\n                return\n\n        if self.length is not None:\n            chunk_len = len(chunk)\n            if self.length >= chunk_len:\n                self.length = self.length - chunk_len\n            else:\n                chunk = chunk[:self.length]\n                self.length = 0\n                if not chunk:\n                    return\n\n        if chunk:\n            if self.chunked:\n                chunk_len_pre = ('%x\\r\\n' % len(chunk)).encode('ascii')\n                chunk = chunk_len_pre + chunk + b'\\r\\n'\n\n            self._write(chunk)\n\n            if self.buffer_size > LIMIT and drain:\n                self.buffer_size = 0\n                await self.drain()", "code_tokens": ["async", "def", "write", "(", "self", ",", "chunk", ":", "bytes", ",", "*", ",", "drain", ":", "bool", "=", "True", ",", "LIMIT", ":", "int", "=", "0x10000", ")", "->", "None", ":", "if", "self", ".", "_on_chunk_sent", "is", "not", "None", ":", "await", "self", ".", "_on_chunk_sent", "(", "chunk", ")", "if", "self", ".", "_compress", "is", "not", "None", ":", "chunk", "=", "self", ".", "_compress", ".", "compress", "(", "chunk", ")", "if", "not", "chunk", ":", "return", "if", "self", ".", "length", "is", "not", "None", ":", "chunk_len", "=", "len", "(", "chunk", ")", "if", "self", ".", "length", ">=", "chunk_len", ":", "self", ".", "length", "=", "self", ".", "length", "-", "chunk_len", "else", ":", "chunk", "=", "chunk", "[", ":", "self", ".", "length", "]", "self", ".", "length", "=", "0", "if", "not", "chunk", ":", "return", "if", "chunk", ":", "if", "self", ".", "chunked", ":", "chunk_len_pre", "=", "(", "'%x\\r\\n'", "%", "len", "(", "chunk", ")", ")", ".", "encode", "(", "'ascii'", ")", "chunk", "=", "chunk_len_pre", "+", "chunk", "+", "b'\\r\\n'", "self", ".", "_write", "(", "chunk", ")", "if", "self", ".", "buffer_size", ">", "LIMIT", "and", "drain", ":", "self", ".", "buffer_size", "=", "0", "await", "self", ".", "drain", "(", ")"], "docstring": "Writes chunk of data to a stream.\n\n        write_eof() indicates end of stream.\n        writer can't be used after write_eof() method being called.\n        write() return drain future.", "docstring_tokens": ["Writes", "chunk", "of", "data", "to", "a", "stream", "."], "sha": "9504fe2affaaff673fa4f3754c1c44221f8ba47d", "url": "https://github.com/aio-libs/aiohttp/blob/9504fe2affaaff673fa4f3754c1c44221f8ba47d/aiohttp/http_writer.py#L70-L105", "partition": "train", "idx": 166831}
{"repo": "Fortran-FOSS-Programmers/ford", "path": "ford/utils.py", "func_name": "quote_split", "original_string": "def quote_split(sep,string):\n    \"\"\"\n    Splits the strings into pieces divided by sep, when sep in not inside quotes.\n    \"\"\"\n    if len(sep) != 1: raise Exception(\"Separation string must be one character long\")\n    retlist = []\n    squote = False\n    dquote = False\n    left = 0\n    i = 0\n    while i < len(string):\n        if string[i] == '\"' and not dquote:\n            if not squote:\n                squote = True\n            elif (i+1) < len(string) and string[i+1] == '\"':\n                i += 1\n            else:\n                squote = False\n        elif string[i] == \"'\" and not squote:\n            if not dquote:\n                dquote = True\n            elif (i+1) < len(string) and string[i+1] == \"'\":\n                i += 1\n            else:\n                dquote = False            \n        elif string[i] == sep and not dquote and not squote:\n            retlist.append(string[left:i])\n            left = i + 1\n        i += 1\n    retlist.append(string[left:])\n    return retlist", "language": "python", "code": "def quote_split(sep,string):\n    \"\"\"\n    Splits the strings into pieces divided by sep, when sep in not inside quotes.\n    \"\"\"\n    if len(sep) != 1: raise Exception(\"Separation string must be one character long\")\n    retlist = []\n    squote = False\n    dquote = False\n    left = 0\n    i = 0\n    while i < len(string):\n        if string[i] == '\"' and not dquote:\n            if not squote:\n                squote = True\n            elif (i+1) < len(string) and string[i+1] == '\"':\n                i += 1\n            else:\n                squote = False\n        elif string[i] == \"'\" and not squote:\n            if not dquote:\n                dquote = True\n            elif (i+1) < len(string) and string[i+1] == \"'\":\n                i += 1\n            else:\n                dquote = False            \n        elif string[i] == sep and not dquote and not squote:\n            retlist.append(string[left:i])\n            left = i + 1\n        i += 1\n    retlist.append(string[left:])\n    return retlist", "code_tokens": ["def", "quote_split", "(", "sep", ",", "string", ")", ":", "if", "len", "(", "sep", ")", "!=", "1", ":", "raise", "Exception", "(", "\"Separation string must be one character long\"", ")", "retlist", "=", "[", "]", "squote", "=", "False", "dquote", "=", "False", "left", "=", "0", "i", "=", "0", "while", "i", "<", "len", "(", "string", ")", ":", "if", "string", "[", "i", "]", "==", "'\"'", "and", "not", "dquote", ":", "if", "not", "squote", ":", "squote", "=", "True", "elif", "(", "i", "+", "1", ")", "<", "len", "(", "string", ")", "and", "string", "[", "i", "+", "1", "]", "==", "'\"'", ":", "i", "+=", "1", "else", ":", "squote", "=", "False", "elif", "string", "[", "i", "]", "==", "\"'\"", "and", "not", "squote", ":", "if", "not", "dquote", ":", "dquote", "=", "True", "elif", "(", "i", "+", "1", ")", "<", "len", "(", "string", ")", "and", "string", "[", "i", "+", "1", "]", "==", "\"'\"", ":", "i", "+=", "1", "else", ":", "dquote", "=", "False", "elif", "string", "[", "i", "]", "==", "sep", "and", "not", "dquote", "and", "not", "squote", ":", "retlist", ".", "append", "(", "string", "[", "left", ":", "i", "]", ")", "left", "=", "i", "+", "1", "i", "+=", "1", "retlist", ".", "append", "(", "string", "[", "left", ":", "]", ")", "return", "retlist"], "docstring": "Splits the strings into pieces divided by sep, when sep in not inside quotes.", "docstring_tokens": ["Splits", "the", "strings", "into", "pieces", "divided", "by", "sep", "when", "sep", "in", "not", "inside", "quotes", "."], "sha": "d46a44eae20d99205292c31785f936fbed47070f", "url": "https://github.com/Fortran-FOSS-Programmers/ford/blob/d46a44eae20d99205292c31785f936fbed47070f/ford/utils.py#L110-L140", "partition": "train", "idx": 232854}
{"repo": "ray-project/ray", "path": "python/ray/rllib/utils/memory.py", "func_name": "concat_aligned", "original_string": "def concat_aligned(items):\n    \"\"\"Concatenate arrays, ensuring the output is 64-byte aligned.\n\n    We only align float arrays; other arrays are concatenated as normal.\n\n    This should be used instead of np.concatenate() to improve performance\n    when the output array is likely to be fed into TensorFlow.\n    \"\"\"\n\n    if len(items) == 0:\n        return []\n    elif len(items) == 1:\n        # we assume the input is aligned. In any case, it doesn't help\n        # performance to force align it since that incurs a needless copy.\n        return items[0]\n    elif (isinstance(items[0], np.ndarray)\n          and items[0].dtype in [np.float32, np.float64, np.uint8]):\n        dtype = items[0].dtype\n        flat = aligned_array(sum(s.size for s in items), dtype)\n        batch_dim = sum(s.shape[0] for s in items)\n        new_shape = (batch_dim, ) + items[0].shape[1:]\n        output = flat.reshape(new_shape)\n        assert output.ctypes.data % 64 == 0, output.ctypes.data\n        np.concatenate(items, out=output)\n        return output\n    else:\n        return np.concatenate(items)", "language": "python", "code": "def concat_aligned(items):\n    \"\"\"Concatenate arrays, ensuring the output is 64-byte aligned.\n\n    We only align float arrays; other arrays are concatenated as normal.\n\n    This should be used instead of np.concatenate() to improve performance\n    when the output array is likely to be fed into TensorFlow.\n    \"\"\"\n\n    if len(items) == 0:\n        return []\n    elif len(items) == 1:\n        # we assume the input is aligned. In any case, it doesn't help\n        # performance to force align it since that incurs a needless copy.\n        return items[0]\n    elif (isinstance(items[0], np.ndarray)\n          and items[0].dtype in [np.float32, np.float64, np.uint8]):\n        dtype = items[0].dtype\n        flat = aligned_array(sum(s.size for s in items), dtype)\n        batch_dim = sum(s.shape[0] for s in items)\n        new_shape = (batch_dim, ) + items[0].shape[1:]\n        output = flat.reshape(new_shape)\n        assert output.ctypes.data % 64 == 0, output.ctypes.data\n        np.concatenate(items, out=output)\n        return output\n    else:\n        return np.concatenate(items)", "code_tokens": ["def", "concat_aligned", "(", "items", ")", ":", "if", "len", "(", "items", ")", "==", "0", ":", "return", "[", "]", "elif", "len", "(", "items", ")", "==", "1", ":", "# we assume the input is aligned. In any case, it doesn't help", "# performance to force align it since that incurs a needless copy.", "return", "items", "[", "0", "]", "elif", "(", "isinstance", "(", "items", "[", "0", "]", ",", "np", ".", "ndarray", ")", "and", "items", "[", "0", "]", ".", "dtype", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", ",", "np", ".", "uint8", "]", ")", ":", "dtype", "=", "items", "[", "0", "]", ".", "dtype", "flat", "=", "aligned_array", "(", "sum", "(", "s", ".", "size", "for", "s", "in", "items", ")", ",", "dtype", ")", "batch_dim", "=", "sum", "(", "s", ".", "shape", "[", "0", "]", "for", "s", "in", "items", ")", "new_shape", "=", "(", "batch_dim", ",", ")", "+", "items", "[", "0", "]", ".", "shape", "[", "1", ":", "]", "output", "=", "flat", ".", "reshape", "(", "new_shape", ")", "assert", "output", ".", "ctypes", ".", "data", "%", "64", "==", "0", ",", "output", ".", "ctypes", ".", "data", "np", ".", "concatenate", "(", "items", ",", "out", "=", "output", ")", "return", "output", "else", ":", "return", "np", ".", "concatenate", "(", "items", ")"], "docstring": "Concatenate arrays, ensuring the output is 64-byte aligned.\n\n    We only align float arrays; other arrays are concatenated as normal.\n\n    This should be used instead of np.concatenate() to improve performance\n    when the output array is likely to be fed into TensorFlow.", "docstring_tokens": ["Concatenate", "arrays", "ensuring", "the", "output", "is", "64", "-", "byte", "aligned", "."], "sha": "4eade036a0505e244c976f36aaa2d64386b5129b", "url": "https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/rllib/utils/memory.py#L66-L92", "partition": "train", "idx": 164440}
{"repo": "polysquare/polysquare-setuptools-lint", "path": "polysquare_setuptools_lint/__init__.py", "func_name": "_run_spellcheck_linter", "original_string": "def _run_spellcheck_linter(matched_filenames, cache_dir, show_lint_files):\n    \"\"\"Run spellcheck-linter on matched_filenames.\"\"\"\n    from polysquarelinter import lint_spelling_only as lint\n    from prospector.message import Message, Location\n\n    for filename in matched_filenames:\n        _debug_linter_status(\"spellcheck-linter\", filename, show_lint_files)\n\n    return_dict = dict()\n\n    def _custom_reporter(error, file_path):\n        line = error.line_offset + 1\n        key = _Key(file_path, line, \"file/spelling_error\")\n        loc = Location(file_path, None, None, line, 0)\n        # suppress(protected-access)\n        desc = lint._SPELLCHECK_MESSAGES[error.error_type].format(error.word)\n        return_dict[key] = Message(\"spellcheck-linter\",\n                                   \"file/spelling_error\",\n                                   loc,\n                                   desc)\n\n    # suppress(protected-access,unused-attribute)\n    lint._report_spelling_error = _custom_reporter\n    lint.main([\n        \"--spellcheck-cache=\" + os.path.join(cache_dir, \"spelling\"),\n        \"--stamp-file-path=\" + os.path.join(cache_dir,\n                                            \"jobstamps\",\n                                            \"polysquarelinter\"),\n        \"--technical-terms=\" + os.path.join(cache_dir, \"technical-terms\"),\n    ] + matched_filenames)\n\n    return return_dict", "language": "python", "code": "def _run_spellcheck_linter(matched_filenames, cache_dir, show_lint_files):\n    \"\"\"Run spellcheck-linter on matched_filenames.\"\"\"\n    from polysquarelinter import lint_spelling_only as lint\n    from prospector.message import Message, Location\n\n    for filename in matched_filenames:\n        _debug_linter_status(\"spellcheck-linter\", filename, show_lint_files)\n\n    return_dict = dict()\n\n    def _custom_reporter(error, file_path):\n        line = error.line_offset + 1\n        key = _Key(file_path, line, \"file/spelling_error\")\n        loc = Location(file_path, None, None, line, 0)\n        # suppress(protected-access)\n        desc = lint._SPELLCHECK_MESSAGES[error.error_type].format(error.word)\n        return_dict[key] = Message(\"spellcheck-linter\",\n                                   \"file/spelling_error\",\n                                   loc,\n                                   desc)\n\n    # suppress(protected-access,unused-attribute)\n    lint._report_spelling_error = _custom_reporter\n    lint.main([\n        \"--spellcheck-cache=\" + os.path.join(cache_dir, \"spelling\"),\n        \"--stamp-file-path=\" + os.path.join(cache_dir,\n                                            \"jobstamps\",\n                                            \"polysquarelinter\"),\n        \"--technical-terms=\" + os.path.join(cache_dir, \"technical-terms\"),\n    ] + matched_filenames)\n\n    return return_dict", "code_tokens": ["def", "_run_spellcheck_linter", "(", "matched_filenames", ",", "cache_dir", ",", "show_lint_files", ")", ":", "from", "polysquarelinter", "import", "lint_spelling_only", "as", "lint", "from", "prospector", ".", "message", "import", "Message", ",", "Location", "for", "filename", "in", "matched_filenames", ":", "_debug_linter_status", "(", "\"spellcheck-linter\"", ",", "filename", ",", "show_lint_files", ")", "return_dict", "=", "dict", "(", ")", "def", "_custom_reporter", "(", "error", ",", "file_path", ")", ":", "line", "=", "error", ".", "line_offset", "+", "1", "key", "=", "_Key", "(", "file_path", ",", "line", ",", "\"file/spelling_error\"", ")", "loc", "=", "Location", "(", "file_path", ",", "None", ",", "None", ",", "line", ",", "0", ")", "# suppress(protected-access)", "desc", "=", "lint", ".", "_SPELLCHECK_MESSAGES", "[", "error", ".", "error_type", "]", ".", "format", "(", "error", ".", "word", ")", "return_dict", "[", "key", "]", "=", "Message", "(", "\"spellcheck-linter\"", ",", "\"file/spelling_error\"", ",", "loc", ",", "desc", ")", "# suppress(protected-access,unused-attribute)", "lint", ".", "_report_spelling_error", "=", "_custom_reporter", "lint", ".", "main", "(", "[", "\"--spellcheck-cache=\"", "+", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "\"spelling\"", ")", ",", "\"--stamp-file-path=\"", "+", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "\"jobstamps\"", ",", "\"polysquarelinter\"", ")", ",", "\"--technical-terms=\"", "+", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "\"technical-terms\"", ")", ",", "]", "+", "matched_filenames", ")", "return", "return_dict"], "docstring": "Run spellcheck-linter on matched_filenames.", "docstring_tokens": ["Run", "spellcheck", "-", "linter", "on", "matched_filenames", "."], "sha": "5df5a6401c7ad6a90b42230eeb99c82cc56952b6", "url": "https://github.com/polysquare/polysquare-setuptools-lint/blob/5df5a6401c7ad6a90b42230eeb99c82cc56952b6/polysquare_setuptools_lint/__init__.py#L358-L389", "partition": "train", "idx": 99891}
{"repo": "Scoppio/RagnarokEngine3", "path": "RagnarokEngine3/RE3.py", "func_name": "Sprite.__execute_rot", "original_string": "def __execute_rot(self, surface):\n        \"\"\"Executes the rotating operation\"\"\"\n        self.image = pygame.transform.rotate(surface, self.__rotation)\n        self.__resize_surface_extents()", "language": "python", "code": "def __execute_rot(self, surface):\n        \"\"\"Executes the rotating operation\"\"\"\n        self.image = pygame.transform.rotate(surface, self.__rotation)\n        self.__resize_surface_extents()", "code_tokens": ["def", "__execute_rot", "(", "self", ",", "surface", ")", ":", "self", ".", "image", "=", "pygame", ".", "transform", ".", "rotate", "(", "surface", ",", "self", ".", "__rotation", ")", "self", ".", "__resize_surface_extents", "(", ")"], "docstring": "Executes the rotating operation", "docstring_tokens": ["Executes", "the", "rotating", "operation"], "sha": "4395d419ccd64fe9327c41f200b72ee0176ad896", "url": "https://github.com/Scoppio/RagnarokEngine3/blob/4395d419ccd64fe9327c41f200b72ee0176ad896/RagnarokEngine3/RE3.py#L1403-L1406", "partition": "train", "idx": 70226}
{"repo": "Feneric/doxypypy", "path": "doxypypy/doxypypy.py", "func_name": "AstWalker._endCodeIfNeeded", "original_string": "def _endCodeIfNeeded(line, inCodeBlock):\n        \"\"\"Simple routine to append end code marker if needed.\"\"\"\n        assert isinstance(line, str)\n        if inCodeBlock:\n            line = '# @endcode{0}{1}'.format(linesep, line.rstrip())\n            inCodeBlock = False\n        return line, inCodeBlock", "language": "python", "code": "def _endCodeIfNeeded(line, inCodeBlock):\n        \"\"\"Simple routine to append end code marker if needed.\"\"\"\n        assert isinstance(line, str)\n        if inCodeBlock:\n            line = '# @endcode{0}{1}'.format(linesep, line.rstrip())\n            inCodeBlock = False\n        return line, inCodeBlock", "code_tokens": ["def", "_endCodeIfNeeded", "(", "line", ",", "inCodeBlock", ")", ":", "assert", "isinstance", "(", "line", ",", "str", ")", "if", "inCodeBlock", ":", "line", "=", "'# @endcode{0}{1}'", ".", "format", "(", "linesep", ",", "line", ".", "rstrip", "(", ")", ")", "inCodeBlock", "=", "False", "return", "line", ",", "inCodeBlock"], "docstring": "Simple routine to append end code marker if needed.", "docstring_tokens": ["Simple", "routine", "to", "append", "end", "code", "marker", "if", "needed", "."], "sha": "a8555b15fa2a758ea8392372de31c0f635cc0d93", "url": "https://github.com/Feneric/doxypypy/blob/a8555b15fa2a758ea8392372de31c0f635cc0d93/doxypypy/doxypypy.py#L112-L118", "partition": "train", "idx": 30993}
{"repo": "joferkington/mplstereonet", "path": "mplstereonet/contouring.py", "func_name": "_exponential_kamb", "original_string": "def _exponential_kamb(cos_dist, sigma=3):\n    \"\"\"Kernel function from Vollmer for exponential smoothing.\"\"\"\n    n = float(cos_dist.size)\n    f = 2 * (1.0 + n / sigma**2)\n    count = np.exp(f * (cos_dist - 1))\n    units = np.sqrt(n * (f/2.0 - 1) / f**2)\n    return count, units", "language": "python", "code": "def _exponential_kamb(cos_dist, sigma=3):\n    \"\"\"Kernel function from Vollmer for exponential smoothing.\"\"\"\n    n = float(cos_dist.size)\n    f = 2 * (1.0 + n / sigma**2)\n    count = np.exp(f * (cos_dist - 1))\n    units = np.sqrt(n * (f/2.0 - 1) / f**2)\n    return count, units", "code_tokens": ["def", "_exponential_kamb", "(", "cos_dist", ",", "sigma", "=", "3", ")", ":", "n", "=", "float", "(", "cos_dist", ".", "size", ")", "f", "=", "2", "*", "(", "1.0", "+", "n", "/", "sigma", "**", "2", ")", "count", "=", "np", ".", "exp", "(", "f", "*", "(", "cos_dist", "-", "1", ")", ")", "units", "=", "np", ".", "sqrt", "(", "n", "*", "(", "f", "/", "2.0", "-", "1", ")", "/", "f", "**", "2", ")", "return", "count", ",", "units"], "docstring": "Kernel function from Vollmer for exponential smoothing.", "docstring_tokens": ["Kernel", "function", "from", "Vollmer", "for", "exponential", "smoothing", "."], "sha": "f6d78ca49807915d4223e864e12bb24d497cc2d6", "url": "https://github.com/joferkington/mplstereonet/blob/f6d78ca49807915d4223e864e12bb24d497cc2d6/mplstereonet/contouring.py#L183-L189", "partition": "train", "idx": 54039}
{"repo": "brmscheiner/ideogram", "path": "ideogram/importAnalysis.py", "func_name": "getImportFromObjects", "original_string": "def getImportFromObjects(node):\n    '''Returns a list of objects referenced by import from node'''\n    somenames = [x.asname for x in node.names if x.asname]\n    othernames = [x.name for x in node.names if not x.asname]\n    return somenames+othernames", "language": "python", "code": "def getImportFromObjects(node):\n    '''Returns a list of objects referenced by import from node'''\n    somenames = [x.asname for x in node.names if x.asname]\n    othernames = [x.name for x in node.names if not x.asname]\n    return somenames+othernames", "code_tokens": ["def", "getImportFromObjects", "(", "node", ")", ":", "somenames", "=", "[", "x", ".", "asname", "for", "x", "in", "node", ".", "names", "if", "x", ".", "asname", "]", "othernames", "=", "[", "x", ".", "name", "for", "x", "in", "node", ".", "names", "if", "not", "x", ".", "asname", "]", "return", "somenames", "+", "othernames"], "docstring": "Returns a list of objects referenced by import from node", "docstring_tokens": ["Returns", "a", "list", "of", "objects", "referenced", "by", "import", "from", "node"], "sha": "422bf566c51fd56f7bbb6e75b16d18d52b4c7568", "url": "https://github.com/brmscheiner/ideogram/blob/422bf566c51fd56f7bbb6e75b16d18d52b4c7568/ideogram/importAnalysis.py#L51-L55", "partition": "train", "idx": 58147}
{"repo": "proteanhq/protean", "path": "src/protean/impl/repository/dict_repo.py", "func_name": "DictRepository.filter", "original_string": "def filter(self, criteria: Q, offset: int = 0, limit: int = 10, order_by: list = ()):\n        \"\"\"Read the repository and return results as per the filer\"\"\"\n\n        if criteria.children:\n            items = list(self._filter(criteria, self.conn['data'][self.schema_name]).values())\n        else:\n            items = list(self.conn['data'][self.schema_name].values())\n\n        # Sort the filtered results based on the order_by clause\n        for o_key in order_by:\n            reverse = False\n            if o_key.startswith('-'):\n                reverse = True\n                o_key = o_key[1:]\n            items = sorted(items, key=itemgetter(o_key), reverse=reverse)\n\n        result = ResultSet(\n            offset=offset,\n            limit=limit,\n            total=len(items),\n            items=items[offset: offset + limit])\n        return result", "language": "python", "code": "def filter(self, criteria: Q, offset: int = 0, limit: int = 10, order_by: list = ()):\n        \"\"\"Read the repository and return results as per the filer\"\"\"\n\n        if criteria.children:\n            items = list(self._filter(criteria, self.conn['data'][self.schema_name]).values())\n        else:\n            items = list(self.conn['data'][self.schema_name].values())\n\n        # Sort the filtered results based on the order_by clause\n        for o_key in order_by:\n            reverse = False\n            if o_key.startswith('-'):\n                reverse = True\n                o_key = o_key[1:]\n            items = sorted(items, key=itemgetter(o_key), reverse=reverse)\n\n        result = ResultSet(\n            offset=offset,\n            limit=limit,\n            total=len(items),\n            items=items[offset: offset + limit])\n        return result", "code_tokens": ["def", "filter", "(", "self", ",", "criteria", ":", "Q", ",", "offset", ":", "int", "=", "0", ",", "limit", ":", "int", "=", "10", ",", "order_by", ":", "list", "=", "(", ")", ")", ":", "if", "criteria", ".", "children", ":", "items", "=", "list", "(", "self", ".", "_filter", "(", "criteria", ",", "self", ".", "conn", "[", "'data'", "]", "[", "self", ".", "schema_name", "]", ")", ".", "values", "(", ")", ")", "else", ":", "items", "=", "list", "(", "self", ".", "conn", "[", "'data'", "]", "[", "self", ".", "schema_name", "]", ".", "values", "(", ")", ")", "# Sort the filtered results based on the order_by clause", "for", "o_key", "in", "order_by", ":", "reverse", "=", "False", "if", "o_key", ".", "startswith", "(", "'-'", ")", ":", "reverse", "=", "True", "o_key", "=", "o_key", "[", "1", ":", "]", "items", "=", "sorted", "(", "items", ",", "key", "=", "itemgetter", "(", "o_key", ")", ",", "reverse", "=", "reverse", ")", "result", "=", "ResultSet", "(", "offset", "=", "offset", ",", "limit", "=", "limit", ",", "total", "=", "len", "(", "items", ")", ",", "items", "=", "items", "[", "offset", ":", "offset", "+", "limit", "]", ")", "return", "result"], "docstring": "Read the repository and return results as per the filer", "docstring_tokens": ["Read", "the", "repository", "and", "return", "results", "as", "per", "the", "filer"], "sha": "0e29873f4aa634aa93cc08ed675dd749c7ed4b0f", "url": "https://github.com/proteanhq/protean/blob/0e29873f4aa634aa93cc08ed675dd749c7ed4b0f/src/protean/impl/repository/dict_repo.py#L190-L211", "partition": "train", "idx": 47089}
{"repo": "resonai/ybt", "path": "yabt/caching.py", "func_name": "get_prebuilt_targets", "original_string": "def get_prebuilt_targets(build_context):\n    \"\"\"Return set of target names that are contained within cached base images\n\n    These targets may be considered \"pre-built\", and skipped during build.\n    \"\"\"\n    logger.info('Scanning for cached base images')\n    # deps that are part of cached based images\n    contained_deps = set()\n    # deps that are needed by images that are going to be built,\n    # but are not part of their base images\n    required_deps = set()\n    # mapping from target name to set of all its deps (descendants)\n    cached_descendants = CachedDescendants(build_context.target_graph)\n\n    for target_name, target in build_context.targets.items():\n        if 'image_caching_behavior' not in target.props:\n            continue\n        image_name = get_image_name(target)\n        image_tag = target.props.image_tag\n        icb = ImageCachingBehavior(image_name, image_tag,\n                                   target.props.image_caching_behavior)\n        target.image_id = handle_build_cache(build_context.conf, image_name,\n                                             image_tag, icb)\n        if target.image_id:\n            # mark deps of cached base image as \"contained\"\n            image_deps = cached_descendants.get(target_name)\n            contained_deps.update(image_deps)\n            contained_deps.add(target.name)\n        else:\n            # mark deps of image that is going to be built\n            # (and are not deps of its base image) as \"required\"\n            image_deps = cached_descendants.get(target_name)\n            base_image_deps = cached_descendants.get(target.props.base_image)\n            required_deps.update(image_deps - base_image_deps)\n    return contained_deps - required_deps", "language": "python", "code": "def get_prebuilt_targets(build_context):\n    \"\"\"Return set of target names that are contained within cached base images\n\n    These targets may be considered \"pre-built\", and skipped during build.\n    \"\"\"\n    logger.info('Scanning for cached base images')\n    # deps that are part of cached based images\n    contained_deps = set()\n    # deps that are needed by images that are going to be built,\n    # but are not part of their base images\n    required_deps = set()\n    # mapping from target name to set of all its deps (descendants)\n    cached_descendants = CachedDescendants(build_context.target_graph)\n\n    for target_name, target in build_context.targets.items():\n        if 'image_caching_behavior' not in target.props:\n            continue\n        image_name = get_image_name(target)\n        image_tag = target.props.image_tag\n        icb = ImageCachingBehavior(image_name, image_tag,\n                                   target.props.image_caching_behavior)\n        target.image_id = handle_build_cache(build_context.conf, image_name,\n                                             image_tag, icb)\n        if target.image_id:\n            # mark deps of cached base image as \"contained\"\n            image_deps = cached_descendants.get(target_name)\n            contained_deps.update(image_deps)\n            contained_deps.add(target.name)\n        else:\n            # mark deps of image that is going to be built\n            # (and are not deps of its base image) as \"required\"\n            image_deps = cached_descendants.get(target_name)\n            base_image_deps = cached_descendants.get(target.props.base_image)\n            required_deps.update(image_deps - base_image_deps)\n    return contained_deps - required_deps", "code_tokens": ["def", "get_prebuilt_targets", "(", "build_context", ")", ":", "logger", ".", "info", "(", "'Scanning for cached base images'", ")", "# deps that are part of cached based images", "contained_deps", "=", "set", "(", ")", "# deps that are needed by images that are going to be built,", "# but are not part of their base images", "required_deps", "=", "set", "(", ")", "# mapping from target name to set of all its deps (descendants)", "cached_descendants", "=", "CachedDescendants", "(", "build_context", ".", "target_graph", ")", "for", "target_name", ",", "target", "in", "build_context", ".", "targets", ".", "items", "(", ")", ":", "if", "'image_caching_behavior'", "not", "in", "target", ".", "props", ":", "continue", "image_name", "=", "get_image_name", "(", "target", ")", "image_tag", "=", "target", ".", "props", ".", "image_tag", "icb", "=", "ImageCachingBehavior", "(", "image_name", ",", "image_tag", ",", "target", ".", "props", ".", "image_caching_behavior", ")", "target", ".", "image_id", "=", "handle_build_cache", "(", "build_context", ".", "conf", ",", "image_name", ",", "image_tag", ",", "icb", ")", "if", "target", ".", "image_id", ":", "# mark deps of cached base image as \"contained\"", "image_deps", "=", "cached_descendants", ".", "get", "(", "target_name", ")", "contained_deps", ".", "update", "(", "image_deps", ")", "contained_deps", ".", "add", "(", "target", ".", "name", ")", "else", ":", "# mark deps of image that is going to be built", "# (and are not deps of its base image) as \"required\"", "image_deps", "=", "cached_descendants", ".", "get", "(", "target_name", ")", "base_image_deps", "=", "cached_descendants", ".", "get", "(", "target", ".", "props", ".", "base_image", ")", "required_deps", ".", "update", "(", "image_deps", "-", "base_image_deps", ")", "return", "contained_deps", "-", "required_deps"], "docstring": "Return set of target names that are contained within cached base images\n\n    These targets may be considered \"pre-built\", and skipped during build.", "docstring_tokens": ["Return", "set", "of", "target", "names", "that", "are", "contained", "within", "cached", "base", "images"], "sha": "5b40df0922ef3383eb85f2b04a26a2db4b81b3fd", "url": "https://github.com/resonai/ybt/blob/5b40df0922ef3383eb85f2b04a26a2db4b81b3fd/yabt/caching.py#L70-L104", "partition": "train", "idx": 44154}
{"repo": "buriburisuri/sugartensor", "path": "sugartensor/sg_transform.py", "func_name": "sg_argmin", "original_string": "def sg_argmin(tensor, opt):\n    r\"\"\"Returns the indices of the minimum values along the specified axis.\n\n    See `tf.argin()` in tensorflow.\n\n    Args:\n      tensor: A `Tensor` (automatically given by chain).\n      opt:\n        axis: Target axis. Default is the last one.\n        name: If provided, replace current tensor's name.\n\n    Returns:\n      A `Tensor`.\n    \"\"\"\n    opt += tf.sg_opt(axis=tensor.get_shape().ndims - 1)\n    return tf.argmin(tensor, opt.axis, opt.name)", "language": "python", "code": "def sg_argmin(tensor, opt):\n    r\"\"\"Returns the indices of the minimum values along the specified axis.\n\n    See `tf.argin()` in tensorflow.\n\n    Args:\n      tensor: A `Tensor` (automatically given by chain).\n      opt:\n        axis: Target axis. Default is the last one.\n        name: If provided, replace current tensor's name.\n\n    Returns:\n      A `Tensor`.\n    \"\"\"\n    opt += tf.sg_opt(axis=tensor.get_shape().ndims - 1)\n    return tf.argmin(tensor, opt.axis, opt.name)", "code_tokens": ["def", "sg_argmin", "(", "tensor", ",", "opt", ")", ":", "opt", "+=", "tf", ".", "sg_opt", "(", "axis", "=", "tensor", ".", "get_shape", "(", ")", ".", "ndims", "-", "1", ")", "return", "tf", ".", "argmin", "(", "tensor", ",", "opt", ".", "axis", ",", "opt", ".", "name", ")"], "docstring": "r\"\"\"Returns the indices of the minimum values along the specified axis.\n\n    See `tf.argin()` in tensorflow.\n\n    Args:\n      tensor: A `Tensor` (automatically given by chain).\n      opt:\n        axis: Target axis. Default is the last one.\n        name: If provided, replace current tensor's name.\n\n    Returns:\n      A `Tensor`.", "docstring_tokens": ["r", "Returns", "the", "indices", "of", "the", "minimum", "values", "along", "the", "specified", "axis", "."], "sha": "d2c039954777c7fbe3eb0c2ae40c45c9854deb40", "url": "https://github.com/buriburisuri/sugartensor/blob/d2c039954777c7fbe3eb0c2ae40c45c9854deb40/sugartensor/sg_transform.py#L199-L214", "partition": "train", "idx": 246247}
