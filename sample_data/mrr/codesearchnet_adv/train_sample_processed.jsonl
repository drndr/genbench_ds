{"code": "def command_help_long ( self ) : indent = \" \" * 2 # replace with current_indent help = \"Command must be one of:\\n\" for action_name in self . parser . valid_commands : help += \"%s%-10s %-70s\\n\" % ( indent , action_name , self . parser . commands [ action_name ] . desc_short . capitalize ( ) ) help += '\\nSee \\'%s help COMMAND\\' for help and information on a command' % self . parser . prog return help", "docstring": "Return command help for use in global parser usage string", "idx": 55241}
{"code": "def rigthgen ( self , value = 0 ) : while True : yield self . newarray ( self . nplanes_right * self . width , value )", "docstring": "Generate rows to fill right pixels in int mode", "idx": 31335}
{"code": "def _get_python_block_output ( src , global_dict , local_dict ) : src = '\\n' . join ( [ l for l in src . split ( '\\n' ) if not l . startswith ( '%' ) and not 'plt.show()' in l ] ) ret_status = True err = '' with _string_io ( ) as s : try : exec ( src , global_dict , global_dict ) except Exception as e : err = str ( e ) ret_status = False return ( ret_status , s . getvalue ( ) + err )", "docstring": "Evaluate python source codes", "idx": 163457}
{"code": "def slicelist ( b ) : slicelst = [ ] started = False for i , e in enumerate ( b ) : if e and not started : start = i started = True elif not e and started : slicelst . append ( slice ( start , i ) ) started = False if e : slicelst . append ( slice ( start , i + 1 ) ) # True in the end. return slicelst", "docstring": "Produce a list of slices given the boolean array b .", "idx": 100496}
{"code": "def in_book_search_highlighted_results ( request ) : results = { } args = request . matchdict ident_hash = args [ 'ident_hash' ] page_ident_hash = args [ 'page_ident_hash' ] try : page_uuid , _ = split_ident_hash ( page_ident_hash ) except IdentHashShortId as e : page_uuid = get_uuid ( e . id ) except IdentHashMissingVersion as e : page_uuid = e . id args [ 'page_uuid' ] = page_uuid args [ 'search_term' ] = request . params . get ( 'q' , '' ) query_type = request . params . get ( 'query_type' , '' ) combiner = '' if query_type : if query_type . lower ( ) == 'or' : combiner = '_or' # Get version from URL params id , version = split_ident_hash ( ident_hash ) args [ 'uuid' ] = id args [ 'version' ] = version with db_connect ( ) as db_connection : with db_connection . cursor ( ) as cursor : cursor . execute ( SQL [ 'get-collated-state' ] , args ) res = cursor . fetchall ( ) if res and res [ 0 ] [ 0 ] : statement = SQL [ 'get-in-collated-book-search-full-page' ] else : statement = SQL [ 'get-in-book-search-full-page' ] cursor . execute ( statement . format ( combiner = combiner ) , args ) res = cursor . fetchall ( ) results [ 'results' ] = { 'query' : [ ] , 'total' : len ( res ) , 'items' : [ ] } results [ 'results' ] [ 'query' ] = { 'search_term' : args [ 'search_term' ] , 'collection_id' : ident_hash , } for uuid , version , title , headline , rank in res : results [ 'results' ] [ 'items' ] . append ( { 'rank' : '{}' . format ( rank ) , 'id' : '{}' . format ( page_ident_hash ) , 'title' : '{}' . format ( title ) , 'html' : '{}' . format ( headline ) , } ) resp = request . response resp . status = '200 OK' resp . content_type = 'application/json' resp . body = json . dumps ( results ) return resp", "docstring": "In - book search - returns a highlighted version of the HTML .", "idx": 48711}
{"code": "def load_module ( filename ) : path , name = os . path . split ( filename ) name , ext = os . path . splitext ( name ) ( file , filename , desc ) = imp . find_module ( name , [ path ] ) try : return imp . load_module ( name , file , filename , desc ) finally : if file : file . close ( )", "docstring": "Loads a module from anywhere in the system .", "idx": 88499}
{"code": "def event_html_page_context ( app , pagename , templatename , context , doctree ) : assert app or pagename or templatename # Unused, for linting. if 'script_files' in context and doctree and any ( hasattr ( n , 'disqus_shortname' ) for n in doctree . traverse ( ) ) : # Clone list to prevent leaking into other pages and add disqus.js to this page. context [ 'script_files' ] = context [ 'script_files' ] [ : ] + [ '_static/disqus.js' ]", "docstring": "Called when the HTML builder has created a context dictionary to render a template with .", "idx": 113021}
{"code": "def cumsum ( vari , axis = None ) : if isinstance ( vari , Poly ) : core = vari . A . copy ( ) for key , val in core . items ( ) : core [ key ] = cumsum ( val , axis ) return Poly ( core , vari . dim , None , vari . dtype ) return np . cumsum ( vari , axis )", "docstring": "Cumulative sum the components of a shapeable quantity along a given axis .", "idx": 206963}
{"code": "def timing ( self , stat , delta , rate = 1 ) : return self . send ( stat , \"%d|ms\" % delta , rate )", "docstring": "Send new timing information . delta is in milliseconds .", "idx": 88459}
{"code": "def _one_hidden ( self , l : int ) -> Tensor : nh = ( self . n_hid if l != self . n_layers - 1 else self . emb_sz ) // self . n_dir return one_param ( self ) . new ( 1 , self . bs , nh ) . zero_ ( )", "docstring": "Return one hidden state .", "idx": 160628}
{"code": "def create_post ( post_uid , post_data ) : title = post_data [ 'title' ] . strip ( ) if len ( title ) < 2 : return False cur_rec = MPost . get_by_uid ( post_uid ) if cur_rec : return False entry = TabPost . create ( title = title , date = datetime . now ( ) , cnt_md = tornado . escape . xhtml_escape ( post_data [ 'cnt_md' ] . strip ( ) ) , cnt_html = tools . markdown2html ( post_data [ 'cnt_md' ] ) , uid = post_uid , time_create = post_data . get ( 'time_create' , tools . timestamp ( ) ) , time_update = post_data . get ( 'time_update' , tools . timestamp ( ) ) , user_name = post_data [ 'user_name' ] , view_count = post_data [ 'view_count' ] if 'view_count' in post_data else 1 , logo = post_data [ 'logo' ] , memo = post_data [ 'memo' ] if 'memo' in post_data else '' , order = post_data [ 'order' ] if 'order' in post_data else '' , keywords = post_data [ 'keywords' ] if 'keywords' in post_data else '' , extinfo = post_data [ 'extinfo' ] if 'extinfo' in post_data else { } , kind = post_data [ 'kind' ] if 'kind' in post_data else '1' , valid = post_data . get ( 'valid' , 1 ) ) return entry . uid", "docstring": "create the post .", "idx": 108732}
{"code": "def offline ( f ) : @ click . pass_context @ verbose def new_func ( ctx , * args , * * kwargs ) : ctx . obj [ \"offline\" ] = True ctx . bitshares = BitShares ( * * ctx . obj ) ctx . blockchain = ctx . bitshares ctx . bitshares . set_shared_instance ( ) return ctx . invoke ( f , * args , * * kwargs ) return update_wrapper ( new_func , f )", "docstring": "This decorator allows you to access ctx . bitshares which is an instance of BitShares with offline = True .", "idx": 110782}
{"code": "def get_order_description ( self , payment , order ) : template = getattr ( settings , 'GETPAID_ORDER_DESCRIPTION' , None ) if template : return Template ( template ) . render ( Context ( { \"payment\" : payment , \"order\" : order } ) ) else : return six . text_type ( order )", "docstring": "Renders order description using django template provided in settings . GETPAID_ORDER_DESCRIPTION or if not provided return unicode representation of Order object .", "idx": 246377}
{"code": "def handle_address_save ( self , sender , instance , * * kwargs ) : objects = self . find_associated_with_address ( instance ) for obj in objects : self . handle_save ( obj . __class__ , obj )", "docstring": "Custom handler for address save", "idx": 103477}
{"code": "def log_histograms ( self , model : Model , histogram_parameters : Set [ str ] ) -> None : for name , param in model . named_parameters ( ) : if name in histogram_parameters : self . add_train_histogram ( \"parameter_histogram/\" + name , param )", "docstring": "Send histograms of parameters to tensorboard .", "idx": 163051}
{"code": "def fetch_alien ( self , ) : parent = self . get_parent ( ) if parent : parentelement = parent . get_element ( ) else : parentelement = self . get_refobjinter ( ) . get_current_element ( ) if not parentelement : self . _alien = True return self . _alien element = self . get_element ( ) if element == parentelement : self . _alien = False # test if it is the element is a global shot # first test if we have a shot # then test if it is in a global sequence. then the shot is global too. # test if the parent element is a shot, if they share the sequence, and element is global elif isinstance ( element , djadapter . models . Shot ) and ( element . sequence . name == djadapter . GLOBAL_NAME or ( isinstance ( parentelement , djadapter . models . Shot ) and parentelement . sequence == element . sequence and element . name == djadapter . GLOBAL_NAME ) ) : self . _alien = False else : assets = parentelement . assets . all ( ) self . _alien = element not in assets return self . _alien", "docstring": "Set and return if the reftrack element is linked to the current scene .", "idx": 89951}
{"code": "def stop_request ( self , stop_now = '0' ) : self . app . interrupted = ( stop_now == '1' ) self . app . will_stop = True return True", "docstring": "Request the daemon to stop", "idx": 19498}
{"code": "def areas ( self ) : v1 = self . points [ self . simplices [ : , 1 ] ] - self . points [ self . simplices [ : , 0 ] ] v2 = self . points [ self . simplices [ : , 2 ] ] - self . points [ self . simplices [ : , 1 ] ] area = 0.5 * ( v1 [ : , 0 ] * v2 [ : , 1 ] - v1 [ : , 1 ] * v2 [ : , 0 ] ) return area", "docstring": "Compute the area of each triangle within the triangulation of points .", "idx": 141628}
{"code": "def iter_rows ( self ) : fileobj = self . _fileobj cls_row = self . cls_row fields = self . fields for idx in range ( self . prolog . records_count ) : data = fileobj . read ( 1 ) marker = struct . unpack ( '<1s' , data ) [ 0 ] is_deleted = marker == b'*' if is_deleted : continue row_values = [ ] for field in fields : val = field . cast ( fileobj . read ( field . len ) ) row_values . append ( val ) yield cls_row ( * row_values )", "docstring": "Generator reading . dbf row one by one .", "idx": 75187}
{"code": "def scroll_up ( self , locator ) : driver = self . _current_application ( ) element = self . _element_find ( locator , True , True ) driver . execute_script ( \"mobile: scroll\" , { \"direction\" : 'up' , 'element' : element . id } )", "docstring": "Scrolls up to element", "idx": 226388}
{"code": "def get_next ( self , protocol = 'http' , format = False , policy = 'loop' ) : if not self . proxies [ protocol ] : return None if policy == 'loop' : idx = self . idx [ protocol ] self . idx [ protocol ] = ( idx + 1 ) % len ( self . proxies [ protocol ] ) elif policy == 'random' : idx = random . randint ( 0 , self . proxy_num ( protocol ) - 1 ) else : self . logger . error ( 'Unsupported get_next policy: {}' . format ( policy ) ) exit ( ) proxy = self . proxies [ protocol ] [ self . addr_list [ protocol ] [ idx ] ] if proxy . weight < random . random ( ) : return self . get_next ( protocol , format , policy ) if format : return proxy . format ( ) else : return proxy", "docstring": "Get the next proxy", "idx": 215421}
{"code": "def get_fc_dimensions ( self , strides , kernel_sizes ) : output_height , output_width , _ = self . hparams . problem . frame_shape output_steps = self . hparams . video_num_target_frames output_shape = np . array ( [ output_steps , output_height , output_width ] ) for curr_stride , kernel_size in zip ( strides , kernel_sizes ) : output_shape = self . expected_output_shape ( output_shape , np . array ( curr_stride ) , 1 , kernel_size ) return np . prod ( output_shape ) * self . hparams . num_discriminator_filters * 8", "docstring": "Get expected fully connected shape after a series of convolutions .", "idx": 161765}
{"code": "def prep_jid ( nocache = False , passed_jid = None , recurse_count = 0 ) : if recurse_count >= 5 : err = 'prep_jid could not store a jid after {0} tries.' . format ( recurse_count ) log . error ( err ) raise salt . exceptions . SaltCacheError ( err ) if passed_jid is None : # this can be a None or an empty string. jid = salt . utils . jid . gen_jid ( __opts__ ) else : jid = passed_jid jid_dir = salt . utils . jid . jid_dir ( jid , _job_dir ( ) , __opts__ [ 'hash_type' ] ) # Make sure we create the jid dir, otherwise someone else is using it, # meaning we need a new jid. if not os . path . isdir ( jid_dir ) : try : os . makedirs ( jid_dir ) except OSError : time . sleep ( 0.1 ) if passed_jid is None : return prep_jid ( nocache = nocache , recurse_count = recurse_count + 1 ) try : with salt . utils . files . fopen ( os . path . join ( jid_dir , 'jid' ) , 'wb+' ) as fn_ : fn_ . write ( salt . utils . stringutils . to_bytes ( jid ) ) if nocache : with salt . utils . files . fopen ( os . path . join ( jid_dir , 'nocache' ) , 'wb+' ) : pass except IOError : log . warning ( 'Could not write out jid file for job %s. Retrying.' , jid ) time . sleep ( 0.1 ) return prep_jid ( passed_jid = jid , nocache = nocache , recurse_count = recurse_count + 1 ) return jid", "docstring": "Return a job id and prepare the job id directory .", "idx": 175623}
{"code": "def _configure_from_mapping ( self , item , whitelist_keys = False , whitelist = None ) : if whitelist is None : whitelist = self . config . keys ( ) if whitelist_keys : item = { k : v for k , v in item . items ( ) if k in whitelist } self . config . from_mapping ( item ) return self", "docstring": "Configure from a mapping or dict like object .", "idx": 111685}
{"code": "def typescript_compile ( source ) : with open ( TS_COMPILER , 'r' ) as tsservices_js : return evaljs ( ( tsservices_js . read ( ) , 'ts.transpile(dukpy.tscode, {options});' . format ( options = TSC_OPTIONS ) ) , tscode = source )", "docstring": "Compiles the given source from TypeScript to ES5 using TypescriptServices . js", "idx": 240172}
{"code": "def lookups ( self , request , model_admin ) : active_objects = self . model . published . all ( ) . annotate ( count_entries_published = Count ( 'entries' ) ) . order_by ( '-count_entries_published' , '-pk' ) for active_object in active_objects : yield ( str ( active_object . pk ) , ungettext_lazy ( '%(item)s (%(count)i entry)' , '%(item)s (%(count)i entries)' , active_object . count_entries_published ) % { 'item' : smart_text ( active_object ) , 'count' : active_object . count_entries_published } )", "docstring": "Return published objects with the number of entries .", "idx": 148707}
{"code": "def untranslateName ( s ) : s = s . replace ( 'DOT' , '.' ) s = s . replace ( 'DOLLAR' , '$' ) # delete 'PY' at start of name components if s [ : 2 ] == 'PY' : s = s [ 2 : ] s = s . replace ( '.PY' , '.' ) return s", "docstring": "Undo Python conversion of CL parameter or variable name .", "idx": 193554}
{"code": "def get_asset_ddo ( self , did ) : response = self . requests_session . get ( f'{self.url}/{did}' ) . content if not response : return { } try : parsed_response = json . loads ( response ) except TypeError : parsed_response = None except ValueError : raise ValueError ( response . decode ( 'UTF-8' ) ) if parsed_response is None : return { } return Asset ( dictionary = parsed_response )", "docstring": "Retrieve asset ddo for a given did .", "idx": 235392}
{"code": "def _pad_block ( self ) : pad_length = _BLOCK_SIZE - self . __position % _BLOCK_SIZE if pad_length and pad_length != _BLOCK_SIZE : self . __writer . write ( '\\x00' * pad_length ) self . __position += pad_length", "docstring": "Pad block with 0 .", "idx": 115753}
{"code": "def packbools ( bools , dtype = 'L' ) : r = NBITS [ dtype ] atoms = ATOMS [ dtype ] for chunk in zip_longest ( * [ iter ( bools ) ] * r , fillvalue = False ) : yield sum ( compress ( atoms , chunk ) )", "docstring": "Yield integers concatenating bools in chunks of dtype bit - length .", "idx": 3724}
{"code": "def signals ( self , signals = ( 'QUIT' , 'USR1' , 'USR2' ) ) : for sig in signals : signal . signal ( getattr ( signal , 'SIG' + sig ) , self . handler )", "docstring": "Register our signal handler", "idx": 44774}
{"code": "def crawler ( site , uri = None ) : config = load_site_config ( site ) model = _get_model ( 'crawl' , config , uri ) visited_set , visited_uri_set , consume_set , crawl_set = get_site_sets ( site , config ) if not visited_set . has ( model . hash ) : visited_set . add ( model . hash ) visited_uri_set . add ( model . uri ) if ( model . is_consume_page and not consume_set . has ( model . hash ) ) : consume_set . add ( model . hash ) consume_q . enqueue ( consumer , site , model . uri ) else : for crawl_uri in model . uris_to_crawl : if ( not visited_uri_set . has ( crawl_uri ) and not crawl_set . has ( crawl_uri ) ) : crawl_set . add ( crawl_uri ) crawl_q . enqueue ( crawler , site , crawl_uri )", "docstring": "Crawl URI using site config .", "idx": 96468}
{"code": "def validate_milestones ( self ) : milestones = self . arc_root_node . get_children ( ) . filter ( arc_element_type__contains = 'mile' ) current_cursor = 0 for mile in milestones : seq = mile . milestone_seq if seq < current_cursor : return mile current_cursor = seq return None", "docstring": "Reviews the arc element tree to ensure that milestones appear in the right order .", "idx": 93536}
{"code": "def score_pairs ( self , pairs ) : pairs = check_input ( pairs , type_of_inputs = 'tuples' , preprocessor = self . preprocessor_ , estimator = self , tuple_size = 2 ) pairwise_diffs = self . transform ( pairs [ : , 1 , : ] - pairs [ : , 0 , : ] ) # (for MahalanobisMixin, the embedding is linear so we can just embed the # difference) return np . sqrt ( np . sum ( pairwise_diffs ** 2 , axis = - 1 ) )", "docstring": "Returns the learned Mahalanobis distance between pairs .", "idx": 152615}
{"code": "def tabledata_insert_all ( self , table_name , rows ) : url = Api . _ENDPOINT + ( Api . _TABLES_PATH % table_name ) + \"/insertAll\" data = { 'kind' : 'bigquery#tableDataInsertAllRequest' , 'rows' : rows } return datalab . utils . Http . request ( url , data = data , credentials = self . _credentials )", "docstring": "Issues a request to insert data into a table .", "idx": 237840}
{"code": "def query ( self , url , method = \"GET\" , params = dict ( ) , headers = dict ( ) ) : access_token = self . _get_at_from_session ( ) oauth = OAuth1 ( self . consumer_key , client_secret = self . secret_key , resource_owner_key = access_token [ 'oauth_token' ] , resource_owner_secret = access_token [ 'oauth_token_secret' ] ) response = getattr ( requests , method . lower ( ) ) ( url , auth = oauth , headers = headers , params = params ) if response . status_code != 200 : raise OAuthError ( _ ( 'No access to private resources at \"%s\".' ) % get_token_prefix ( self . request_token_url ) ) return response . text", "docstring": "Request a API endpoint at url with params being either the POST or GET data .", "idx": 127201}
{"code": "def add ( self , task , multiprocess = False , processes = 0 ) : if self . _first_task is None and hasattr ( task , 'task_id' ) : self . _first_task = task . task_id self . add_succeeded = True if multiprocess : queue = multiprocessing . Manager ( ) . Queue ( ) pool = multiprocessing . Pool ( processes = processes if processes > 0 else None ) else : queue = DequeQueue ( ) pool = SingleProcessPool ( ) self . _validate_task ( task ) pool . apply_async ( check_complete , [ task , queue ] ) # we track queue size ourselves because len(queue) won't work for multiprocessing queue_size = 1 try : seen = { task . task_id } while queue_size : current = queue . get ( ) queue_size -= 1 item , is_complete = current for next in self . _add ( item , is_complete ) : if next . task_id not in seen : self . _validate_task ( next ) seen . add ( next . task_id ) pool . apply_async ( check_complete , [ next , queue ] ) queue_size += 1 except ( KeyboardInterrupt , TaskException ) : raise except Exception as ex : self . add_succeeded = False formatted_traceback = traceback . format_exc ( ) self . _log_unexpected_error ( task ) task . trigger_event ( Event . BROKEN_TASK , task , ex ) self . _email_unexpected_error ( task , formatted_traceback ) raise finally : pool . close ( ) pool . join ( ) return self . add_succeeded", "docstring": "Add a Task for the worker to check and possibly schedule and run .", "idx": 171578}
{"code": "def _set_join ( self , query = None ) : if not query : query = self . _query foreign_key = '%s.%s' % ( self . _related . get_table ( ) , self . _second_key ) query . join ( self . _parent . get_table ( ) , self . get_qualified_parent_key_name ( ) , '=' , foreign_key )", "docstring": "Set the join clause for the query .", "idx": 178888}
{"code": "def export_items ( elastic_url , in_index , out_index , elastic_url_out = None , search_after = False , search_after_value = None , limit = None , copy = False ) : if not limit : limit = DEFAULT_LIMIT if search_after_value : search_after_value_timestamp = int ( search_after_value [ 0 ] ) search_after_value_uuid = search_after_value [ 1 ] search_after_value = [ search_after_value_timestamp , search_after_value_uuid ] logging . info ( \"Exporting items from %s/%s to %s\" , elastic_url , in_index , out_index ) count_res = requests . get ( '%s/%s/_count' % ( elastic_url , in_index ) ) try : count_res . raise_for_status ( ) except requests . exceptions . HTTPError : if count_res . status_code == 404 : logging . error ( \"The index does not exists: %s\" , in_index ) else : logging . error ( count_res . text ) sys . exit ( 1 ) logging . info ( \"Total items to copy: %i\" , count_res . json ( ) [ 'count' ] ) # Time to upload the items with the correct mapping elastic_in = ElasticSearch ( elastic_url , in_index ) if not copy : # Create the correct mapping for the data sources detected from in_index ds_mapping = find_mapping ( elastic_url , in_index ) else : logging . debug ( 'Using the input index mapping' ) ds_mapping = extract_mapping ( elastic_url , in_index ) if not elastic_url_out : elastic_out = ElasticSearch ( elastic_url , out_index , mappings = ds_mapping ) else : elastic_out = ElasticSearch ( elastic_url_out , out_index , mappings = ds_mapping ) # Time to just copy from in_index to our_index uid_field = find_uuid ( elastic_url , in_index ) backend = find_perceval_backend ( elastic_url , in_index ) if search_after : total = elastic_out . bulk_upload ( fetch ( elastic_in , backend , limit , search_after_value , scroll = False ) , uid_field ) else : total = elastic_out . bulk_upload ( fetch ( elastic_in , backend , limit ) , uid_field ) logging . info ( \"Total items copied: %i\" , total )", "docstring": "Export items from in_index to out_index using the correct mapping", "idx": 18278}
{"code": "def update_fallbackserver ( self , serverid , data ) : return self . api_call ( ENDPOINTS [ 'fallbackservers' ] [ 'update' ] , dict ( serverid = serverid ) , body = data )", "docstring": "Update Fallback server", "idx": 85478}
{"code": "def write_screen_name_to_topics ( filepath , user_label_matrix , node_to_id , id_to_name , label_to_lemma , lemma_to_keyword , separator = \",\" ) : user_label_matrix = spsp . coo_matrix ( user_label_matrix ) shape = user_label_matrix . shape nnz = user_label_matrix . getnnz ( ) row = user_label_matrix . row col = user_label_matrix . col data = user_label_matrix . data name_to_topic_set = defaultdict ( set ) for edge in range ( row . size ) : node = row [ edge ] user_twitter_id = node_to_id [ node ] name = id_to_name [ user_twitter_id ] label = col [ edge ] lemma = label_to_lemma [ label ] # topic = lemma_to_keyword[lemma] name_to_topic_set [ name ] . add ( lemma ) with open ( filepath , \"w\" ) as f : # Write metadata. file_row = \"n_rows:\" + separator + str ( shape [ 0 ] ) + separator + \"nnz:\" + separator + str ( nnz ) + separator + \"\\n\" f . write ( file_row ) for name , topic_set in name_to_topic_set . items ( ) : file_row = list ( ) file_row . append ( name ) file_row . extend ( topic_set ) file_row = separator . join ( file_row ) + \"\\n\" f . write ( file_row )", "docstring": "Writes a user name and associated topic names per row .", "idx": 47451}
{"code": "def push ( self , dir ) : self . stack . append ( os . getcwd ( ) ) os . chdir ( dir or os . getcwd ( ) )", "docstring": "Push cwd on stack and change to dir .", "idx": 58749}
{"code": "def date ( objet ) : if objet : return \"{}/{}/{}\" . format ( objet . day , objet . month , objet . year ) return \"\"", "docstring": "abstractRender d une date datetime . date", "idx": 181793}
{"code": "def _create_default_config_file ( self ) : logger . info ( 'Initialize Maya launcher, creating config file...\\n' ) self . add_section ( self . DEFAULTS ) self . add_section ( self . PATTERNS ) self . add_section ( self . ENVIRONMENTS ) self . add_section ( self . EXECUTABLES ) self . set ( self . DEFAULTS , 'executable' , None ) self . set ( self . DEFAULTS , 'environment' , None ) self . set ( self . PATTERNS , 'exclude' , ', ' . join ( self . EXLUDE_PATTERNS ) ) self . set ( self . PATTERNS , 'icon_ext' , ', ' . join ( self . ICON_EXTENSIONS ) ) self . config_file . parent . mkdir ( exist_ok = True ) self . config_file . touch ( ) with self . config_file . open ( 'wb' ) as f : self . write ( f ) # If this function is run inform the user that a new file has been\r # created.\r sys . exit ( 'Maya launcher has successfully created config file at:\\n' ' \"{}\"' . format ( str ( self . config_file ) ) )", "docstring": "If config file does not exists create and set default values .", "idx": 75604}
{"code": "def jinja_extensions_feature ( app ) : # register jinja filters app . jinja_env . globals [ 'momentjs' ] = MomentJsFilters app . jinja_env . filters . update ( MomentJsFilters ( ) . get_filters ( ) ) app . jinja_env . filters . update ( DateFilters ( ) . get_filters ( ) ) app . jinja_env . filters . update ( HumanizeFilters ( ) . get_filters ( ) ) # register custom jinja functions app . jinja_env . globals . update ( dict ( asset = functions . asset , dev_proxy = functions . dev_proxy ) )", "docstring": "Enables custom templating extensions", "idx": 13284}
{"code": "def remove ( self , transport ) : if transport . uid in self . transports : del ( self . transports [ transport . uid ] )", "docstring": "removes a transport if a member of this group", "idx": 251351}
{"code": "def set ( conf ) : for name , value in conf . items ( ) : if value is not None : setattr ( Conf , name . upper ( ) , value )", "docstring": "Applies a configuration to the global config object", "idx": 38591}
{"code": "def get_stage_for_epoch ( self , epoch_start , window_length = None , attr = 'stage' ) : for epoch in self . epochs : if epoch [ 'start' ] == epoch_start : return epoch [ attr ] if window_length is not None : epoch_length = epoch [ 'end' ] - epoch [ 'start' ] if logical_and ( window_length < epoch_length , 0 <= ( epoch_start - epoch [ 'start' ] ) < epoch_length ) : return epoch [ attr ]", "docstring": "Return stage for one specific epoch .", "idx": 23566}
{"code": "def build ( self , bug : Bug , force : bool = True , quiet : bool = False ) -> None : self . __installation . build . build ( bug . image , force = force , quiet = quiet )", "docstring": "Builds the Docker image associated with a given bug .", "idx": 30061}
{"code": "def is_correct ( self , question_id ) : response = self . get_response ( question_id = question_id ) if response . is_answered ( ) : item = self . _get_item ( response . get_item_id ( ) ) return item . is_response_correct ( response ) raise errors . IllegalState ( )", "docstring": "is the question answered correctly", "idx": 81826}
{"code": "def address ( self , street , city = None , state = None , zipcode = None , * * kwargs ) : fields = { 'street' : street , 'city' : city , 'state' : state , 'zip' : zipcode , } return self . _fetch ( 'address' , fields , * * kwargs )", "docstring": "Geocode an address .", "idx": 39779}
{"code": "def entity_readme_url ( self , entity_id , channel = None ) : url = '{}/{}/readme' . format ( self . url , _get_path ( entity_id ) ) return _add_channel ( url , channel )", "docstring": "Generate the url path for the readme of an entity .", "idx": 5550}
{"code": "def _check_regr ( self , regr , new_reg ) : body = getattr ( new_reg , 'body' , new_reg ) for k , v in body . items ( ) : if k == 'resource' or not v : continue if regr . body [ k ] != v : raise errors . UnexpectedUpdate ( regr ) if regr . body . key != self . key . public_key ( ) : raise errors . UnexpectedUpdate ( regr ) return regr", "docstring": "Check that a registration response contains the registration we were expecting .", "idx": 191211}
{"code": "def create_objects_for_type ( self , raw_objects , o_type ) : # Ex: the above code do for timeperiods: # timeperiods = [] # for timeperiodcfg in objects['timeperiod']: #    t = Timeperiod(timeperiodcfg) #    timeperiods.append(t) # self.timeperiods = Timeperiods(timeperiods) types_creations = self . __class__ . types_creations ( cls , clss , prop , initial_index , _ ) = types_creations [ o_type ] # List to store the created objects lst = [ ] try : logger . info ( \"- creating '%s' objects\" , o_type ) for obj_cfg in raw_objects [ o_type ] : # We create the object my_object = cls ( obj_cfg ) # and append it to the list lst . append ( my_object ) if not lst : logger . info ( \"  none.\" ) except KeyError : logger . info ( \"  no %s objects in the configuration\" , o_type ) # Create the objects list and set it in our properties setattr ( self , prop , clss ( lst , initial_index ) )", "docstring": "Generic function to create objects regarding the o_type", "idx": 19721}
{"code": "def move_out_64 ( library , session , space , offset , length , data , extended = False ) : converted_buffer = ( ViUInt64 * length ) ( * tuple ( data ) ) if extended : return library . viMoveOut64Ex ( session , space , offset , length , converted_buffer ) else : return library . viMoveOut64 ( session , space , offset , length , converted_buffer )", "docstring": "Moves an 64 - bit block of data from local memory to the specified address space and offset .", "idx": 228878}
{"code": "def generalized_negative_binomial ( mu = 1 , alpha = 1 , shape = _Null , dtype = _Null , * * kwargs ) : return _random_helper ( _internal . _random_generalized_negative_binomial , _internal . _sample_generalized_negative_binomial , [ mu , alpha ] , shape , dtype , kwargs )", "docstring": "Draw random samples from a generalized negative binomial distribution .", "idx": 163848}
{"code": "def conf ( ) : stanza = '' stanzas = [ ] in_stanza = False ret = { } pos = 0 try : with salt . utils . files . fopen ( _detect_conf ( ) , 'r' ) as _fp : for line in _fp : line = salt . utils . stringutils . to_unicode ( line ) if line . startswith ( '#' ) : continue if line . startswith ( '\\n' ) : in_stanza = False if 'title' in stanza : stanza += 'order {0}' . format ( pos ) pos += 1 stanzas . append ( stanza ) stanza = '' continue if line . strip ( ) . startswith ( 'title' ) : if in_stanza : stanza += 'order {0}' . format ( pos ) pos += 1 stanzas . append ( stanza ) stanza = '' else : in_stanza = True if in_stanza : stanza += line if not in_stanza : key , value = _parse_line ( line ) ret [ key ] = value if in_stanza : if not line . endswith ( '\\n' ) : line += '\\n' stanza += line stanza += 'order {0}' . format ( pos ) pos += 1 stanzas . append ( stanza ) except ( IOError , OSError ) as exc : msg = \"Could not read grub config: {0}\" raise CommandExecutionError ( msg . format ( exc ) ) ret [ 'stanzas' ] = [ ] for stanza in stanzas : mydict = { } for line in stanza . strip ( ) . splitlines ( ) : key , value = _parse_line ( line ) mydict [ key ] = value ret [ 'stanzas' ] . append ( mydict ) return ret", "docstring": "Parse GRUB conf file", "idx": 122690}
{"code": "def _username ( ) : if pwd : username = pwd . getpwuid ( os . getuid ( ) ) . pw_name else : username = getpass . getuser ( ) return username", "docstring": "Grain for the minion username", "idx": 176972}
{"code": "def csv_reader ( unicode_csv_data , dialect = None , * * kwargs ) : import csv dialect = dialect or csv . excel if is_py3 : # Python3 supports encoding by default, so just return the object for row in csv . reader ( unicode_csv_data , dialect = dialect , * * kwargs ) : yield [ cell for cell in row ] else : # csv.py doesn't do Unicode; encode temporarily as UTF-8: reader = csv . reader ( utf_8_encoder ( unicode_csv_data ) , dialect = dialect , * * kwargs ) for row in reader : # decode UTF-8 back to Unicode, cell by cell: yield [ unicode ( cell , \"utf-8\" ) for cell in row ]", "docstring": "csv . reader doesn t support Unicode input so need to use some tricks to work around this .", "idx": 169963}
{"code": "def signal_handler ( sig , frame ) : print ( '\\nYou pressed Ctrl+C' ) if board is not None : board . send_reset ( ) board . shutdown ( ) sys . exit ( 0 )", "docstring": "Helper method to shutdown the RedBot if Ctrl - c is pressed", "idx": 140808}
{"code": "def edit_dedicated_fwl_rules ( self , firewall_id , rules ) : mask = ( 'mask[networkVlan[firewallInterfaces' '[firewallContextAccessControlLists]]]' ) svc = self . client [ 'Network_Vlan_Firewall' ] fwl = svc . getObject ( id = firewall_id , mask = mask ) network_vlan = fwl [ 'networkVlan' ] for fwl1 in network_vlan [ 'firewallInterfaces' ] : if fwl1 [ 'name' ] == 'inside' : continue for control_list in fwl1 [ 'firewallContextAccessControlLists' ] : if control_list [ 'direction' ] == 'out' : continue fwl_ctx_acl_id = control_list [ 'id' ] template = { 'firewallContextAccessControlListId' : fwl_ctx_acl_id , 'rules' : rules } svc = self . client [ 'Network_Firewall_Update_Request' ] return svc . createObject ( template )", "docstring": "Edit the rules for dedicated firewall .", "idx": 234496}
{"code": "def search_prefix ( self ) : # extract operator if 'operator' in request . json : operator = request . json [ 'operator' ] else : operator = 'equals' # fetch attributes from request.json attr = XhrController . extract_prefix_attr ( request . json ) # build query dict n = 0 q = { } for key , val in attr . items ( ) : if n == 0 : q = { 'operator' : operator , 'val1' : key , 'val2' : val } else : q = { 'operator' : 'and' , 'val1' : { 'operator' : operator , 'val1' : key , 'val2' : val } , 'val2' : q } n += 1 # extract search options search_opts = { } if 'children_depth' in request . json : search_opts [ 'children_depth' ] = request . json [ 'children_depth' ] if 'parents_depth' in request . json : search_opts [ 'parents_depth' ] = request . json [ 'parents_depth' ] if 'include_neighbors' in request . json : search_opts [ 'include_neighbors' ] = request . json [ 'include_neighbors' ] if 'max_result' in request . json : search_opts [ 'max_result' ] = request . json [ 'max_result' ] if 'offset' in request . json : search_opts [ 'offset' ] = request . json [ 'offset' ] try : result = Prefix . search ( q , search_opts ) except NipapError , e : return json . dumps ( { 'error' : 1 , 'message' : e . args , 'type' : type ( e ) . __name__ } ) return json . dumps ( result , cls = NipapJSONEncoder )", "docstring": "Search prefixes . Does not yet incorporate all the functions of the search_prefix API function due to difficulties with transferring a complete dict - to - sql encoded data structure .", "idx": 140419}
{"code": "def add_redact_annotation ( request , query ) : return query . annotate ( redact = django . db . models . Exists ( d1_gmn . app . models . Permission . objects . filter ( sciobj = django . db . models . OuterRef ( 'sciobj' ) , subject__subject__in = request . all_subjects_set , level__gte = d1_gmn . app . auth . WRITE_LEVEL , ) , negated = True , ) )", "docstring": "Flag LogEntry records that require ipAddress and subject fields to be redacted before being returned to the client .", "idx": 45037}
{"code": "def get_temperature_from_pressure ( self ) : self . _init_pressure ( ) # Ensure pressure sensor is initialised temp = 0 data = self . _pressure . pressureRead ( ) if ( data [ 2 ] ) : # Temp valid temp = data [ 3 ] return temp", "docstring": "Returns the temperature in Celsius from the pressure sensor", "idx": 198300}
{"code": "def conv2d_with_blocks ( conv_input , conv_filter , strides , padding , h_blocks_dim = None , w_blocks_dim = None , name = None ) : filter_h_dim , filter_w_dim = conv_filter . shape . dims [ : 2 ] assert filter_h_dim . size % 2 == 1 assert filter_w_dim . size % 2 == 1 h_dim , w_dim = conv_input . shape . dims [ - 3 : - 1 ] # If h_blocks_dim and w_blocks_dim is not split, directly call conv2d. if h_blocks_dim is None and w_blocks_dim is None : return conv2d ( conv_input , conv_filter , strides , padding , name ) # Padding 'VALID' is not supported yet. if padding != \"SAME\" : raise NotImplementedError ( \"conv2d_with_blocks requires padding=SAME\" ) # Halo exchange for h_blocks and w_blocks. for blocks_dim , block_size_dim , halo_size in [ ( h_blocks_dim , h_dim , filter_h_dim . size // 2 ) , ( w_blocks_dim , w_dim , filter_w_dim . size // 2 ) ] : if halo_size > 0 : if blocks_dim is not None : conv_input = halo_exchange ( conv_input , blocks_dim , block_size_dim , halo_size ) else : conv_input = pad ( conv_input , [ halo_size , halo_size ] , block_size_dim . name ) return conv2d ( conv_input , conv_filter , strides , \"VALID\" , name )", "docstring": "conv2d operation with spatial partitioning .", "idx": 222752}
{"code": "def defvar ( varname ) : if 'pyraf' in sys . modules : #ONLY if pyraf is already loaded, import iraf into the namespace from pyraf import iraf else : # else set iraf to None so it knows to not use iraf's environment iraf = None if iraf : _irafdef = iraf . envget ( varname ) else : _irafdef = 0 return varname in _varDict or varname in os . environ or _irafdef", "docstring": "Returns true if CL variable is defined .", "idx": 193557}
{"code": "def cleanup_event_loop ( self ) : for task in asyncio . Task . all_tasks ( loop = self . loop ) : if self . debug : warnings . warn ( 'Cancelling task: %s' % task ) task . _log_destroy_pending = False task . cancel ( ) self . loop . close ( ) self . loop . set_exception_handler ( self . loop_exception_handler_save ) self . loop_exception_handler_save = None self . loop_policy = None self . loop = None", "docstring": "Cleanup an event loop and close it down forever .", "idx": 94487}
{"code": "def reset_window_layout ( self ) : answer = QMessageBox . warning ( self , _ ( \"Warning\" ) , _ ( \"Window layout will be reset to default settings: \" \"this affects window position, size and dockwidgets.\\n\" \"Do you want to continue?\" ) , QMessageBox . Yes | QMessageBox . No ) if answer == QMessageBox . Yes : self . setup_layout ( default = True )", "docstring": "Reset window layout to default", "idx": 170791}
{"code": "def list_metric_defs_for_resource ( access_token , subscription_id , resource_group , resource_provider , resource_type , resource_name ) : endpoint = '' . join ( [ get_rm_endpoint ( ) , '/subscriptions/' , subscription_id , '/resourceGroups/' , resource_group , '/providers/' , resource_provider , '/' , resource_type , '/' , resource_name , '/providers/microsoft.insights' , '/metricdefinitions?api-version=' , INSIGHTS_METRICS_API ] ) return do_get ( endpoint , access_token )", "docstring": "List the monitoring metric definitions for a resource .", "idx": 110297}
{"code": "def apply_signal ( signal_function , volume_signal , ) : # How many timecourses are there within the signal_function timepoints = signal_function . shape [ 0 ] timecourses = signal_function . shape [ 1 ] # Preset volume signal = np . zeros ( [ volume_signal . shape [ 0 ] , volume_signal . shape [ 1 ] , volume_signal . shape [ 2 ] , timepoints ] ) # Find all the non-zero voxels in the brain idxs = np . where ( volume_signal != 0 ) if timecourses == 1 : # If there is only one time course supplied then duplicate it for # every voxel signal_function = np . matlib . repmat ( signal_function , 1 , len ( idxs [ 0 ] ) ) elif len ( idxs [ 0 ] ) != timecourses : raise IndexError ( 'The number of non-zero voxels in the volume and ' 'the number of timecourses does not match. Aborting' ) # For each coordinate with a non zero voxel, fill in the timecourse for # that voxel for idx_counter in range ( len ( idxs [ 0 ] ) ) : x = idxs [ 0 ] [ idx_counter ] y = idxs [ 1 ] [ idx_counter ] z = idxs [ 2 ] [ idx_counter ] # Pull out the function for this voxel signal_function_temp = signal_function [ : , idx_counter ] # Multiply the voxel value by the function timecourse signal [ x , y , z , : ] = volume_signal [ x , y , z ] * signal_function_temp return signal", "docstring": "Combine the signal volume with its timecourse", "idx": 204421}
{"code": "def reset ( self ) : animation_gen = self . _frame_function ( * self . _animation_args , * * self . _animation_kwargs ) self . _current_generator = itertools . cycle ( util . concatechain ( animation_gen , self . _back_up_generator ) )", "docstring": "Reset the current animation generator .", "idx": 96344}
{"code": "def _compute_mean ( self , C , mag , rhypo , hypo_depth , mean , idx ) : mean [ idx ] = ( C [ 'C1' ] + C [ 'C2' ] * mag + C [ 'C3' ] * np . log ( rhypo [ idx ] + C [ 'C4' ] * np . exp ( C [ 'C5' ] * mag ) ) + C [ 'C6' ] * hypo_depth )", "docstring": "Compute mean value according to equations 10 and 11 page 226 .", "idx": 213963}
{"code": "def check_array ( array , force_2d = False , n_feats = None , ndim = None , min_samples = 1 , name = 'Input data' , verbose = True ) : # make array if force_2d : array = make_2d ( array , verbose = verbose ) ndim = 2 else : array = np . array ( array ) # cast to float dtype = array . dtype if dtype . kind not in [ 'i' , 'f' ] : try : array = array . astype ( 'float' ) except ValueError as e : raise ValueError ( '{} must be type int or float, ' 'but found type: {}\\n' 'Try transforming data with a LabelEncoder first.' . format ( name , dtype . type ) ) # check finite if not ( np . isfinite ( array ) . all ( ) ) : raise ValueError ( '{} must not contain Inf nor NaN' . format ( name ) ) # check ndim if ndim is not None : if array . ndim != ndim : raise ValueError ( '{} must have {} dimensions. ' 'found shape {}' . format ( name , ndim , array . shape ) ) # check n_feats if n_feats is not None : m = array . shape [ 1 ] if m != n_feats : raise ValueError ( '{} must have {} features, ' 'but found {}' . format ( name , n_feats , m ) ) # minimum samples n = array . shape [ 0 ] if n < min_samples : raise ValueError ( '{} should have at least {} samples, ' 'but found {}' . format ( name , min_samples , n ) ) return array", "docstring": "tool to perform basic data validation . called by check_X and check_y .", "idx": 220439}
{"code": "def append ( self , name , value ) : with self . pipe as pipe : return pipe . append ( self . redis_key ( name ) , self . valueparse . encode ( value ) )", "docstring": "Appends the string value to the value at key . If key doesn t already exist create it with a value of value . Returns the new length of the value at key .", "idx": 73471}
{"code": "def _reset_errors ( self , msg = None ) : if msg is not None and msg in self . _errors : del self . _errors [ msg ] else : self . _errors = { }", "docstring": "Resets the logging throttle cache so the next error is emitted regardless of the value in self . server_error_interval", "idx": 217208}
{"code": "def check_status ( status , expected , path , headers = None , resp_headers = None , body = None , extras = None ) : if status in expected : return msg = ( 'Expect status %r from Google Storage. But got status %d.\\n' 'Path: %r.\\n' 'Request headers: %r.\\n' 'Response headers: %r.\\n' 'Body: %r.\\n' 'Extra info: %r.\\n' % ( expected , status , path , headers , resp_headers , body , extras ) ) if status == httplib . UNAUTHORIZED : raise AuthorizationError ( msg ) elif status == httplib . FORBIDDEN : raise ForbiddenError ( msg ) elif status == httplib . NOT_FOUND : raise NotFoundError ( msg ) elif status == httplib . REQUEST_TIMEOUT : raise TimeoutError ( msg ) elif status == httplib . REQUESTED_RANGE_NOT_SATISFIABLE : raise InvalidRange ( msg ) elif ( status == httplib . OK and 308 in expected and httplib . OK not in expected ) : raise FileClosedError ( msg ) elif status >= 500 : raise ServerError ( msg ) else : raise FatalError ( msg )", "docstring": "Check HTTP response status is expected .", "idx": 212241}
{"code": "def set_trace ( self , frame = None , break_ = True ) : # We are already tracing, do nothing trace_log . info ( 'Setting trace %s (stepping %s) (current_trace: %s)' % ( pretty_frame ( frame or sys . _getframe ( ) . f_back ) , self . stepping , sys . gettrace ( ) ) ) if self . stepping or self . closed : return self . reset ( ) trace = ( self . trace_dispatch if trace_log . level >= 30 else self . trace_debug_dispatch ) trace_frame = frame = frame or sys . _getframe ( ) . f_back while frame : frame . f_trace = trace frame = frame . f_back self . state = Step ( trace_frame ) if break_ else Running ( trace_frame ) sys . settrace ( trace )", "docstring": "Break at current state", "idx": 207179}
{"code": "def moving_average ( self , sampling_period , window_size = None , start = None , end = None , placement = 'center' , pandas = False ) : start , end , mask = self . _check_boundaries ( start , end ) # default to sampling_period if not given if window_size is None : window_size = sampling_period sampling_period = self . _check_regularization ( start , end , sampling_period ) # convert to datetime if the times are datetimes full_window = window_size * 1. # convert to float if int or do nothing half_window = full_window / 2. # divide by 2 if ( isinstance ( start , datetime . datetime ) and not isinstance ( full_window , datetime . timedelta ) ) : half_window = datetime . timedelta ( seconds = half_window ) full_window = datetime . timedelta ( seconds = full_window ) result = [ ] current_time = start while current_time <= end : if placement == 'center' : window_start = current_time - half_window window_end = current_time + half_window elif placement == 'left' : window_start = current_time window_end = current_time + full_window elif placement == 'right' : window_start = current_time - full_window window_end = current_time else : msg = 'unknown placement \"{}\"' . format ( placement ) raise ValueError ( msg ) # calculate mean over window and add (t, v) tuple to list try : mean = self . mean ( window_start , window_end ) except TypeError as e : if 'NoneType' in str ( e ) : mean = None else : raise e result . append ( ( current_time , mean ) ) current_time += sampling_period # convert to pandas Series if pandas=True if pandas : try : import pandas as pd except ImportError : msg = \"can't have pandas=True if pandas is not installed\" raise ImportError ( msg ) result = pd . Series ( [ v for t , v in result ] , index = [ t for t , v in result ] , ) return result", "docstring": "Averaging over regular intervals", "idx": 205094}
{"code": "def _TerminateProcess ( self , process ) : pid = process . pid logger . warning ( 'Terminating process: (PID: {0:d}).' . format ( pid ) ) process . terminate ( ) # Wait for the process to exit. process . join ( timeout = self . _PROCESS_JOIN_TIMEOUT ) if process . is_alive ( ) : logger . warning ( 'Killing process: (PID: {0:d}).' . format ( pid ) ) self . _KillProcess ( pid )", "docstring": "Terminate a process .", "idx": 147206}
{"code": "def setTargetRange ( self , targetRange , padding = None ) : # viewBox.setRange doesn't accept an axis number :-( if self . axisNumber == X_AXIS : xRange , yRange = targetRange , None else : xRange , yRange = None , targetRange # Do not set disableAutoRange to True in setRange; it triggers 'one last' auto range. # This is why the viewBox' autorange must be False at construction. self . viewBox . setRange ( xRange = xRange , yRange = yRange , padding = padding , update = False , disableAutoRange = False )", "docstring": "Sets the range of the target .", "idx": 109479}
{"code": "def contains ( self , value ) : str_value = StringConverter . to_nullable_string ( value ) for element in self : str_element = StringConverter . to_string ( element ) if str_value == None and str_element == None : return True if str_value == None or str_element == None : continue if str_value == str_element : return True return False", "docstring": "Checks if this array contains a value . The check uses direct comparison between elements and the specified value .", "idx": 90576}
{"code": "def target_names ( targets ) : names = [ ] for entry in targets : if isinstance ( entry , ast . Name ) : names . append ( entry . id ) elif isinstance ( entry , ast . Tuple ) : for element in entry . elts : if isinstance ( element , ast . Name ) : names . append ( element . id ) return names", "docstring": "Retrieves the target names", "idx": 84706}
{"code": "def dispatch ( self , request , * args , * * kwargs ) : lessonSession = request . session . get ( PRIVATELESSON_VALIDATION_STR , { } ) try : self . lesson = PrivateLessonEvent . objects . get ( id = lessonSession . get ( 'lesson' ) ) except ( ValueError , ObjectDoesNotExist ) : messages . error ( request , _ ( 'Invalid lesson identifier passed to sign-up form.' ) ) return HttpResponseRedirect ( reverse ( 'bookPrivateLesson' ) ) expiry = parse_datetime ( lessonSession . get ( 'expiry' , '' ) , ) if not expiry or expiry < timezone . now ( ) : messages . info ( request , _ ( 'Your registration session has expired. Please try again.' ) ) return HttpResponseRedirect ( reverse ( 'bookPrivateLesson' ) ) self . payAtDoor = lessonSession . get ( 'payAtDoor' , False ) return super ( PrivateLessonStudentInfoView , self ) . dispatch ( request , * args , * * kwargs )", "docstring": "Handle the session data passed by the prior view .", "idx": 236461}
{"code": "def _get_queue_types ( fed_arrays , data_sources ) : try : return [ data_sources [ n ] . dtype for n in fed_arrays ] except KeyError as e : raise ValueError ( \"Array '{k}' has no data source!\" . format ( k = e . message ) ) , None , sys . exc_info ( ) [ 2 ]", "docstring": "Given a list of arrays to feed in fed_arrays return a list of associated queue types obtained from tuples in the data_sources dictionary", "idx": 187918}
{"code": "def on_retry ( self , exc , task_id , args , kwargs , einfo ) : super ( LoggedTask , self ) . on_retry ( exc , task_id , args , kwargs , einfo ) log . warning ( '[{}] retried due to {}' . format ( task_id , getattr ( einfo , 'traceback' , None ) ) )", "docstring": "Capture the exception that caused the task to be retried if any .", "idx": 12680}
{"code": "def wrap_results_for_axis ( self ) : results = self . results # we have requested to expand if self . result_type == 'expand' : result = self . infer_to_same_shape ( ) # we have a non-series and don't want inference elif not isinstance ( results [ 0 ] , ABCSeries ) : from pandas import Series result = Series ( results ) result . index = self . res_index # we may want to infer results else : result = self . infer_to_same_shape ( ) return result", "docstring": "return the results for the columns", "idx": 160420}
{"code": "def filter_children ( self , ctype : ContentType = None ) -> List [ SchemaNode ] : if ctype is None : ctype = self . content_type ( ) return [ c for c in self . children if not isinstance ( c , ( RpcActionNode , NotificationNode ) ) and c . content_type ( ) . value & ctype . value != 0 ]", "docstring": "Return receiver s children based on content type .", "idx": 119725}
{"code": "def make_router ( * routings ) : routes = [ ] for routing in routings : methods , regex , app = routing [ : 3 ] if isinstance ( methods , basestring ) : methods = ( methods , ) vars = routing [ 3 ] if len ( routing ) >= 4 else { } routes . append ( ( methods , re . compile ( unicode ( regex ) ) , app , vars ) ) def router ( environ , start_response ) : \"\"\"Dispatch request to controllers.\"\"\" req = webob . Request ( environ ) split_path_info = req . path_info . split ( '/' ) if split_path_info [ 0 ] : # When path_info doesn't start with a \"/\" this is an error or a attack => Reject request. # An example of an URL with such a invalid path_info: http://127.0.0.1http%3A//127.0.0.1%3A80/result?... ctx = contexts . Ctx ( req ) headers = wsgihelpers . handle_cross_origin_resource_sharing ( ctx ) return wsgihelpers . respond_json ( ctx , dict ( apiVersion = 1 , error = dict ( code = 400 , # Bad Request message = ctx . _ ( u\"Invalid path: {0}\" ) . format ( req . path_info ) , ) , ) , headers = headers , ) ( environ , start_response ) for methods , regex , app , vars in routes : match = regex . match ( req . path_info ) if match is not None : if methods is not None and req . method not in methods : ctx = contexts . Ctx ( req ) headers = wsgihelpers . handle_cross_origin_resource_sharing ( ctx ) return wsgihelpers . respond_json ( ctx , dict ( apiVersion = 1 , error = dict ( code = 405 , message = ctx . _ ( u\"You cannot use HTTP {} to access this URL. Use one of {}.\" ) . format ( req . method , methods ) , ) , ) , headers = headers , ) ( environ , start_response ) if getattr ( req , 'urlvars' , None ) is None : req . urlvars = { } req . urlvars . update ( match . groupdict ( ) ) req . urlvars . update ( vars ) req . script_name += req . path_info [ : match . end ( ) ] req . path_info = req . path_info [ match . end ( ) : ] return app ( req . environ , start_response ) ctx = contexts . Ctx ( req ) headers = wsgihelpers . handle_cross_origin_resource_sharing ( ctx ) return wsgihelpers . respond_json ( ctx , dict ( apiVersion = 1 , error = dict ( code = 404 , # Not Found message = ctx . _ ( u\"Path not found: {0}\" ) . format ( req . path_info ) , ) , ) , headers = headers , ) ( environ , start_response ) return router", "docstring": "Return a WSGI application that dispatches requests to controllers", "idx": 72702}
{"code": "def delete_version ( self , project , version ) : url = self . _build_url ( constants . DELETE_VERSION_ENDPOINT ) data = { 'project' : project , 'version' : version } self . client . post ( url , data = data , timeout = self . timeout ) return True", "docstring": "Deletes a specific version of a project . First class maps to Scrapyd s delete version endpoint .", "idx": 212673}
{"code": "async def write ( self , chunk : bytes , * , drain : bool = True , LIMIT : int = 0x10000 ) -> None : if self . _on_chunk_sent is not None : await self . _on_chunk_sent ( chunk ) if self . _compress is not None : chunk = self . _compress . compress ( chunk ) if not chunk : return if self . length is not None : chunk_len = len ( chunk ) if self . length >= chunk_len : self . length = self . length - chunk_len else : chunk = chunk [ : self . length ] self . length = 0 if not chunk : return if chunk : if self . chunked : chunk_len_pre = ( '%x\\r\\n' % len ( chunk ) ) . encode ( 'ascii' ) chunk = chunk_len_pre + chunk + b'\\r\\n' self . _write ( chunk ) if self . buffer_size > LIMIT and drain : self . buffer_size = 0 await self . drain ( )", "docstring": "Writes chunk of data to a stream .", "idx": 166831}
{"code": "def quote_split ( sep , string ) : if len ( sep ) != 1 : raise Exception ( \"Separation string must be one character long\" ) retlist = [ ] squote = False dquote = False left = 0 i = 0 while i < len ( string ) : if string [ i ] == '\"' and not dquote : if not squote : squote = True elif ( i + 1 ) < len ( string ) and string [ i + 1 ] == '\"' : i += 1 else : squote = False elif string [ i ] == \"'\" and not squote : if not dquote : dquote = True elif ( i + 1 ) < len ( string ) and string [ i + 1 ] == \"'\" : i += 1 else : dquote = False elif string [ i ] == sep and not dquote and not squote : retlist . append ( string [ left : i ] ) left = i + 1 i += 1 retlist . append ( string [ left : ] ) return retlist", "docstring": "Splits the strings into pieces divided by sep when sep in not inside quotes .", "idx": 232854}
{"code": "def concat_aligned ( items ) : if len ( items ) == 0 : return [ ] elif len ( items ) == 1 : # we assume the input is aligned. In any case, it doesn't help # performance to force align it since that incurs a needless copy. return items [ 0 ] elif ( isinstance ( items [ 0 ] , np . ndarray ) and items [ 0 ] . dtype in [ np . float32 , np . float64 , np . uint8 ] ) : dtype = items [ 0 ] . dtype flat = aligned_array ( sum ( s . size for s in items ) , dtype ) batch_dim = sum ( s . shape [ 0 ] for s in items ) new_shape = ( batch_dim , ) + items [ 0 ] . shape [ 1 : ] output = flat . reshape ( new_shape ) assert output . ctypes . data % 64 == 0 , output . ctypes . data np . concatenate ( items , out = output ) return output else : return np . concatenate ( items )", "docstring": "Concatenate arrays ensuring the output is 64 - byte aligned .", "idx": 164440}
{"code": "def _run_spellcheck_linter ( matched_filenames , cache_dir , show_lint_files ) : from polysquarelinter import lint_spelling_only as lint from prospector . message import Message , Location for filename in matched_filenames : _debug_linter_status ( \"spellcheck-linter\" , filename , show_lint_files ) return_dict = dict ( ) def _custom_reporter ( error , file_path ) : line = error . line_offset + 1 key = _Key ( file_path , line , \"file/spelling_error\" ) loc = Location ( file_path , None , None , line , 0 ) # suppress(protected-access) desc = lint . _SPELLCHECK_MESSAGES [ error . error_type ] . format ( error . word ) return_dict [ key ] = Message ( \"spellcheck-linter\" , \"file/spelling_error\" , loc , desc ) # suppress(protected-access,unused-attribute) lint . _report_spelling_error = _custom_reporter lint . main ( [ \"--spellcheck-cache=\" + os . path . join ( cache_dir , \"spelling\" ) , \"--stamp-file-path=\" + os . path . join ( cache_dir , \"jobstamps\" , \"polysquarelinter\" ) , \"--technical-terms=\" + os . path . join ( cache_dir , \"technical-terms\" ) , ] + matched_filenames ) return return_dict", "docstring": "Run spellcheck - linter on matched_filenames .", "idx": 99891}
{"code": "def __execute_rot ( self , surface ) : self . image = pygame . transform . rotate ( surface , self . __rotation ) self . __resize_surface_extents ( )", "docstring": "Executes the rotating operation", "idx": 70226}
{"code": "def _endCodeIfNeeded ( line , inCodeBlock ) : assert isinstance ( line , str ) if inCodeBlock : line = '# @endcode{0}{1}' . format ( linesep , line . rstrip ( ) ) inCodeBlock = False return line , inCodeBlock", "docstring": "Simple routine to append end code marker if needed .", "idx": 30993}
{"code": "def _exponential_kamb ( cos_dist , sigma = 3 ) : n = float ( cos_dist . size ) f = 2 * ( 1.0 + n / sigma ** 2 ) count = np . exp ( f * ( cos_dist - 1 ) ) units = np . sqrt ( n * ( f / 2.0 - 1 ) / f ** 2 ) return count , units", "docstring": "Kernel function from Vollmer for exponential smoothing .", "idx": 54039}
{"code": "def getImportFromObjects ( node ) : somenames = [ x . asname for x in node . names if x . asname ] othernames = [ x . name for x in node . names if not x . asname ] return somenames + othernames", "docstring": "Returns a list of objects referenced by import from node", "idx": 58147}
{"code": "def filter ( self , criteria : Q , offset : int = 0 , limit : int = 10 , order_by : list = ( ) ) : if criteria . children : items = list ( self . _filter ( criteria , self . conn [ 'data' ] [ self . schema_name ] ) . values ( ) ) else : items = list ( self . conn [ 'data' ] [ self . schema_name ] . values ( ) ) # Sort the filtered results based on the order_by clause for o_key in order_by : reverse = False if o_key . startswith ( '-' ) : reverse = True o_key = o_key [ 1 : ] items = sorted ( items , key = itemgetter ( o_key ) , reverse = reverse ) result = ResultSet ( offset = offset , limit = limit , total = len ( items ) , items = items [ offset : offset + limit ] ) return result", "docstring": "Read the repository and return results as per the filer", "idx": 47089}
{"code": "def get_prebuilt_targets ( build_context ) : logger . info ( 'Scanning for cached base images' ) # deps that are part of cached based images contained_deps = set ( ) # deps that are needed by images that are going to be built, # but are not part of their base images required_deps = set ( ) # mapping from target name to set of all its deps (descendants) cached_descendants = CachedDescendants ( build_context . target_graph ) for target_name , target in build_context . targets . items ( ) : if 'image_caching_behavior' not in target . props : continue image_name = get_image_name ( target ) image_tag = target . props . image_tag icb = ImageCachingBehavior ( image_name , image_tag , target . props . image_caching_behavior ) target . image_id = handle_build_cache ( build_context . conf , image_name , image_tag , icb ) if target . image_id : # mark deps of cached base image as \"contained\" image_deps = cached_descendants . get ( target_name ) contained_deps . update ( image_deps ) contained_deps . add ( target . name ) else : # mark deps of image that is going to be built # (and are not deps of its base image) as \"required\" image_deps = cached_descendants . get ( target_name ) base_image_deps = cached_descendants . get ( target . props . base_image ) required_deps . update ( image_deps - base_image_deps ) return contained_deps - required_deps", "docstring": "Return set of target names that are contained within cached base images", "idx": 44154}
{"code": "def sg_argmin ( tensor , opt ) : opt += tf . sg_opt ( axis = tensor . get_shape ( ) . ndims - 1 ) return tf . argmin ( tensor , opt . axis , opt . name )", "docstring": "r Returns the indices of the minimum values along the specified axis .", "idx": 246247}
