{"input": "// Serialize the CHD. The serialized form is conducive to mmapped access. See\n// the Mmap function for details. [SEP] func (c *CHD) Write(w io.Writer) error {\n\twrite := func(nd ...interface{}) error {\n\t\tfor _, d := range nd {\n\t\t\tif err := binary.Write(w, binary.LittleEndian, d); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\tdata := []interface{}{\n\t\tuint32(len(c.r)), c.r,\n\t\tuint32(len(c.indices)), c.indices,\n\t\tuint32(len(c.keys)),\n\t}\n\n\tif err := write(data...); err != nil {\n\t\treturn err\n\t}\n\n\tfor i := range c.keys {\n\t\tk, v := c.keys[i], c.values[i]\n\t\tif err := write(uint32(len(k)), uint32(len(v))); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := w.Write(k); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := w.Write(v); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Return an expression evaluating if a filter should be used (based on active\n// criteria). [SEP] func activeCriteria(filter []string) string {\n\texpr := \"\"\n\tfor i, name := range filter {\n\t\tif i > 0 {\n\t\t\texpr += \" && \"\n\t\t}\n\t\texpr += fmt.Sprintf(\"criteria[%q] != nil\", name)\n\t}\n\n\treturn expr\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Infom executes the same function on the default Base instance [SEP] func Infom(m *Attrs, msg string, a ...interface{}) error {\n\treturn curDefault.Infom(m, msg, a...)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Accept implements the corresponding method of net.Listener for\n// TestListener [SEP] func (l *TestListener) Accept() (net.Conn, error) {\n\tconn := <-l.connCh\n\tif conn == nil {\n\t\treturn nil, errors.New(\"Accept() has already been called on this TestListener\")\n\t}\n\treturn conn, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// DetermineRaftNode figures out what raft node ID and address we have, if any.\n//\n// This decision is based on the value of the cluster.https_address config key\n// and on the rows in the raft_nodes table, both stored in the node-level\n// SQLite database.\n//\n// The following rules are applied:\n//\n// - If no cluster.https_address config key is set, this is a non-clustered node\n//   and the returned RaftNode will have ID 1 but no address, to signal that\n//   the node should setup an in-memory raft cluster where the node itself\n//   is the only member and leader.\n//\n// - If cluster.https_address config key is set, but there is no row in the\n//   raft_nodes table, this is a brand new clustered node that is joining a\n//   cluster, and same behavior as the previous case applies.\n//\n// - If cluster.https_address config key is set and there is at least one row\n//   in the raft_nodes table, then this node is considered a raft node if\n//   cluster.https_address matches one of the rows in raft_nodes. In that case,\n//   the matching db.RaftNode row is returned, otherwise nil. [SEP] func DetermineRaftNode(tx *db.NodeTx) (*db.RaftNode, error) {\n\tconfig, err := ConfigLoad(tx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\taddress := config.ClusterAddress()\n\n\t// If cluster.https_address is the empty string, then this LXD instance is\n\t// not running in clustering mode.\n\tif address == \"\" {\n\t\treturn &db.RaftNode{ID: 1}, nil\n\t}\n\n\tnodes, err := tx.RaftNodes()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If cluster.https_address and the raft_nodes table is not populated,\n\t// this must be a joining node.\n\tif len(nodes) == 0 {\n\t\treturn &db.RaftNode{ID: 1}, nil\n\t}\n\n\t// Try to find a matching node.\n\tfor _, node := range nodes {\n\t\tif node.Address == address {\n\t\t\treturn &node, nil\n\t\t}\n\t}\n\n\treturn nil, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// RegisterCustomDecoder allows to define custom type decoders. [SEP] func RegisterCustomDecoder(fn CustomTypeDecoder, types []interface{}, fields []interface{}) {\n\trawFunc := (func([]string) (interface{}, error))(fn)\n\tdecoder.RegisterCustomType(rawFunc, types, fields)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// TODO: We could add support for multiple images in a single archive, so\n//       that people could use docker-archive:opensuse.tar:opensuse:leap as\n//       the source of an image.\n// \tTo do for both the NewSourceFromFile and NewSourceFromStream functions\n// NewSourceFromFile returns a tarfile.Source for the specified path. [SEP] func NewSourceFromFile(path string) (*Source, error) {\n\tfile, err := os.Open(path)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"error opening file %q\", path)\n\t}\n\tdefer file.Close()\n\n\t// If the file is already not compressed we can just return the file itself\n\t// as a source. Otherwise we pass the stream to NewSourceFromStream.\n\tstream, isCompressed, err := compression.AutoDecompress(file)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Error detecting compression for file %q\", path)\n\t}\n\tdefer stream.Close()\n\tif !isCompressed {\n\t\treturn &Source{\n\t\t\ttarPath: path,\n\t\t}, nil\n\t}\n\treturn NewSourceFromStream(stream)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// JavaScript renders the named files using the 'application/javascript'\n// content type and the github.com/gobuffalo/plush\n// package for templating. If more than 1 file is provided\n// the second file will be considered a \"layout\" file\n// and the first file will be the \"content\" file which will\n// be placed into the \"layout\" using \"<%= yield %>\". If no\n// second file is provided and an `JavaScriptLayout` is specified\n// in the options, then that layout file will be used\n// automatically. [SEP] func (e *Engine) JavaScript(names ...string) Renderer {\n\tif e.JavaScriptLayout != \"\" && len(names) == 1 {\n\t\tnames = append(names, e.JavaScriptLayout)\n\t}\n\thr := &templateRenderer{\n\t\tEngine:      e,\n\t\tcontentType: \"application/javascript\",\n\t\tnames:       names,\n\t}\n\treturn hr\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// dial configures and dials any grpc balancer target. [SEP] func (c *Client) dial(target string, creds *credentials.TransportCredentials, dopts ...grpc.DialOption) (*grpc.ClientConn, error) {\n\topts, err := c.dialSetupOpts(creds, dopts...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to configure dialer: %v\", err)\n\t}\n\n\tif c.Username != \"\" && c.Password != \"\" {\n\t\tc.tokenCred = &authTokenCredential{\n\t\t\ttokenMu: &sync.RWMutex{},\n\t\t}\n\n\t\tctx, cancel := c.ctx, func() {}\n\t\tif c.cfg.DialTimeout > 0 {\n\t\t\tctx, cancel = context.WithTimeout(ctx, c.cfg.DialTimeout)\n\t\t}\n\n\t\terr = c.getToken(ctx)\n\t\tif err != nil {\n\t\t\tif toErr(ctx, err) != rpctypes.ErrAuthNotEnabled {\n\t\t\t\tif err == ctx.Err() && ctx.Err() != c.ctx.Err() {\n\t\t\t\t\terr = context.DeadlineExceeded\n\t\t\t\t}\n\t\t\t\tcancel()\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else {\n\t\t\topts = append(opts, grpc.WithPerRPCCredentials(c.tokenCred))\n\t\t}\n\t\tcancel()\n\t}\n\n\topts = append(opts, c.cfg.DialOptions...)\n\n\tdctx := c.ctx\n\tif c.cfg.DialTimeout > 0 {\n\t\tvar cancel context.CancelFunc\n\t\tdctx, cancel = context.WithTimeout(c.ctx, c.cfg.DialTimeout)\n\t\tdefer cancel() // TODO: Is this right for cases where grpc.WithBlock() is not set on the dial options?\n\t}\n\n\tconn, err := grpc.DialContext(dctx, target, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn conn, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// PlotData plots the results onto a graph and saves the output to the given writer [SEP] func PlotData(interval time.Duration, r results.ResultSet, w io.Writer) {\n\tset := r.Reduce(interval)\n\tt := results.TabularResults{}\n\trows := t.Tabulate(set)\n\n\tseriesY := createYSeries(rows)\n\tseriesX := createXSeries(rows)\n\tticks := createXTicks(rows)\n\n\tgraph := chart.Chart{\n\t\tBackground: chart.Style{\n\t\t\tPadding: chart.Box{\n\t\t\t\tTop:    50,\n\t\t\t\tLeft:   25,\n\t\t\t\tRight:  25,\n\t\t\t\tBottom: 25,\n\t\t\t},\n\t\t\tFillColor: drawing.ColorFromHex(\"efefef\"),\n\t\t},\n\t\tXAxis: chart.XAxis{\n\t\t\tName:      \"Elapsed Time (s)\",\n\t\t\tNameStyle: chart.StyleShow(),\n\t\t\tStyle:     chart.StyleShow(),\n\t\t\tValueFormatter: func(v interface{}) string {\n\t\t\t\treturn fmt.Sprintf(\"%.0f\", v)\n\t\t\t},\n\t\t\tTicks: ticks,\n\t\t},\n\t\tYAxis: chart.YAxis{\n\t\t\tName:      \"Count\",\n\t\t\tNameStyle: chart.StyleShow(),\n\t\t\tStyle:     chart.StyleShow(),\n\t\t\tValueFormatter: func(v interface{}) string {\n\t\t\t\treturn fmt.Sprintf(\"%.0f\", v)\n\t\t\t},\n\t\t},\n\t\tYAxisSecondary: chart.YAxis{\n\t\t\tName:      \"Time (ms)\",\n\t\t\tNameStyle: chart.StyleShow(),\n\t\t\tStyle:     chart.StyleShow(),\n\t\t\tValueFormatter: func(v interface{}) string {\n\t\t\t\treturn fmt.Sprintf(\"%.2f\", v)\n\t\t\t},\n\t\t},\n\t\tSeries: []chart.Series{\n\t\t\tchart.ContinuousSeries{\n\t\t\t\tName:    \"Success\",\n\t\t\t\tXValues: seriesX[\"x\"],\n\t\t\t\tYValues: seriesY[\"y.success\"],\n\t\t\t\tStyle: chart.Style{\n\t\t\t\t\tShow:        true,                             //note; if we set ANY other properties, we must set this to true.\n\t\t\t\t\tStrokeColor: drawing.ColorGreen,               // will supercede defaults\n\t\t\t\t\tFillColor:   drawing.ColorGreen.WithAlpha(64), // will supercede defaults\n\t\t\t\t},\n\t\t\t},\n\t\t\tchart.ContinuousSeries{\n\t\t\t\tName:    \"Failure\",\n\t\t\t\tXValues: seriesX[\"x\"],\n\t\t\t\tYValues: seriesY[\"y.failure\"],\n\t\t\t\tStyle: chart.Style{\n\t\t\t\t\tShow:        true,                           //note; if we set ANY other properties, we must set this to true.\n\t\t\t\t\tStrokeColor: drawing.ColorRed,               // will supercede defaults\n\t\t\t\t\tFillColor:   drawing.ColorRed.WithAlpha(64), // will supercede defaults\n\t\t\t\t},\n\t\t\t},\n\t\t\tchart.ContinuousSeries{\n\t\t\t\tName:    \"Timeout\",\n\t\t\t\tXValues: seriesX[\"x\"],\n\t\t\t\tYValues: seriesY[\"y.timeout\"],\n\t\t\t\tStyle: chart.Style{\n\t\t\t\t\tShow:        true,                                         //note; if we set ANY other properties, we must set this to true.\n\t\t\t\t\tStrokeColor: drawing.ColorFromHex(\"FFD133\"),               // will supercede defaults\n\t\t\t\t\tFillColor:   drawing.ColorFromHex(\"FFD133\").WithAlpha(64), // will supercede defaults\n\t\t\t\t},\n\t\t\t},\n\t\t\tchart.ContinuousSeries{\n\t\t\t\tName:    \"Threads\",\n\t\t\t\tXValues: seriesX[\"x\"],\n\t\t\t\tYValues: seriesY[\"y.threads\"],\n\t\t\t\tStyle: chart.Style{\n\t\t\t\t\tShow:        true,                           //note; if we set ANY other properties, we must set this to true.\n\t\t\t\t\tStrokeColor: drawing.ColorFromHex(\"FF338D\"), // will supercede defaults\n\t\t\t\t},\n\t\t\t},\n\t\t\tchart.ContinuousSeries{\n\t\t\t\tYAxis:   chart.YAxisSecondary,\n\t\t\t\tName:    \"Request time (ms)\",\n\t\t\t\tXValues: seriesX[\"x\"],\n\t\t\t\tYValues: seriesY[\"y.request\"],\n\t\t\t\tStyle: chart.Style{\n\t\t\t\t\tShow:        true,              //note; if we set ANY other properties, we must set this to true.\n\t\t\t\t\tStrokeColor: drawing.ColorBlue, // will supercede defaults\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\t//note we have to do this as a separate step because we need a reference to graph\n\tgraph.Elements = []chart.Renderable{\n\t\tchart.Legend(&graph),\n\t}\n\n\tgraph.Render(chart.PNG, w)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "/* Inializes IplImage header */ [SEP] func (img *IplImage) InitHeader(w, h, depth, channels, origin, align int) {\n\tC.cvInitImageHeader(\n\t\t(*C.IplImage)(img),\n\t\tC.cvSize(C.int(w), C.int(h)),\n\t\tC.int(depth),\n\t\tC.int(channels),\n\t\tC.int(origin),\n\t\tC.int(align),\n\t)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// BlobInfoFromOCI1Descriptor returns a types.BlobInfo based on the input OCI1 descriptor. [SEP] func BlobInfoFromOCI1Descriptor(desc imgspecv1.Descriptor) types.BlobInfo {\n\treturn types.BlobInfo{\n\t\tDigest:      desc.Digest,\n\t\tSize:        desc.Size,\n\t\tURLs:        desc.URLs,\n\t\tAnnotations: desc.Annotations,\n\t\tMediaType:   desc.MediaType,\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MarshalJSON supports json.Marshaler interface [SEP] func (v DeleteCookiesParams) MarshalJSON() ([]byte, error) {\n\tw := jwriter.Writer{}\n\teasyjsonC5a4559bEncodeGithubComChromedpCdprotoNetwork60(&w, v)\n\treturn w.Buffer.BuildBytes(), w.Error\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// List the caches for this `provisionerId`/`workerType` that should to be\n// purged if they are from before the time given in the response.\n//\n// This is intended to be used by workers to determine which caches to purge.\n//\n// See #purgeRequests [SEP] func (purgeCache *PurgeCache) PurgeRequests(provisionerId, workerType, since string) (*OpenPurgeRequestList, error) {\n\tv := url.Values{}\n\tif since != \"\" {\n\t\tv.Add(\"since\", since)\n\t}\n\tcd := tcclient.Client(*purgeCache)\n\tresponseObject, _, err := (&cd).APICall(nil, \"GET\", \"/purge-cache/\"+url.QueryEscape(provisionerId)+\"/\"+url.QueryEscape(workerType), new(OpenPurgeRequestList), v)\n\treturn responseObject.(*OpenPurgeRequestList), err\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Get Labels from Regexp matches [SEP] func getLabelsFromREMatches(matches [][]string) (labels []string) {\n\tfor _, match := range matches {\n\t\tfor _, label := range strings.Split(match[0], \" \")[1:] {\n\t\t\tlabel = strings.ToLower(match[1] + \"/\" + strings.TrimSpace(label))\n\t\t\tlabels = append(labels, label)\n\t\t}\n\t}\n\treturn\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// link is LINK from the papers. [SEP] func (d *ltDom) link(v, w vName) {\n\td.ancestor[w] = v\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// String returns the name of the operation. [SEP] func (o Operation) String() string {\n\tswitch o {\n\tcase List:\n\t\treturn \"List\"\n\tcase Find:\n\t\treturn \"Find\"\n\tcase Create:\n\t\treturn \"Create\"\n\tcase Update:\n\t\treturn \"Update\"\n\tcase Delete:\n\t\treturn \"Delete\"\n\tcase CollectionAction:\n\t\treturn \"CollectionAction\"\n\tcase ResourceAction:\n\t\treturn \"ResourceAction\"\n\t}\n\n\treturn \"\"\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Add will add a stream to the watcher. [SEP] func (w *Watcher) Add(stream *Stream) {\n\t// initialize model\n\tcoal.Init(stream.Model)\n\n\t// check existence\n\tif w.streams[stream.Name()] != nil {\n\t\tpanic(fmt.Sprintf(`spark: stream with name \"%s\" already exists`, stream.Name()))\n\t}\n\n\t// save stream\n\tw.streams[stream.Name()] = stream\n\n\t// open stream\n\tcoal.OpenStream(stream.Store, stream.Model, nil, func(e coal.Event, id bson.ObjectId, m coal.Model, token []byte) {\n\t\t// ignore real deleted events when soft delete has been enabled\n\t\tif stream.SoftDelete && e == coal.Deleted {\n\t\t\treturn\n\t\t}\n\n\t\t// handle soft deleted documents\n\t\tif stream.SoftDelete && e == coal.Updated {\n\t\t\t// get soft delete field\n\t\t\tsoftDeleteField := coal.L(stream.Model, \"fire-soft-delete\", true)\n\n\t\t\t// get deleted time\n\t\t\tt := m.MustGet(softDeleteField).(*time.Time)\n\n\t\t\t// change type if document has been soft deleted\n\t\t\tif t != nil && !t.IsZero() {\n\t\t\t\te = coal.Deleted\n\t\t\t}\n\t\t}\n\n\t\t// create event\n\t\tevt := &Event{\n\t\t\tType:   e,\n\t\t\tID:     id,\n\t\t\tModel:  m,\n\t\t\tStream: stream,\n\t\t}\n\n\t\t// broadcast event\n\t\tw.manager.broadcast(evt)\n\t}, nil, func(err error) bool {\n\t\t// report error\n\t\tw.Reporter(err)\n\n\t\treturn true\n\t})\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Do executes Tethering.unbind against the provided context. [SEP] func (p *UnbindParams) Do(ctx context.Context) (err error) {\n\treturn cdp.Execute(ctx, CommandUnbind, p, nil)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MarshalJSON supports json.Marshaler interface [SEP] func (v GetScriptSourceReturns) MarshalJSON() ([]byte, error) {\n\tw := jwriter.Writer{}\n\teasyjsonC5a4559bEncodeGithubComChromedpCdprotoDebugger33(&w, v)\n\treturn w.Buffer.BuildBytes(), w.Error\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// newOpenShiftClientConfigLoadingRules is a modified copy of openshift/origin/pkg/cmd/cli/config.NewOpenShiftClientConfigLoadingRules.\n// NewOpenShiftClientConfigLoadingRules returns file priority loading rules for OpenShift.\n// 1. --config value\n// 2. if KUBECONFIG env var has a value, use it. Otherwise, ~/.kube/config file [SEP] func newOpenShiftClientConfigLoadingRules() *clientConfigLoadingRules {\n\tchain := []string{}\n\n\tenvVarFile := os.Getenv(\"KUBECONFIG\")\n\tif len(envVarFile) != 0 {\n\t\tchain = append(chain, filepath.SplitList(envVarFile)...)\n\t} else {\n\t\tchain = append(chain, recommendedHomeFile)\n\t}\n\n\treturn &clientConfigLoadingRules{\n\t\tPrecedence: chain,\n\t\t// REMOVED: Migration support; run (oc login) to trigger migration\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// NetworkInterfaceAddress returns the first non-loopback address of any of the\n// system network interfaces.\n//\n// Return the empty string if none is found. [SEP] func NetworkInterfaceAddress() string {\n\tifaces, err := net.Interfaces()\n\tif err != nil {\n\t\treturn \"\"\n\t}\n\tfor _, iface := range ifaces {\n\t\tif shared.IsLoopback(&iface) {\n\t\t\tcontinue\n\t\t}\n\t\taddrs, err := iface.Addrs()\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tif len(addrs) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\taddr, ok := addrs[0].(*net.IPNet)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\treturn addr.IP.String()\n\t}\n\treturn \"\"\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// ExecContainer requests that LXD spawns a command inside the container [SEP] func (r *ProtocolLXD) ExecContainer(containerName string, exec api.ContainerExecPost, args *ContainerExecArgs) (Operation, error) {\n\tif exec.RecordOutput {\n\t\tif !r.HasExtension(\"container_exec_recording\") {\n\t\t\treturn nil, fmt.Errorf(\"The server is missing the required \\\"container_exec_recording\\\" API extension\")\n\t\t}\n\t}\n\n\t// Send the request\n\top, _, err := r.queryOperation(\"POST\", fmt.Sprintf(\"/containers/%s/exec\", url.QueryEscape(containerName)), exec, \"\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\topAPI := op.Get()\n\n\t// Process additional arguments\n\tif args != nil {\n\t\t// Parse the fds\n\t\tfds := map[string]string{}\n\n\t\tvalue, ok := opAPI.Metadata[\"fds\"]\n\t\tif ok {\n\t\t\tvalues := value.(map[string]interface{})\n\t\t\tfor k, v := range values {\n\t\t\t\tfds[k] = v.(string)\n\t\t\t}\n\t\t}\n\n\t\t// Call the control handler with a connection to the control socket\n\t\tif args.Control != nil && fds[\"control\"] != \"\" {\n\t\t\tconn, err := r.GetOperationWebsocket(opAPI.ID, fds[\"control\"])\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tgo args.Control(conn)\n\t\t}\n\n\t\tif exec.Interactive {\n\t\t\t// Handle interactive sections\n\t\t\tif args.Stdin != nil && args.Stdout != nil {\n\t\t\t\t// Connect to the websocket\n\t\t\t\tconn, err := r.GetOperationWebsocket(opAPI.ID, fds[\"0\"])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\t// And attach stdin and stdout to it\n\t\t\t\tgo func() {\n\t\t\t\t\tshared.WebsocketSendStream(conn, args.Stdin, -1)\n\t\t\t\t\t<-shared.WebsocketRecvStream(args.Stdout, conn)\n\t\t\t\t\tconn.Close()\n\n\t\t\t\t\tif args.DataDone != nil {\n\t\t\t\t\t\tclose(args.DataDone)\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t} else {\n\t\t\t\tif args.DataDone != nil {\n\t\t\t\t\tclose(args.DataDone)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t// Handle non-interactive sessions\n\t\t\tdones := map[int]chan bool{}\n\t\t\tconns := []*websocket.Conn{}\n\n\t\t\t// Handle stdin\n\t\t\tif fds[\"0\"] != \"\" {\n\t\t\t\tconn, err := r.GetOperationWebsocket(opAPI.ID, fds[\"0\"])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tconns = append(conns, conn)\n\t\t\t\tdones[0] = shared.WebsocketSendStream(conn, args.Stdin, -1)\n\t\t\t}\n\n\t\t\t// Handle stdout\n\t\t\tif fds[\"1\"] != \"\" {\n\t\t\t\tconn, err := r.GetOperationWebsocket(opAPI.ID, fds[\"1\"])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tconns = append(conns, conn)\n\t\t\t\tdones[1] = shared.WebsocketRecvStream(args.Stdout, conn)\n\t\t\t}\n\n\t\t\t// Handle stderr\n\t\t\tif fds[\"2\"] != \"\" {\n\t\t\t\tconn, err := r.GetOperationWebsocket(opAPI.ID, fds[\"2\"])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tconns = append(conns, conn)\n\t\t\t\tdones[2] = shared.WebsocketRecvStream(args.Stderr, conn)\n\t\t\t}\n\n\t\t\t// Wait for everything to be done\n\t\t\tgo func() {\n\t\t\t\tfor i, chDone := range dones {\n\t\t\t\t\t// Skip stdin, dealing with it separately below\n\t\t\t\t\tif i == 0 {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\t<-chDone\n\t\t\t\t}\n\n\t\t\t\tif fds[\"0\"] != \"\" {\n\t\t\t\t\tif args.Stdin != nil {\n\t\t\t\t\t\targs.Stdin.Close()\n\t\t\t\t\t}\n\n\t\t\t\t\t// Empty the stdin channel but don't block on it as\n\t\t\t\t\t// stdin may be stuck in Read()\n\t\t\t\t\tgo func() {\n\t\t\t\t\t\t<-dones[0]\n\t\t\t\t\t}()\n\t\t\t\t}\n\n\t\t\t\tfor _, conn := range conns {\n\t\t\t\t\tconn.Close()\n\t\t\t\t}\n\n\t\t\t\tif args.DataDone != nil {\n\t\t\t\t\tclose(args.DataDone)\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\t}\n\n\treturn op, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// title: app unlock\n// path: /apps/{app}/lock\n// method: DELETE\n// produce: application/json\n// responses:\n//   200: Ok\n//   401: Unauthorized\n//   404: App not found [SEP] func forceDeleteLock(w http.ResponseWriter, r *http.Request, t auth.Token) (err error) {\n\tappName := r.URL.Query().Get(\":app\")\n\ta, err := getAppFromContext(appName, r)\n\tif err != nil {\n\t\treturn err\n\t}\n\tallowed := permission.Check(t, permission.PermAppAdminUnlock,\n\t\tcontextsForApp(&a)...,\n\t)\n\tif !allowed {\n\t\treturn permission.ErrUnauthorized\n\t}\n\tevt, err := event.New(&event.Opts{\n\t\tTarget:     appTarget(appName),\n\t\tKind:       permission.PermAppAdminUnlock,\n\t\tOwner:      t,\n\t\tCustomData: event.FormToCustomData(InputFields(r)),\n\t\tAllowed:    event.Allowed(permission.PermAppReadEvents, contextsForApp(&a)...),\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() { evt.Done(err) }()\n\tapp.ReleaseApplicationLock(a.Name)\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// makeAddressable returns a value that is always addressable.\n// It returns the input verbatim if it is already addressable,\n// otherwise it creates a new value and returns an addressable copy. [SEP] func makeAddressable(v reflect.Value) reflect.Value {\n\tif v.CanAddr() {\n\t\treturn v\n\t}\n\tvc := reflect.New(v.Type()).Elem()\n\tvc.Set(v)\n\treturn vc\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// ReloadData reloads configuration file from memory [SEP] func (c *ConfigFile) ReloadData(in io.Reader) (err error) {\n\tvar cfg *ConfigFile\n\tif len(c.fileNames) != 1 {\n\t\treturn fmt.Errorf(\"Multiple files loaded, unable to mix in-memory and file data\")\n\t}\n\n\tcfg, err = LoadFromReader(in)\n\tif err == nil {\n\t\t*c = *cfg\n\t}\n\treturn err\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "/*\nSetAttr will set the value for the attribute with the name key.  If the key\nalready exists it will be overwritten with the new value.\n*/ [SEP] func (b *Base) SetAttr(key string, value interface{}) {\n\tb.BaseAttrs.SetAttr(key, value)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// SetPresubmits updates c.Presubmits to jobs, after compiling and validating their regexes. [SEP] func (c *JobConfig) SetPresubmits(jobs map[string][]Presubmit) error {\n\tnj := map[string][]Presubmit{}\n\tfor k, v := range jobs {\n\t\tnj[k] = make([]Presubmit, len(v))\n\t\tcopy(nj[k], v)\n\t\tif err := SetPresubmitRegexes(nj[k]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tc.Presubmits = nj\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Gets the logFile and acquires and RLock() for the mmap. You must call RUnlock on the file\n// (if non-nil) [SEP] func (vlog *valueLog) getFileRLocked(fid uint32) (*logFile, error) {\n\tvlog.filesLock.RLock()\n\tdefer vlog.filesLock.RUnlock()\n\tret, ok := vlog.filesMap[fid]\n\tif !ok {\n\t\t// log file has gone away, will need to retry the operation.\n\t\treturn nil, ErrRetry\n\t}\n\tret.lock.RLock()\n\treturn ret, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// PrintCommitInfo pretty-prints commit info. [SEP] func PrintCommitInfo(w io.Writer, commitInfo *pfs.CommitInfo, fullTimestamps bool) {\n\tfmt.Fprintf(w, \"%s\\t\", commitInfo.Commit.Repo.Name)\n\tfmt.Fprintf(w, \"%s\\t\", commitInfo.Branch.Name)\n\tfmt.Fprintf(w, \"%s\\t\", commitInfo.Commit.ID)\n\tif commitInfo.ParentCommit != nil {\n\t\tfmt.Fprintf(w, \"%s\\t\", commitInfo.ParentCommit.ID)\n\t} else {\n\t\tfmt.Fprint(w, \"<none>\\t\")\n\t}\n\tif fullTimestamps {\n\t\tfmt.Fprintf(w, \"%s\\t\", commitInfo.Started.String())\n\t} else {\n\t\tfmt.Fprintf(w, \"%s\\t\", pretty.Ago(commitInfo.Started))\n\t}\n\tif commitInfo.Finished != nil {\n\t\tfmt.Fprintf(w, fmt.Sprintf(\"%s\\t\", pretty.TimeDifference(commitInfo.Started, commitInfo.Finished)))\n\t\tfmt.Fprintf(w, \"%s\\t\\n\", units.BytesSize(float64(commitInfo.SizeBytes)))\n\t} else {\n\t\tfmt.Fprintf(w, \"-\\t\")\n\t\t// Open commits don't have meaningful size information\n\t\tfmt.Fprintf(w, \"-\\t\\n\")\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// WithHTTPClient adds the HTTPClient to the patch apps app routes route params [SEP] func (o *PatchAppsAppRoutesRouteParams) WithHTTPClient(client *http.Client) *PatchAppsAppRoutesRouteParams {\n\to.SetHTTPClient(client)\n\treturn o\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// run is the root of the goroutines for managing a watcher client [SEP] func (w *watchGrpcStream) run() {\n\tvar wc pb.Watch_WatchClient\n\tvar closeErr error\n\n\t// substreams marked to close but goroutine still running; needed for\n\t// avoiding double-closing recvc on grpc stream teardown\n\tclosing := make(map[*watcherStream]struct{})\n\n\tdefer func() {\n\t\tw.closeErr = closeErr\n\t\t// shutdown substreams and resuming substreams\n\t\tfor _, ws := range w.substreams {\n\t\t\tif _, ok := closing[ws]; !ok {\n\t\t\t\tclose(ws.recvc)\n\t\t\t\tclosing[ws] = struct{}{}\n\t\t\t}\n\t\t}\n\t\tfor _, ws := range w.resuming {\n\t\t\tif _, ok := closing[ws]; ws != nil && !ok {\n\t\t\t\tclose(ws.recvc)\n\t\t\t\tclosing[ws] = struct{}{}\n\t\t\t}\n\t\t}\n\t\tw.joinSubstreams()\n\t\tfor range closing {\n\t\t\tw.closeSubstream(<-w.closingc)\n\t\t}\n\t\tw.wg.Wait()\n\t\tw.owner.closeStream(w)\n\t}()\n\n\t// start a stream with the etcd grpc server\n\tif wc, closeErr = w.newWatchClient(); closeErr != nil {\n\t\treturn\n\t}\n\n\tcancelSet := make(map[int64]struct{})\n\n\tvar cur *pb.WatchResponse\n\tfor {\n\t\tselect {\n\t\t// Watch() requested\n\t\tcase req := <-w.reqc:\n\t\t\tswitch wreq := req.(type) {\n\t\t\tcase *watchRequest:\n\t\t\t\toutc := make(chan WatchResponse, 1)\n\t\t\t\t// TODO: pass custom watch ID?\n\t\t\t\tws := &watcherStream{\n\t\t\t\t\tinitReq: *wreq,\n\t\t\t\t\tid:      -1,\n\t\t\t\t\toutc:    outc,\n\t\t\t\t\t// unbuffered so resumes won't cause repeat events\n\t\t\t\t\trecvc: make(chan *WatchResponse),\n\t\t\t\t}\n\n\t\t\t\tws.donec = make(chan struct{})\n\t\t\t\tw.wg.Add(1)\n\t\t\t\tgo w.serveSubstream(ws, w.resumec)\n\n\t\t\t\t// queue up for watcher creation/resume\n\t\t\t\tw.resuming = append(w.resuming, ws)\n\t\t\t\tif len(w.resuming) == 1 {\n\t\t\t\t\t// head of resume queue, can register a new watcher\n\t\t\t\t\twc.Send(ws.initReq.toPB())\n\t\t\t\t}\n\t\t\tcase *progressRequest:\n\t\t\t\twc.Send(wreq.toPB())\n\t\t\t}\n\n\t\t// new events from the watch client\n\t\tcase pbresp := <-w.respc:\n\t\t\tif cur == nil || pbresp.Created || pbresp.Canceled {\n\t\t\t\tcur = pbresp\n\t\t\t} else if cur != nil && cur.WatchId == pbresp.WatchId {\n\t\t\t\t// merge new events\n\t\t\t\tcur.Events = append(cur.Events, pbresp.Events...)\n\t\t\t\t// update \"Fragment\" field; last response with \"Fragment\" == false\n\t\t\t\tcur.Fragment = pbresp.Fragment\n\t\t\t}\n\n\t\t\tswitch {\n\t\t\tcase pbresp.Created:\n\t\t\t\t// response to head of queue creation\n\t\t\t\tif ws := w.resuming[0]; ws != nil {\n\t\t\t\t\tw.addSubstream(pbresp, ws)\n\t\t\t\t\tw.dispatchEvent(pbresp)\n\t\t\t\t\tw.resuming[0] = nil\n\t\t\t\t}\n\n\t\t\t\tif ws := w.nextResume(); ws != nil {\n\t\t\t\t\twc.Send(ws.initReq.toPB())\n\t\t\t\t}\n\n\t\t\t\t// reset for next iteration\n\t\t\t\tcur = nil\n\n\t\t\tcase pbresp.Canceled && pbresp.CompactRevision == 0:\n\t\t\t\tdelete(cancelSet, pbresp.WatchId)\n\t\t\t\tif ws, ok := w.substreams[pbresp.WatchId]; ok {\n\t\t\t\t\t// signal to stream goroutine to update closingc\n\t\t\t\t\tclose(ws.recvc)\n\t\t\t\t\tclosing[ws] = struct{}{}\n\t\t\t\t}\n\n\t\t\t\t// reset for next iteration\n\t\t\t\tcur = nil\n\n\t\t\tcase cur.Fragment:\n\t\t\t\t// watch response events are still fragmented\n\t\t\t\t// continue to fetch next fragmented event arrival\n\t\t\t\tcontinue\n\n\t\t\tdefault:\n\t\t\t\t// dispatch to appropriate watch stream\n\t\t\t\tok := w.dispatchEvent(cur)\n\n\t\t\t\t// reset for next iteration\n\t\t\t\tcur = nil\n\n\t\t\t\tif ok {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\t// watch response on unexpected watch id; cancel id\n\t\t\t\tif _, ok := cancelSet[pbresp.WatchId]; ok {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tcancelSet[pbresp.WatchId] = struct{}{}\n\t\t\t\tcr := &pb.WatchRequest_CancelRequest{\n\t\t\t\t\tCancelRequest: &pb.WatchCancelRequest{\n\t\t\t\t\t\tWatchId: pbresp.WatchId,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\treq := &pb.WatchRequest{RequestUnion: cr}\n\t\t\t\twc.Send(req)\n\t\t\t}\n\n\t\t// watch client failed on Recv; spawn another if possible\n\t\tcase err := <-w.errc:\n\t\t\tif isHaltErr(w.ctx, err) || toErr(w.ctx, err) == v3rpc.ErrNoLeader {\n\t\t\t\tcloseErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif wc, closeErr = w.newWatchClient(); closeErr != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ws := w.nextResume(); ws != nil {\n\t\t\t\twc.Send(ws.initReq.toPB())\n\t\t\t}\n\t\t\tcancelSet = make(map[int64]struct{})\n\n\t\tcase <-w.ctx.Done():\n\t\t\treturn\n\n\t\tcase ws := <-w.closingc:\n\t\t\tw.closeSubstream(ws)\n\t\t\tdelete(closing, ws)\n\t\t\t// no more watchers on this stream, shutdown\n\t\t\tif len(w.substreams)+len(w.resuming) == 0 {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "//\n// Write writes len(p) bytes from p to the underlying data stream.\n// It returns the number of bytes written from p (0 <= n <= len(p))\n// and any error encountered that caused the write to stop early.\n// Write must return a non-nil error if it returns n < len(p).\n//\n// Write doesn't modify b.User, so once a []byte is pinned with\n// a call to Bytes(), it should remain valid even with additional\n// calls to Write() that come after the Bytes() call.\n// [SEP] func (b *AtomicFixedSizeRingBuf) Write(p []byte) (n int, err error) {\n\tb.tex.Lock()\n\tdefer b.tex.Unlock()\n\n\tfor {\n\t\tif len(p) == 0 {\n\t\t\t// nothing (left) to copy in; notice we shorten our\n\t\t\t// local copy p (below) as we read from it.\n\t\t\treturn\n\t\t}\n\n\t\twriteCapacity := b.N - b.readable\n\t\tif writeCapacity <= 0 {\n\t\t\t// we are all full up already.\n\t\t\treturn n, io.ErrShortWrite\n\t\t}\n\t\tif len(p) > writeCapacity {\n\t\t\terr = io.ErrShortWrite\n\t\t\t// leave err set and\n\t\t\t// keep going, write what we can.\n\t\t}\n\n\t\twriteStart := (b.Beg + b.readable) % b.N\n\n\t\tupperLim := intMin2(writeStart+writeCapacity, b.N)\n\n\t\tk := copy(b.A[b.Use][writeStart:upperLim], p)\n\n\t\tn += k\n\t\tb.readable += k\n\t\tp = p[k:]\n\n\t\t// we can fill from b.A[b.Use][0:something] from\n\t\t// p's remainder, so loop\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// CubicCurveTo adds a cubic bezier curve to the current path [SEP] func (p *Path) CubicCurveTo(cx1, cy1, cx2, cy2, x, y float64) {\n\tif len(p.Components) == 0 { //special case when no move has been done\n\t\tp.MoveTo(x, y)\n\t} else {\n\t\tp.appendToPath(CubicCurveToCmp, cx1, cy1, cx2, cy2, x, y)\n\t}\n\tp.x = x\n\tp.y = y\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// makeBroadcastRouteStatusSlice takes a snapshot of the broadcast routes in routes. [SEP] func makeBroadcastRouteStatusSlice(r *routes) []broadcastRouteStatus {\n\tr.RLock()\n\tdefer r.RUnlock()\n\n\tvar slice []broadcastRouteStatus\n\tfor source, via := range r.broadcast {\n\t\tvar hops []string\n\t\tfor _, hop := range via {\n\t\t\thops = append(hops, hop.String())\n\t\t}\n\t\tslice = append(slice, broadcastRouteStatus{source.String(), hops})\n\t}\n\treturn slice\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// WithMaxVirtualTimeTaskStarvationCount if set this specifies the maximum\n// number of tasks that can be run before virtual is forced forwards to prevent\n// deadlock. [SEP] func (p SetVirtualTimePolicyParams) WithMaxVirtualTimeTaskStarvationCount(maxVirtualTimeTaskStarvationCount int64) *SetVirtualTimePolicyParams {\n\tp.MaxVirtualTimeTaskStarvationCount = maxVirtualTimeTaskStarvationCount\n\treturn &p\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "//GetResourceByName convenience method to find and return a resource by name [SEP] func (s *ConcoursePipeline) GetResourceByName(name string) *atc.ResourceConfig {\n\tfor i, v := range s.Resources {\n\t\tif v.Name == name {\n\t\t\treturn &s.Resources[i]\n\t\t}\n\t}\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// WebpackCheck will compare the current default Buffalo\n// webpack.config.js against the applications webpack.config.js. If they are\n// different you have the option to overwrite the existing webpack.config.js\n// file with the new one. [SEP] func WebpackCheck(r *Runner) error {\n\tfmt.Println(\"~~~ Checking webpack.config.js ~~~\")\n\n\tif !r.App.WithWebpack {\n\t\treturn nil\n\t}\n\n\tbox := webpack.Templates\n\n\tf, err := box.FindString(\"webpack.config.js.tmpl\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttmpl, err := template.New(\"webpack\").Parse(f)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tbb := &bytes.Buffer{}\n\terr = tmpl.Execute(bb, map[string]interface{}{\n\t\t\"opts\": &webpack.Options{\n\t\t\tApp: r.App,\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tb, err := ioutil.ReadFile(\"webpack.config.js\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif string(b) == bb.String() {\n\t\treturn nil\n\t}\n\n\tif !ask(\"Your webpack.config.js file is different from the latest Buffalo template.\\nWould you like to replace yours with the latest template?\") {\n\t\tfmt.Println(\"\\tSkipping webpack.config.js\")\n\t\treturn nil\n\t}\n\n\twf, err := os.Create(\"webpack.config.js\")\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = wf.Write(bb.Bytes())\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn wf.Close()\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// findOwnersForFile returns the OWNERS file path furthest down the tree for a specified file\n// using ownerMap to check for entries [SEP] func findOwnersForFile(log *logrus.Entry, path string, ownerMap map[string]map[*regexp.Regexp]sets.String) string {\n\td := path\n\n\tfor ; d != baseDirConvention; d = canonicalize(filepath.Dir(d)) {\n\t\trelative, err := filepath.Rel(d, path)\n\t\tif err != nil {\n\t\t\tlog.WithError(err).WithField(\"path\", path).Errorf(\"Unable to find relative path between %q and path.\", d)\n\t\t\treturn \"\"\n\t\t}\n\t\tfor re, n := range ownerMap[d] {\n\t\t\tif re != nil && !re.MatchString(relative) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif len(n) != 0 {\n\t\t\t\treturn d\n\t\t\t}\n\t\t}\n\t}\n\treturn \"\"\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MapSlice applies a function that processes slices of strings to the strings\n// in \"ps\" and returns a new PlatformStrings with the results. [SEP] func (ps *PlatformStrings) MapSlice(f func([]string) ([]string, error)) (PlatformStrings, []error) {\n\tvar errors []error\n\n\tmapSlice := func(ss []string) []string {\n\t\trs, err := f(ss)\n\t\tif err != nil {\n\t\t\terrors = append(errors, err)\n\t\t\treturn nil\n\t\t}\n\t\treturn rs\n\t}\n\n\tmapStringMap := func(m map[string][]string) map[string][]string {\n\t\tif m == nil {\n\t\t\treturn nil\n\t\t}\n\t\trm := make(map[string][]string)\n\t\tfor k, ss := range m {\n\t\t\tss = mapSlice(ss)\n\t\t\tif len(ss) > 0 {\n\t\t\t\trm[k] = ss\n\t\t\t}\n\t\t}\n\t\tif len(rm) == 0 {\n\t\t\treturn nil\n\t\t}\n\t\treturn rm\n\t}\n\n\tmapPlatformMap := func(m map[Platform][]string) map[Platform][]string {\n\t\tif m == nil {\n\t\t\treturn nil\n\t\t}\n\t\trm := make(map[Platform][]string)\n\t\tfor k, ss := range m {\n\t\t\tss = mapSlice(ss)\n\t\t\tif len(ss) > 0 {\n\t\t\t\trm[k] = ss\n\t\t\t}\n\t\t}\n\t\tif len(rm) == 0 {\n\t\t\treturn nil\n\t\t}\n\t\treturn rm\n\t}\n\n\tresult := PlatformStrings{\n\t\tGeneric:  mapSlice(ps.Generic),\n\t\tOS:       mapStringMap(ps.OS),\n\t\tArch:     mapStringMap(ps.Arch),\n\t\tPlatform: mapPlatformMap(ps.Platform),\n\t}\n\treturn result, errors\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// UnmarshalJSON supports json.Unmarshaler interface [SEP] func (v *SetPlaybackRateParams) UnmarshalJSON(data []byte) error {\n\tr := jlexer.Lexer{Data: data}\n\teasyjsonC5a4559bDecodeGithubComChromedpCdprotoAnimation1(&r, v)\n\treturn r.Error()\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// zfsPoolVolumeCreate creates a ZFS dataset with a set of given properties. [SEP] func zfsPoolVolumeCreate(dataset string, properties ...string) (string, error) {\n\tcmd := []string{\"zfs\", \"create\"}\n\n\tfor _, prop := range properties {\n\t\tcmd = append(cmd, []string{\"-o\", prop}...)\n\t}\n\n\tcmd = append(cmd, []string{\"-p\", dataset}...)\n\n\treturn shared.RunCommand(cmd[0], cmd[1:]...)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// NewCache creates a new cache. [SEP] func NewCache(size int) (*Cache, error) {\n\tc, err := lru.NewWithEvict(size, func(key interface{}, value interface{}) {\n\t\tgo func() {\n\t\t\ttree, ok := value.(*dbHashTree)\n\t\t\tif !ok {\n\t\t\t\tlogrus.Infof(\"non hashtree slice value of type: %v\", reflect.TypeOf(value))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := tree.Destroy(); err != nil {\n\t\t\t\tlogrus.Infof(\"failed to destroy hashtree: %v\", err)\n\t\t\t}\n\t\t}()\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Cache{c}, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// AddFlags adds flags to the FlagSet that populate\n// the GCS upload options struct given. [SEP] func (o *Options) AddFlags(fs *flag.FlagSet) {\n\tfs.StringVar(&o.SrcRoot, \"src-root\", \"\", \"Where to root source checkouts\")\n\tfs.StringVar(&o.Log, \"log\", \"\", \"Where to write logs\")\n\tfs.StringVar(&o.GitUserName, \"git-user-name\", DefaultGitUserName, \"Username to set in git config\")\n\tfs.StringVar(&o.GitUserEmail, \"git-user-email\", DefaultGitUserEmail, \"Email to set in git config\")\n\tfs.Var(&o.refs, \"repo\", \"Mapping of Git URI to refs to check out, can be provided more than once\")\n\tfs.Var(&o.keys, \"ssh-key\", \"Path to SSH key to enable during cloning, can be provided more than once\")\n\tfs.Var(&o.clonePath, \"clone-alias\", \"Format string for the path to clone to\")\n\tfs.Var(&o.cloneURI, \"uri-prefix\", \"Format string for the URI prefix to clone from\")\n\tfs.IntVar(&o.MaxParallelWorkers, \"max-workers\", 0, \"Maximum number of parallel workers, unset for unlimited.\")\n\tfs.StringVar(&o.CookiePath, \"cookiefile\", \"\", \"Path to git http.cookiefile\")\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "/* Sets image ROI (region of interest) (COI is not changed) */ [SEP] func (img *IplImage) SetROI(rect Rect) {\n\tC.cvSetImageROI((*C.IplImage)(img), C.CvRect(rect))\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MarshalJSON supports json.Marshaler interface [SEP] func (v GetBestEffortCoverageReturns) MarshalJSON() ([]byte, error) {\n\tw := jwriter.Writer{}\n\teasyjsonC5a4559bEncodeGithubComChromedpCdprotoProfiler19(&w, v)\n\treturn w.Buffer.BuildBytes(), w.Error\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// UnmarshalJSON supports json.Unmarshaler interface [SEP] func (v *StopScreencastParams) UnmarshalJSON(data []byte) error {\n\tr := jlexer.Lexer{Data: data}\n\teasyjsonC5a4559bDecodeGithubComChromedpCdprotoPage3(&r, v)\n\treturn r.Error()\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// importKeysFromBytes imports public keys from the supplied blob and returns their identities.\n// The blob is assumed to have an appropriate format (the caller is expected to know which one).\n// NOTE: This may modify long-term state (e.g. key storage in a directory underlying the mechanism);\n// but we do not make this public, it can only be used through newEphemeralGPGSigningMechanism. [SEP] func (m *gpgmeSigningMechanism) importKeysFromBytes(blob []byte) ([]string, error) {\n\tinputData, err := gpgme.NewDataBytes(blob)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tres, err := m.ctx.Import(inputData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tkeyIdentities := []string{}\n\tfor _, i := range res.Imports {\n\t\tif i.Result == nil {\n\t\t\tkeyIdentities = append(keyIdentities, i.Fingerprint)\n\t\t}\n\t}\n\treturn keyIdentities, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MarshalJSON supports json.Marshaler interface [SEP] func (v GetSearchResultsReturns) MarshalJSON() ([]byte, error) {\n\tw := jwriter.Writer{}\n\teasyjsonC5a4559bEncodeGithubComChromedpCdprotoDom32(&w, v)\n\treturn w.Buffer.BuildBytes(), w.Error\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// UnmarshalJSON supports json.Unmarshaler interface [SEP] func (v *FocusParams) UnmarshalJSON(data []byte) error {\n\tr := jlexer.Lexer{Data: data}\n\teasyjsonC5a4559bDecodeGithubComChromedpCdprotoDom54(&r, v)\n\treturn r.Error()\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// CheckoutPullRequest does exactly that. [SEP] func (r *Repo) CheckoutPullRequest(number int) error {\n\tr.logger.Infof(\"Fetching and checking out %s#%d.\", r.repo, number)\n\tif b, err := retryCmd(r.logger, r.Dir, r.git, \"fetch\", r.base+\"/\"+r.repo, fmt.Sprintf(\"pull/%d/head:pull%d\", number, number)); err != nil {\n\t\treturn fmt.Errorf(\"git fetch failed for PR %d: %v. output: %s\", number, err, string(b))\n\t}\n\tco := r.gitCommand(\"checkout\", fmt.Sprintf(\"pull%d\", number))\n\tif b, err := co.CombinedOutput(); err != nil {\n\t\treturn fmt.Errorf(\"git checkout failed for PR %d: %v. output: %s\", number, err, string(b))\n\t}\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// ListJob returns info about all jobs.\n// If pipelineName is non empty then only jobs that were started by the named pipeline will be returned\n// If inputCommit is non-nil then only jobs which took the specific commits as inputs will be returned.\n// The order of the inputCommits doesn't matter.\n// If outputCommit is non-nil then only the job which created that commit as output will be returned. [SEP] func (c APIClient) ListJob(pipelineName string, inputCommit []*pfs.Commit, outputCommit *pfs.Commit) ([]*pps.JobInfo, error) {\n\tvar result []*pps.JobInfo\n\tif err := c.ListJobF(pipelineName, inputCommit, outputCommit, func(ji *pps.JobInfo) error {\n\t\tresult = append(result, ji)\n\t\treturn nil\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn result, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Seek brings us to element >= key if reversed is false. Otherwise, <= key. [SEP] func (s *ConcatIterator) Seek(key []byte) {\n\tvar idx int\n\tif !s.reversed {\n\t\tidx = sort.Search(len(s.tables), func(i int) bool {\n\t\t\treturn y.CompareKeys(s.tables[i].Biggest(), key) >= 0\n\t\t})\n\t} else {\n\t\tn := len(s.tables)\n\t\tidx = n - 1 - sort.Search(n, func(i int) bool {\n\t\t\treturn y.CompareKeys(s.tables[n-1-i].Smallest(), key) <= 0\n\t\t})\n\t}\n\tif idx >= len(s.tables) || idx < 0 {\n\t\ts.setIdx(-1)\n\t\treturn\n\t}\n\t// For reversed=false, we know s.tables[i-1].Biggest() < key. Thus, the\n\t// previous table cannot possibly contain key.\n\ts.setIdx(idx)\n\ts.cur.Seek(key)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// IsRecursionRequest checks whether the given HTTP request is marked with the\n// \"recursion\" flag in its form values. [SEP] func IsRecursionRequest(r *http.Request) bool {\n\trecursionStr := r.FormValue(\"recursion\")\n\n\trecursion, err := strconv.Atoi(recursionStr)\n\tif err != nil {\n\t\treturn false\n\t}\n\n\treturn recursion != 0\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// putConfig parses and returns the configuration data from the storage backend. [SEP] func putConfig(ctx context.Context, s logical.Storage, cfg *config) error {\n\tentry, err := logical.StorageEntryJSON(\"config\", cfg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%v: failed to generate storage entry\", err)\n\t}\n\tif err := s.Put(ctx, entry); err != nil {\n\t\treturn fmt.Errorf(\"%v: failed to write configuration to storage\", err)\n\t}\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MardkownSHAList prints the list of commits in a markdown-friendly way. [SEP] func MarkdownSHAList(org, repo string, list []github.GitCommit) string {\n\tlines := make([]string, len(list))\n\tlineFmt := \"- [%s](https://github.com/%s/%s/commits/%s) %s\"\n\tfor i, commit := range list {\n\t\tif commit.SHA == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\t// if we somehow encounter a SHA that's less than 7 characters, we will\n\t\t// just use it as is.\n\t\tshortSHA := commit.SHA\n\t\tif len(shortSHA) > 7 {\n\t\t\tshortSHA = shortSHA[:7]\n\t\t}\n\n\t\t// get the first line of the commit\n\t\tmessage := strings.Split(commit.Message, \"\\n\")[0]\n\n\t\tlines[i] = fmt.Sprintf(lineFmt, shortSHA, org, repo, commit.SHA, message)\n\t}\n\treturn strings.Join(lines, \"\\n\")\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Delete an item\n// Delete always succeed if an item exists. [SEP] func (w *Writer) Delete(bs []byte) (success bool) {\n\t_, success = w.Delete2(bs)\n\treturn\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// DeferByte reserves space in the buffer for a single byte, and returns a\n// reference that can be used to update that byte later [SEP] func (w *WriteBuffer) DeferByte() ByteRef {\n\tif len(w.remaining) == 0 {\n\t\tw.setErr(ErrBufferFull)\n\t\treturn ByteRef(nil)\n\t}\n\n\t// Always zero out references, since the caller expects the default to be 0.\n\tw.remaining[0] = 0\n\tbufRef := ByteRef(w.remaining[0:])\n\tw.remaining = w.remaining[1:]\n\treturn bufRef\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// GetConfigs returns all configs [SEP] func (s *Storage) GetConfigs() ([]common.ResourcesConfig, error) {\n\tvar configs []common.ResourcesConfig\n\titems, err := s.configs.List()\n\tif err != nil {\n\t\treturn configs, err\n\t}\n\tfor _, i := range items {\n\t\tvar conf common.ResourcesConfig\n\t\tconf, err = common.ItemToResourcesConfig(i)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tconfigs = append(configs, conf)\n\t}\n\treturn configs, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "//ResValue returns the string value of the attribute [SEP] func (a XMLNode) ResValue() string {\n\tswitch a.NodeType {\n\tcase tree.NtAttr:\n\t\treturn a.Token.(*xml.Attr).Value\n\tcase tree.NtChd:\n\t\treturn string(a.Token.(xml.CharData))\n\tcase tree.NtComm:\n\t\treturn string(a.Token.(xml.Comment))\n\t}\n\t//case tree.NtPi:\n\treturn string(a.Token.(xml.ProcInst).Inst)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// ValidateOU checks the OU of a verified peer cert and raises 403 if the OU doesn't match any OU in the AllowedOUs list. [SEP] func (a *Auth) ValidateOU(verifiedCert *x509.Certificate) error {\n\tvar failed []string\n\n\tfor _, ou := range a.opt.AllowedOUs {\n\t\tfor _, clientOU := range verifiedCert.Subject.OrganizationalUnit {\n\t\t\tif ou == clientOU {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tfailed = append(failed, clientOU)\n\t\t}\n\t}\n\treturn fmt.Errorf(\"cert failed OU validation for %v, Allowed: %v\", failed, a.opt.AllowedOUs)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// GoogleServiceAccountCredentialsFile is an option that loads Google Service\n// Account credentials for use with the StackdriverHook from the specified\n// file.\n//\n// Google Service Account credentials can be downloaded from the Google Cloud\n// console: https://console.cloud.google.com/iam-admin/serviceaccounts/ [SEP] func GoogleServiceAccountCredentialsFile(path string) Option {\n\treturn func(sh *StackdriverHook) error {\n\t\tbuf, err := ioutil.ReadFile(path)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn GoogleServiceAccountCredentialsJSON(buf)(sh)\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Step 1 is the removal of standard suffixes, all of which must\n// occur in RV.\n//\n//\n// Search for a PERFECTIVE GERUND ending. If one is found remove it, and\n// that is then the end of step 1. Otherwise try and remove a REFLEXIVE\n// ending, and then search in turn for (1) an ADJECTIVAL, (2) a VERB or\n// (3) a NOUN ending. As soon as one of the endings (1) to (3) is found\n// remove it, and terminate step 1.\n// [SEP] func step1(word *snowballword.SnowballWord) bool {\n\n\t// `stop` will be used to signal early termination\n\tvar stop bool\n\n\t// Search for a PERFECTIVE GERUND ending\n\tstop = removePerfectiveGerundEnding(word)\n\tif stop {\n\t\treturn true\n\t}\n\n\t// Next remove reflexive endings\n\tword.RemoveFirstSuffixIn(word.RVstart, \"ся\", \"сь\")\n\n\t// Next remove adjectival endings\n\tstop = removeAdjectivalEnding(word)\n\tif stop {\n\t\treturn true\n\t}\n\n\t// Next remove verb endings\n\tstop = removeVerbEnding(word)\n\tif stop {\n\t\treturn true\n\t}\n\n\t// Next remove noun endings\n\tsuffix, _ := word.RemoveFirstSuffixIn(word.RVstart,\n\t\t\"иями\", \"ями\", \"иях\", \"иям\", \"ием\", \"ией\", \"ами\", \"ях\",\n\t\t\"ям\", \"ья\", \"ью\", \"ье\", \"ом\", \"ой\", \"ов\", \"ия\", \"ию\",\n\t\t\"ий\", \"ии\", \"ие\", \"ем\", \"ей\", \"еи\", \"ев\", \"ах\", \"ам\",\n\t\t\"я\", \"ю\", \"ь\", \"ы\", \"у\", \"о\", \"й\", \"и\", \"е\", \"а\",\n\t)\n\tif suffix != \"\" {\n\t\treturn true\n\t}\n\n\treturn false\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// FindFunc returns the function which contains the code at address pc, if any. [SEP] func (p *Process) FindFunc(pc core.Address) *Func {\n\treturn p.funcTab.find(pc)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// prowToGCS returns the GCS key corresponding to the given prow key [SEP] func (s *Spyglass) prowToGCS(prowKey string) (string, error) {\n\tjobName, buildID, err := s.KeyToJob(prowKey)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"could not get GCS src: %v\", err)\n\t}\n\n\tjob, err := s.jobAgent.GetProwJob(jobName, buildID)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"Failed to get prow job from src %q: %v\", prowKey, err)\n\t}\n\n\turl := job.Status.URL\n\tprefix := s.config().Plank.GetJobURLPrefix(job.Spec.Refs)\n\tif !strings.HasPrefix(url, prefix) {\n\t\treturn \"\", fmt.Errorf(\"unexpected job URL %q when finding GCS path: expected something starting with %q\", url, prefix)\n\t}\n\treturn url[len(prefix):], nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// UnmarshalJSON supports json.Unmarshaler interface [SEP] func (v *ReadReturns) UnmarshalJSON(data []byte) error {\n\tr := jlexer.Lexer{Data: data}\n\teasyjsonC5a4559bDecodeGithubComChromedpCdprotoIo2(&r, v)\n\treturn r.Error()\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Delete takes name of the prowJob and deletes it. Returns an error if one occurs. [SEP] func (c *FakeProwJobs) Delete(name string, options *v1.DeleteOptions) error {\n\t_, err := c.Fake.\n\t\tInvokes(testing.NewDeleteAction(prowjobsResource, c.ns, name), &prowjobsv1.ProwJob{})\n\n\treturn err\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// BreadthMatchAll performs a breadth first search of the Part tree and returns all parts that cause\n// the given matcher to return true [SEP] func (p *Part) BreadthMatchAll(matcher PartMatcher) []*Part {\n\tq := list.New()\n\tq.PushBack(p)\n\n\tmatches := make([]*Part, 0, 10)\n\n\t// Push children onto queue and attempt to match in that order\n\tfor q.Len() > 0 {\n\t\te := q.Front()\n\t\tp := e.Value.(*Part)\n\t\tif matcher(p) {\n\t\t\tmatches = append(matches, p)\n\t\t}\n\t\tq.Remove(e)\n\t\tc := p.FirstChild\n\t\tfor c != nil {\n\t\t\tq.PushBack(c)\n\t\t\tc = c.NextSibling\n\t\t}\n\t}\n\n\treturn matches\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// AskForConfirmation prompts user for yes/no response [SEP] func AskForConfirmation() bool {\n\tvar response string\n\t_, err := fmt.Scanln(&response)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tokayResponses := []string{\"y\", \"yes\"}\n\tnokayResponses := []string{\"n\", \"no\"}\n\tif StringInSlice(strings.ToLower(response), okayResponses) {\n\t\treturn true\n\t}\n\tif StringInSlice(strings.ToLower(response), nokayResponses) {\n\t\treturn false\n\t}\n\tfmt.Println(\"Please type yes or no and then press enter:\")\n\treturn AskForConfirmation()\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Do executes Target.setRemoteLocations against the provided context. [SEP] func (p *SetRemoteLocationsParams) Do(ctx context.Context) (err error) {\n\treturn cdp.Execute(ctx, CommandSetRemoteLocations, p, nil)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// UnmarshalJSON supports json.Unmarshaler interface [SEP] func (v *CloseParams) UnmarshalJSON(data []byte) error {\n\tr := jlexer.Lexer{Data: data}\n\teasyjsonC5a4559bDecodeGithubComChromedpCdprotoBrowser19(&r, v)\n\treturn r.Error()\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Locator returns a locator for the given resource [SEP] func (r *SecurityGroup) Locator(api *API) *SecurityGroupLocator {\n\tfor _, l := range r.Links {\n\t\tif l[\"rel\"] == \"self\" {\n\t\t\treturn api.SecurityGroupLocator(l[\"href\"])\n\t\t}\n\t}\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MarshalJSON supports json.Marshaler interface [SEP] func (v RemoveAttributeParams) MarshalJSON() ([]byte, error) {\n\tw := jwriter.Writer{}\n\teasyjsonC5a4559bEncodeGithubComChromedpCdprotoDom16(&w, v)\n\treturn w.Buffer.BuildBytes(), w.Error\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// GetAlphaNum4 retrieves the AlphaNum4 value from the union,\n// returning ok if the union's switch indicated the value is valid. [SEP] func (u Asset) GetAlphaNum4() (result AssetAlphaNum4, ok bool) {\n\tarmName, _ := u.ArmForSwitch(int32(u.Type))\n\n\tif armName == \"AlphaNum4\" {\n\t\tresult = *u.AlphaNum4\n\t\tok = true\n\t}\n\n\treturn\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// GetNetwork returns a Network entry for the provided name [SEP] func (r *ProtocolLXD) GetNetwork(name string) (*api.Network, string, error) {\n\tif !r.HasExtension(\"network\") {\n\t\treturn nil, \"\", fmt.Errorf(\"The server is missing the required \\\"network\\\" API extension\")\n\t}\n\n\tnetwork := api.Network{}\n\n\t// Fetch the raw value\n\tetag, err := r.queryStruct(\"GET\", fmt.Sprintf(\"/networks/%s\", url.QueryEscape(name)), nil, \"\", &network)\n\tif err != nil {\n\t\treturn nil, \"\", err\n\t}\n\n\treturn &network, etag, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "/* Inpaints the selected region in the image */ [SEP] func Inpaint(src, inpaint_mask, dst *IplImage, inpaintRange float64, flags int) {\n\tC.cvInpaint(\n\t\tunsafe.Pointer(src),\n\t\tunsafe.Pointer(inpaint_mask),\n\t\tunsafe.Pointer(dst),\n\t\tC.double(inpaintRange),\n\t\tC.int(flags),\n\t)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// title: token list\n// path: /tokens\n// method: GET\n// produce: application/json\n// responses:\n//   200: List tokens\n//   204: No content\n//   401: Unauthorized [SEP] func tokenList(w http.ResponseWriter, r *http.Request, t auth.Token) error {\n\ttokens, err := servicemanager.TeamToken.FindByUserToken(t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(tokens) == 0 {\n\t\tw.WriteHeader(http.StatusNoContent)\n\t\treturn nil\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\treturn json.NewEncoder(w).Encode(tokens)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "/*\n----------\nPath = Badges.xml\nSize = 4065633\nPacked Size = 18990516\nModified = 2015-03-09 14:30:49\nAttributes = ....A\nCRC = 2C468F32\nEncrypted = -\nMethod = BZip2\nBlock = 0\n*/ [SEP] func advanceToFirstEntry(scanner *bufio.Scanner) error {\n\tfor scanner.Scan() {\n\t\ts := scanner.Text()\n\t\tif s == \"----------\" {\n\t\t\treturn nil\n\t\t}\n\t}\n\terr := scanner.Err()\n\tif err == nil {\n\t\terr = ErrNoEntries\n\t}\n\treturn err\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// NewIterator returns a new iterator of the Table [SEP] func (t *Table) NewIterator(reversed bool) *Iterator {\n\tt.IncrRef() // Important.\n\tti := &Iterator{t: t, reversed: reversed}\n\tti.next()\n\treturn ti\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// tryAddLevel0Table returns true if ok and no stalling. [SEP] func (s *levelHandler) tryAddLevel0Table(t *table.Table) bool {\n\ty.AssertTrue(s.level == 0)\n\t// Need lock as we may be deleting the first table during a level 0 compaction.\n\ts.Lock()\n\tdefer s.Unlock()\n\tif len(s.tables) >= s.db.opt.NumLevelZeroTablesStall {\n\t\treturn false\n\t}\n\n\ts.tables = append(s.tables, t)\n\tt.IncrRef()\n\ts.totalSize += t.Size()\n\n\treturn true\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Respond without doing anything.\n// This endpoint is used to check that the service is up.\n//\n// See #ping [SEP] func (auth *Auth) Ping() error {\n\tcd := tcclient.Client(*auth)\n\t_, _, err := (&cd).APICall(nil, \"GET\", \"/ping\", nil, nil)\n\treturn err\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Active is true if all the states are active. [SEP] func (m *MultiState) Active() bool {\n\tfor _, state := range m.states {\n\t\tif !state.Active() {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Approvers returns ALL of the users who are approvers for the\n// requested file (including approvers in parent dirs' OWNERS).\n// If pkg/OWNERS has user1 and pkg/util/OWNERS has user2 this\n// will return both user1 and user2 for the path pkg/util/sets/file.go [SEP] func (o *RepoOwners) Approvers(path string) sets.String {\n\treturn o.entriesForFile(path, o.approvers, false)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "//go:generate stringer -type=reqResReaderState [SEP] func (w *reqResWriter) argWriter(last bool, inState reqResWriterState, outState reqResWriterState) (ArgWriter, error) {\n\tif w.err != nil {\n\t\treturn nil, w.err\n\t}\n\n\tif w.state != inState {\n\t\treturn nil, w.failed(errReqResWriterStateMismatch{state: w.state, expectedState: inState})\n\t}\n\n\targWriter, err := w.contents.ArgWriter(last)\n\tif err != nil {\n\t\treturn nil, w.failed(err)\n\t}\n\n\tw.state = outState\n\treturn argWriter, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Caller to commit must hold a write lock. [SEP] func (wb *WriteBatch) commit() error {\n\tif wb.err != nil {\n\t\treturn wb.err\n\t}\n\t// Get a new txn before we commit this one. So, the new txn doesn't need\n\t// to wait for this one to commit.\n\twb.wg.Add(1)\n\twb.txn.CommitWith(wb.callback)\n\twb.txn = wb.db.newTransaction(true, true)\n\t// See comment about readTs in NewWriteBatch.\n\twb.txn.readTs = wb.db.orc.readMark.DoneUntil()\n\treturn wb.err\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// WithBackoff sets the `BackoffFunc `used to control time between retries. [SEP] func withBackoff(bf backoffFunc) retryOption {\n\treturn retryOption{applyFunc: func(o *options) {\n\t\to.backoffFunc = bf\n\t}}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MarshalJSON supports json.Marshaler interface [SEP] func (v EventExecutionContextDestroyed) MarshalJSON() ([]byte, error) {\n\tw := jwriter.Writer{}\n\teasyjsonC5a4559bEncodeGithubComChromedpCdprotoRuntime31(&w, v)\n\treturn w.Buffer.BuildBytes(), w.Error\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Set sets the EWMA's value. [SEP] func (e *VariableEWMA) Set(value float64) {\n\te.value = value\n\tif e.count <= WARMUP_SAMPLES {\n\t\te.count = WARMUP_SAMPLES + 1\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// makeChecks renders a ContextPolicy into the corresponding GitHub api object.\n//\n// Returns nil when input policy is nil.\n// Otherwise returns non-nil Contexts (empty if unset) and Strict iff Strict is true [SEP] func makeChecks(cp *branchprotection.ContextPolicy) *github.RequiredStatusChecks {\n\tif cp == nil {\n\t\treturn nil\n\t}\n\treturn &github.RequiredStatusChecks{\n\t\tContexts: append([]string{}, sets.NewString(cp.Contexts...).List()...),\n\t\tStrict:   makeBool(cp.Strict),\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// MakeCommand returns an `aggregate` command. [SEP] func MakeCommand() *cobra.Command {\n\tflags := &flags{}\n\tcmd := &cobra.Command{\n\t\tUse:   \"aggregate [files...]\",\n\t\tShort: \"Aggregates multiple Go coverage files.\",\n\t\tLong: `Given multiple Go coverage files from identical binaries recorded in\n\"count\" or \"atomic\" mode, produces a new Go coverage file in the same mode\nthat counts how many of those coverage profiles hit a block at least once.`,\n\t\tRun: func(cmd *cobra.Command, args []string) {\n\t\t\trun(flags, cmd, args)\n\t\t},\n\t}\n\tcmd.Flags().StringVarP(&flags.OutputFile, \"output\", \"o\", \"-\", \"output file\")\n\treturn cmd\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// NewClient returns a configured Rundeck client. [SEP] func NewClient(config *ClientConfig) (*Client, error) {\n\tt := &http.Transport{\n\t\tTLSClientConfig: &tls.Config{\n\t\t\tInsecureSkipVerify: config.AllowUnverifiedSSL,\n\t\t},\n\t}\n\thttpClient := &http.Client{\n\t\tTransport: t,\n\t}\n\n\tapiPath, _ := url.Parse(\"api/13/\")\n\tbaseURL, err := url.Parse(config.BaseURL)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid base URL: %s\", err.Error())\n\t}\n\tapiURL := baseURL.ResolveReference(apiPath)\n\n\treturn &Client{\n\t\thttpClient: httpClient,\n\t\tapiURL:     apiURL,\n\t\tauthToken:  config.AuthToken,\n\t}, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Locator returns a locator for the given resource [SEP] func (r *NetworkGateway) Locator(api *API) *NetworkGatewayLocator {\n\tfor _, l := range r.Links {\n\t\tif l[\"rel\"] == \"self\" {\n\t\t\treturn api.NetworkGatewayLocator(l[\"href\"])\n\t\t}\n\t}\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// newDockerClientFromRef returns a new dockerClient instance for refHostname (a host a specified in the Docker image reference, not canonicalized to dockerRegistry)\n// “write” specifies whether the client will be used for \"write\" access (in particular passed to lookaside.go:toplevelFromSection) [SEP] func newDockerClientFromRef(sys *types.SystemContext, ref dockerReference, write bool, actions string) (*dockerClient, error) {\n\tregistry := reference.Domain(ref.ref)\n\tusername, password, err := config.GetAuthentication(sys, registry)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"error getting username and password\")\n\t}\n\tsigBase, err := configuredSignatureStorageBase(sys, ref, write)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tclient, err := newDockerClient(sys, registry, ref.ref.Name())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclient.username = username\n\tclient.password = password\n\tclient.signatureBase = sigBase\n\tclient.scope.actions = actions\n\tclient.scope.remoteName = reference.Path(ref.ref)\n\treturn client, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// WithBackendNodeID backend identifier of the node to resolve. [SEP] func (p ResolveNodeParams) WithBackendNodeID(backendNodeID cdp.BackendNodeID) *ResolveNodeParams {\n\tp.BackendNodeID = backendNodeID\n\treturn &p\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Set is required for kingpin interfaces to allow command line params\n// to be set to our map datatype [SEP] func (o *MapByteOption) Set(value string) error {\n\tparts := stringMapRegex.Split(value, 2)\n\tif len(parts) != 2 {\n\t\treturn fmt.Errorf(\"expected KEY=VALUE got '%s'\", value)\n\t}\n\tval := ByteOption{}\n\tval.Set(parts[1])\n\t(*o)[parts[0]] = val\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// DeleteImage deletes the named image from the registry, if supported. [SEP] func (ref archiveReference) DeleteImage(ctx context.Context, sys *types.SystemContext) error {\n\t// Not really supported, for safety reasons.\n\treturn errors.New(\"Deleting images not implemented for docker-archive: images\")\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// readKey reads a private rsa key from path.\n// The key is expected to be in PEM format. [SEP] func readKey(path string) (crypto.Signer, error) {\n\tb, err := ioutil.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\td, _ := pem.Decode(b)\n\tif d == nil {\n\t\treturn nil, fmt.Errorf(\"no block found in %q\", path)\n\t}\n\tswitch d.Type {\n\tcase rsaPrivateKey:\n\t\treturn x509.ParsePKCS1PrivateKey(d.Bytes)\n\tcase ecPrivateKey:\n\t\treturn x509.ParseECPrivateKey(d.Bytes)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"%q is unsupported\", d.Type)\n\t}\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// cephRBDVolumeRename renames a given RBD storage volume\n// Note that this usually requires that the image be unmapped under its original\n// name, then renamed, and finally will be remapped again. If it is not unmapped\n// under its original name and the callers maps it under its new name the image\n// will be mapped twice. This will prevent it from being deleted. [SEP] func cephRBDVolumeRename(clusterName string, poolName string, volumeType string,\n\toldVolumeName string, newVolumeName string, userName string) error {\n\t_, err := shared.RunCommand(\n\t\t\"rbd\",\n\t\t\"--id\", userName,\n\t\t\"--cluster\", clusterName,\n\t\t\"mv\",\n\t\tfmt.Sprintf(\"%s/%s_%s\", poolName, volumeType, oldVolumeName),\n\t\tfmt.Sprintf(\"%s/%s_%s\", poolName, volumeType, newVolumeName))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// Broadcast returns the set of peer names that should be notified\n// when we receive a broadcast message originating from the named peer\n// based on established and symmetric connections. [SEP] func (r *routes) Broadcast(name PeerName) []PeerName {\n\treturn r.lookupOrCalculate(name, &r.broadcast, true)\n}", "target": 1, "target_options": ["no_match", "match"]}
{"input": "// CreateContainerBackup requests that LXD creates a new backup for the container [SEP] func (r *ProtocolLXD) CreateContainerBackup(containerName string, backup api.ContainerBackupsPost) (Operation, error) {\n\tif !r.HasExtension(\"container_backup\") {\n\t\treturn nil, fmt.Errorf(\"The server is missing the required \\\"container_backup\\\" API extension\")\n\t}\n\n\t// Send the request\n\top, _, err := r.queryOperation(\"POST\", fmt.Sprintf(\"/containers/%s/backups\",\n\t\turl.QueryEscape(containerName)), backup, \"\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn op, nil\n}", "target": 1, "target_options": ["no_match", "match"]}
